#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
OpenBLASä¼˜åŒ–åˆ†æ - ç¡¬ç¼–ç å·¥ä½œæµç¼–æ’å™¨
å…³é”®æ”¹è¿›ï¼šè·¯å¾„ç”±ä»£ç æ§åˆ¶ï¼ŒAgentåªç”Ÿæˆå†…å®¹
"""

import os
import time
import json
from typing import Dict, List
from typing_extensions import TypedDict
from datetime import datetime
from dotenv import load_dotenv

from langgraph.graph import StateGraph, START, END
from langchain.output_parsers import StructuredOutputParser, ResponseSchema

from agent import (
    AgentFactory,
    FileManager,
    AnalysisTask
)

load_dotenv()


# ===== å·¥ä½œæµçŠ¶æ€ =====
class WorkState(TypedDict):
    """å·¥ä½œæµçŠ¶æ€"""
    report_folder: str
    algorithms: List[str]
    current_algorithm_index: int
    completed_tasks: List[str]
    errors: List[str]


# ===== å·¥ä½œæµ =====
class Workflow:
    """å·¥ä½œæµ"""
    
    def __init__(self):
        self.factory = AgentFactory()
        self.file_mgr = FileManager()
        
        # åˆ›å»ºä¸“å®¶Agents
        self.scout = self.factory.create_scout_specialist()
        self.analyzer = self.factory.create_analyzer_specialist()
        self.individual_summarizer = self.factory.create_individual_summarizer()
        self.final_summarizer = self.factory.create_final_summarizer()
        
        # æ„å»ºå·¥ä½œæµ
        self.workflow = self._build_workflow()
    
    
    def _build_workflow(self) -> StateGraph:
        """æ„å»ºç¡¬ç¼–ç çš„é¡ºåºå·¥ä½œæµ"""
        workflow = StateGraph(WorkState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("orchestrator", self.orchestrator_node)
        workflow.add_node("scout_work", self.scout_work)
        workflow.add_node("analyzer_work", self.analyzer_work)
        workflow.add_node("individual_summary_work", self.individual_summary_work)
        workflow.add_node("final_summary_work", self.final_summary_work)
        
        # è®¾ç½®å…¥å£
        workflow.add_edge(START, "orchestrator")
        
        # ç¼–æ’å™¨å†³ç­–è·¯ç”±
        workflow.add_conditional_edges(
            "orchestrator",
            self._orchestrator_route,
            {
                "scout": "scout_work",
                "analyze": "analyzer_work",
                "individual_summary": "individual_summary_work",
                "final_summary": "final_summary_work",
                "complete": END
            }
        )
        
        # å„èŠ‚ç‚¹å®Œæˆåè¿”å›ç¼–æ’å™¨
        for node in ["scout_work", "analyzer_work", 
                     "individual_summary_work", "final_summary_work"]:
            workflow.add_edge(node, "orchestrator")
        
        return workflow.compile()
    
    def _orchestrator_route(self, state: WorkState) -> str:
        """ç¼–æ’å™¨å†³ç­–ä¸‹ä¸€æ­¥è¡ŒåŠ¨ - åŸºäºå·²å®Œæˆä»»åŠ¡å’Œç®—å­ç´¢å¼•"""
        algorithms = state["algorithms"]
        current_idx = state["current_algorithm_index"]
        completed = state["completed_tasks"]
        
        # é¦–å…ˆæ£€æŸ¥å½“å‰ç®—å­æ˜¯å¦å®Œæˆï¼Œå¦‚æœå®Œæˆåˆ™ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ª
        if current_idx < len(algorithms):
            current_algo = algorithms[current_idx]
            if (f"scout_{current_algo}" in completed and 
                f"analyze_{current_algo}" in completed and 
                f"individual_summary_{current_algo}" in completed):
                # å½“å‰ç®—å­å®Œæˆï¼Œç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ª
                print(f"âœ… {current_algo} ç®—å­å®Œæˆï¼ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªç®—å­...")
                state["current_algorithm_index"] += 1
                current_idx = state["current_algorithm_index"]
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ç®—å­éƒ½å®Œæˆ
        if current_idx >= len(algorithms):
            # æ£€æŸ¥æ˜¯å¦éœ€è¦final summary
            if "final_summary" not in completed:
                return "final_summary"
            return "complete"
        
        current_algo = algorithms[current_idx]
        
        # æŒ‰å›ºå®šé¡ºåºï¼šscout â†’ analyze â†’ individual_summary
        if f"scout_{current_algo}" not in completed:
            return "scout"
        elif f"analyze_{current_algo}" not in completed:
            return "analyze"
        elif f"individual_summary_{current_algo}" not in completed:
            return "individual_summary"
        else:
            # è¿™ä¸ªåˆ†æ”¯ä¸åº”è¯¥åˆ°è¾¾ï¼Œå› ä¸ºä¸Šé¢å·²ç»å¤„ç†äº†ç®—å­å®Œæˆçš„æƒ…å†µ
            return "complete"
    
    def orchestrator_node(self, state: WorkState) -> WorkState:
        """ç¼–æ’å™¨èŠ‚ç‚¹ - æ˜¾ç¤ºçŠ¶æ€ä½†ä¸åšå†³ç­–"""
        print(f"\nğŸ¯ [Orchestrator] åˆ†æå·¥ä½œæµçŠ¶æ€...")
        
        algorithms = state["algorithms"]
        current_idx = state["current_algorithm_index"]
        completed = state["completed_tasks"]
        
        if current_idx < len(algorithms):
            current_algo = algorithms[current_idx]
            
            # æ˜¾ç¤ºè¿›åº¦
            total_tasks = len(algorithms) * 3 + 1  # æ¯ä¸ªç®—å­3ä¸ªä»»åŠ¡ + 1ä¸ªfinal
            print(f"ğŸ“Š è¿›åº¦: {len(completed)}/{total_tasks} ä»»åŠ¡å®Œæˆ")
            print(f"ğŸ“ å½“å‰ç®—å­: {current_algo} ({current_idx + 1}/{len(algorithms)})")
        else:
            print(f"ğŸ“Š æ‰€æœ‰ç®—å­å®Œæˆï¼Œå‡†å¤‡æœ€ç»ˆæ€»ç»“")
        
        return state
    
    def scout_work(self, state: WorkState) -> WorkState:
        """Scoutå·¥ä½œ - ç›´æ¥æ‰«ækernelç›®å½•ï¼ŒæŒ‰ç®—å­ç§ç±»åˆ†ç»„æ‰€æœ‰æ–‡ä»¶"""
        report_folder = state["report_folder"]
        
        try:
            print(f"ğŸ” æ‰«ækernelç›®å½•...")
            
            discovery_path = self.file_mgr.get_discovery_output_path(report_folder, "all_algorithms")
            
            # ç›´æ¥åœ¨Pythonä¸­å¤„ç†æ–‡ä»¶æ‰«æï¼Œé¿å…å¤§æ¨¡å‹è°ƒç”¨
            all_algorithms = self._scan_and_classify_files()
            
            # æ„å»ºæœ€ç»ˆç»“æœ
            final_discovery = {
                "algorithms": list(all_algorithms.values()),
                "total_algorithms": len(all_algorithms),
                "total_files": sum(len(algo["files"]) for algo in all_algorithms.values()),
                "timestamp": time.strftime('%Y-%m-%d %H:%M:%S')
            }
            
            # ä¿å­˜å‘ç°ç»“æœ
            success = FileManager.save_content(discovery_path, json.dumps(final_discovery, ensure_ascii=False, indent=2))
            if success:
                # æ›´æ–°stateä¸­çš„algorithmsåˆ—è¡¨
                algorithm_names = list(all_algorithms.keys())
                state["algorithms"] = algorithm_names
                print(f"âœ… å‘ç° {len(algorithm_names)} ç§ç®—å­ï¼Œå…± {final_discovery['total_files']} ä¸ªæ–‡ä»¶")
                
                state["completed_tasks"].append("scout_all")
            else:
                print(f"âŒ ä¿å­˜å¤±è´¥")
                state["errors"].append("Scoutä¿å­˜å¤±è´¥")
            
        except Exception as e:
            error_msg = f"Scoutå¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            state["errors"].append(error_msg)
        
        return state
    
    def _scan_and_classify_files(self) -> Dict[str, Dict]:
        """ç›´æ¥æ‰«æå¹¶åˆ†ç±»æ–‡ä»¶"""
        import os
        import re
        
        kernel_path = "openblas-output/GENERIC/kernel"
        if not os.path.exists(kernel_path):
            return {}
        
        # è·å–æ‰€æœ‰.cæ–‡ä»¶
        all_files = []
        for file in os.listdir(kernel_path):
            if file.endswith('.c') and 'clean' in file:
                all_files.append(file)
        
        all_files.sort()
        print(f"  æ‰¾åˆ° {len(all_files)} ä¸ªæ–‡ä»¶")
        
        # ç®—å­åˆ†ç±»è§„åˆ™
        algorithm_patterns = {
            'axpy': r'.*axpy.*',
            'gemm': r'.*gemm.*',
            'dot': r'.*(dot|dotu|dotc).*',
            'asum': r'.*asum.*',
            'nrm2': r'.*nrm2.*',
            'scal': r'.*scal.*',
            'copy': r'.*copy.*',
            'swap': r'.*swap.*',
            'amax': r'.*amax.*',
            'amin': r'.*amin.*',
            'ger': r'.*ger.*',
            'gemv': r'.*gemv.*',
            'symv': r'.*symv.*',
            'hemv': r'.*hemv.*',
            'trmm': r'.*trmm.*',
            'trsm': r'.*trsm.*',
            'symm': r'.*symm.*',
            'hemm': r'.*hemm.*',
            'rot': r'.*rot.*',
            'rotm': r'.*rotm.*',
            'geadd': r'.*geadd.*',
            'imatcopy': r'.*imatcopy.*',
            'omatcopy': r'.*omatcopy.*',
            'laswp': r'.*laswp.*',
            'max': r'.*max.*',
            'min': r'.*min.*',
            'sum': r'.*sum.*',
            'neg': r'.*neg.*'
        }
        
        # åˆ†ç±»æ–‡ä»¶
        algorithms = {}
        for filename in all_files:
            classified = False
            for algo_name, pattern in algorithm_patterns.items():
                if re.match(pattern, filename, re.IGNORECASE):
                    if algo_name not in algorithms:
                        algorithms[algo_name] = {"algorithm": algo_name, "files": []}
                    algorithms[algo_name]["files"].append({
                        "name": filename
                    })
                    classified = True
                    break
            
            # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°å·²çŸ¥æ¨¡å¼ï¼Œå°è¯•æå–ç®—å­å
            if not classified:
                # ç®€å•çš„å¯å‘å¼ï¼šå–æ–‡ä»¶åçš„ç¬¬ä¸€ä¸ªå•è¯éƒ¨åˆ†
                base_name = filename.replace('.clean.c', '')
                # ç§»é™¤å‰ç¼€å­—æ¯ï¼ˆå¦‚s, d, c, zï¼‰
                if len(base_name) > 1 and base_name[0] in 'sdcz':
                    potential_algo = base_name[1:]
                else:
                    potential_algo = base_name
                
                # è¿›ä¸€æ­¥æ¸…ç†
                potential_algo = re.sub(r'_.*', '', potential_algo)  # ç§»é™¤ä¸‹åˆ’çº¿åçš„éƒ¨åˆ†
                
                if len(potential_algo) > 2:  # åªè€ƒè™‘é•¿åº¦å¤§äº2çš„ç®—å­å
                    if potential_algo not in algorithms:
                        algorithms[potential_algo] = {"algorithm": potential_algo, "files": []}
                    algorithms[potential_algo]["files"].append({
                        "name": filename
                    })
        
        return algorithms
    
    def _discover_algorithm_files(self, algorithm: str) -> List[Dict[str, str]]:
        """åŠ¨æ€å‘ç°ç®—å­ç›¸å…³æ–‡ä»¶"""
        import glob
        import re
        
        base_dir = "openblas-output/GENERIC/kernel"
        if not os.path.exists(base_dir):
            return []
        
        # æœç´¢æ¨¡å¼ï¼šç®—å­åç›¸å…³çš„æ–‡ä»¶
        patterns = [
            f"*{algorithm}*.c",
            f"*{algorithm.upper()}*.c",
            f"{algorithm}_*.c",
            f"{algorithm.upper()}_*.c",
            f"*_{algorithm}.c",
            f"*_{algorithm.upper()}.c"
        ]
        
        found_files = []
        for pattern in patterns:
            files = glob.glob(os.path.join(base_dir, pattern))
            found_files.extend(files)
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡ï¼ˆé€‰æ‹©å‰5ä¸ªæœ€ç›¸å…³çš„ï¼‰
        unique_files = list(set(found_files))
        
        # æŒ‰ç›¸å…³æ€§æ’åºï¼ˆæ–‡ä»¶ååŒ…å«ç®—å­åçš„ä¼˜å…ˆï¼‰
        def relevance_score(filepath):
            filename = os.path.basename(filepath).lower()
            algo_lower = algorithm.lower()
            
            if filename.startswith(algo_lower):
                return 3
            elif algo_lower in filename:
                return 2
            elif algorithm.upper() in os.path.basename(filepath):
                return 1
            else:
                return 0
        
        unique_files.sort(key=relevance_score, reverse=True)
        
        # é€‰æ‹©å‰5ä¸ªæœ€ç›¸å…³çš„æ–‡ä»¶
        selected_files = unique_files[:5]
        
        # è½¬æ¢ä¸ºæ‰€éœ€æ ¼å¼
        result = []
        for filepath in selected_files:
            filename = os.path.basename(filepath)
            result.append({
                "path": filename,
                "type": "discovered",
                "description": f"åŠ¨æ€å‘ç°çš„{algorithm}ç›¸å…³æ–‡ä»¶"
            })
        
        return result
    
    def analyzer_work(self, state: WorkState) -> WorkState:
        """Analyzerå·¥ä½œ - åˆ†æç®—å­æ–‡ä»¶"""
        current_algo = state["algorithms"][state["current_algorithm_index"]]
        report_folder = state["report_folder"]
        
        try:
            # ä»all_algorithms discoveryæ–‡ä»¶ä¸­è·å–å½“å‰ç®—å­çš„æ–‡ä»¶åˆ—è¡¨
            discovery_path = self.file_mgr.get_discovery_output_path(report_folder, "all_algorithms")
            with open(discovery_path, 'r', encoding='utf-8') as f:
                discovery_data = json.load(f)
            
            # æ‰¾åˆ°å½“å‰ç®—å­çš„æ–‡ä»¶åˆ—è¡¨
            input_files = []
            if "algorithms" in discovery_data:
                for algo_info in discovery_data["algorithms"]:
                    if algo_info.get("algorithm") == current_algo:
                        input_files = algo_info.get("files", [])
                        break
            
            if not input_files:
                raise ValueError(f"æœªæ‰¾åˆ°{current_algo}ç®—å­çš„æ–‡ä»¶åˆ—è¡¨")
            # è·å–åˆ†æç»“æœæ–‡ä»¶è·¯å¾„
            analysis_path = self.file_mgr.get_analysis_output_path(report_folder, current_algo)
            
            # è¯»å–å·²æœ‰çš„åˆ†æç»“æœï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            existing_analyses = []
            if os.path.exists(analysis_path):
                try:
                    with open(analysis_path, 'r', encoding='utf-8') as f:
                        existing_data = json.load(f)
                        existing_analyses = existing_data.get("individual_analyses", [])
                except:
                    existing_analyses = []
            
            # é€ä¸ªåˆ†ææ¯ä¸ªæ–‡ä»¶å¹¶å¢é‡ä¿å­˜
            for i, file_info in enumerate(input_files):
                file_name = file_info.get("name", "")
                if not file_name:
                    continue
                
                print(f"  ğŸ“„ åˆ†ææ–‡ä»¶ {i+1}/{len(input_files)}: {file_name}")
                
                analyzer_input = f"""åˆ†æ{current_algo}ç®—å­æ–‡ä»¶: {file_name}

æŒ‰ä¸‰å±‚ä¼˜åŒ–ç­–ç•¥æ¡†æ¶è¿›è¡Œç»“æ„åŒ–åˆ†æï¼Œæ¯ä¸ªå±‚æ¬¡è¾“å‡ºå¤šä¸ªä¼˜åŒ–ç­–ç•¥ã€‚

**åˆ†æè¦æ±‚**ï¼š
- å®Œå…¨åŸºäºä»£ç å†…å®¹å‘ç°ä¼˜åŒ–ç­–ç•¥
- æ¯ä¸ªç­–ç•¥å¿…é¡»åŒ…å«è¯¦ç»†çš„ç»“æ„åŒ–ä¿¡æ¯
- æä¾›è¶³å¤Ÿçš„ä»£ç ä¸Šä¸‹æ–‡æ¥æ”¯æ’‘ä¼˜åŒ–ç­–ç•¥çš„è§£é‡Š

**ä¸‰å±‚ä¼˜åŒ–æ¡†æ¶**ï¼š
**ç®—æ³•è®¾è®¡å±‚æ¬¡**ï¼šè¯†åˆ«ç®—æ³•å±‚ä¼˜åŒ–ç­–ç•¥ï¼ˆå¦‚"å¤æ•°è¿ç®—å±•å¼€"ã€"åˆ†å—è®¡ç®—"ã€"é¢„è®¡ç®—ä¼˜åŒ–"ç­‰ï¼‰
**ä»£ç ä¼˜åŒ–å±‚æ¬¡**ï¼šè¯†åˆ«ä»£ç å±‚ä¼˜åŒ–ç­–ç•¥ï¼ˆå¦‚"å¾ªç¯å±•å¼€"ã€"æŒ‡é’ˆé€’å¢"ã€"æ¡ä»¶åˆ†æ”¯ä¼˜åŒ–"ç­‰ï¼‰  
**ç‰¹æœ‰æŒ‡ä»¤å±‚æ¬¡**ï¼šè¯†åˆ«æŒ‡ä»¤å±‚ä¼˜åŒ–ç­–ç•¥ï¼ˆå¦‚"SIMDå‘é‡åŒ–"ã€"è‡ªåŠ¨å‘é‡åŒ–é€‚é…"ã€"å†…è”æ±‡ç¼–"ç­‰ï¼‰

**è¾“å‡ºæ ¼å¼è¦æ±‚**ï¼š
æ¯ä¸ªç­–ç•¥åŒ…å«ï¼š
- name: è§„èŒƒåŒ–ç­–ç•¥åç§°
- description_details: åŒ…å«strategy_rationaleã€implementation_patternã€performance_impactã€trade_offså››ä¸ªå­—æ®µ
- code_context: åŒ…å«snippetï¼ˆå®Œæ•´ä»£ç å—ï¼‰ã€highlighted_codeï¼ˆæ ¸å¿ƒè¯­å¥ï¼‰ã€explanationä¸‰ä¸ªå­—æ®µ

**ç­–ç•¥å‘½åè¦æ±‚**ï¼šä½¿ç”¨è§„èŒƒç®€ç»ƒçš„æŠ€æœ¯æœ¯è¯­ï¼Œé¿å…å£è¯­åŒ–è¡¨è¾¾

ä½¿ç”¨read_source_fileå·¥å…·è¯»å–æ–‡ä»¶å†…å®¹ï¼Œç„¶åè¾“å‡ºç»“æ„åŒ–çš„JSONæ ¼å¼åˆ†æç»“æœã€‚"""
                
                result = self.analyzer.invoke({"input": analyzer_input})
                time.sleep(2)
                
                file_analysis = self._extract_json_from_result(result)
                existing_analyses.append(file_analysis)
                
                # æ¯åˆ†æä¸€ä¸ªæ–‡ä»¶å°±ä¿å­˜ä¸€æ¬¡ï¼ˆå¢é‡ä¿å­˜ï¼‰
                updated_analysis = {
                    "algorithm": current_algo,
                    "total_files": len(input_files),
                    "analyzed_files": len(existing_analyses),
                    "individual_analyses": existing_analyses,
                    "timestamp": datetime.now().isoformat()
                }
                
                success = FileManager.save_content(analysis_path, json.dumps(updated_analysis, ensure_ascii=False, indent=2))
                if not success:
                    print(f"    âŒ ä¿å­˜å¤±è´¥: {analysis_path}")
                print(f"    âœ… å¢é‡ä¿å­˜: {os.path.basename(analysis_path)} (å·²åˆ†æ {len(existing_analyses)}/{len(input_files)} ä¸ªæ–‡ä»¶)")
            state["completed_tasks"].append(f"analyze_{current_algo}")
            print(f"âœ… Analyzerå®Œæˆ: {analysis_path}")
            
        except Exception as e:
            print(f"âŒ Analyzerå¤±è´¥ ({current_algo}): {str(e)}")
            state["errors"].append(str(e))
        
        return state
    
    
    def individual_summary_work(self, state: WorkState) -> WorkState:
        """Individual Summaryå·¥ä½œ - å•ç®—å­å¢é‡æ•´åˆ"""
        current_algo = state["algorithms"][state["current_algorithm_index"]]
        report_folder = state["report_folder"]
        
        try:
            analysis_path = self.file_mgr.get_analysis_output_path(report_folder, current_algo)
            summary_path = self.file_mgr.get_individual_summary_path(report_folder, current_algo)
            
            # è¯»å–åˆ†æç»“æœ
            with open(analysis_path, 'r', encoding='utf-8') as f:
                analysis_data = json.load(f)
            
            individual_analyses = analysis_data.get("individual_analyses", [])
            if not individual_analyses:
                raise ValueError(f"{current_algo}ç®—å­æ²¡æœ‰åˆ†æç»“æœ")
            
            # åˆå§‹åŒ–ï¼šå°†ç¬¬ä¸€ä¸ªç»“æ„åŒ–åˆ†æç»“æœè½¬æ¢ä¸ºsummaryæ ¼å¼
            print(f"  ğŸ”„ åˆå§‹åŒ–summary (åŸºäºç¬¬1ä¸ªæ–‡ä»¶)")
            first_analysis = individual_analyses[0]
            
            # è¾…åŠ©å‡½æ•°ï¼šå°†ç»“æ„åŒ–ç­–ç•¥è½¬æ¢ä¸ºsummaryæ ¼å¼
            def convert_to_summary_format(strategies):
                summary_strategies = []
                for strategy in strategies:
                    if isinstance(strategy, dict) and "name" in strategy:
                        if "description_details" in strategy:
                            # ä»ç»“æ„åŒ–æ ¼å¼æå–æ ¸å¿ƒå†…å®¹
                            details = strategy["description_details"]
                            unified_desc = f"{details.get('strategy_rationale', '')} {details.get('implementation_pattern', '')} {details.get('performance_impact', '')}".strip()
                        else:
                            # å…¼å®¹æ—§æ ¼å¼
                            unified_desc = strategy.get("description", "")
                        
                        summary_strategies.append({
                            "name": strategy["name"],
                            "unified_description": unified_desc
                        })
                return summary_strategies
            
            current_summary = {
                "algorithm": current_algo,
                "algorithm_level_optimizations": convert_to_summary_format(first_analysis.get("algorithm_level_optimizations", [])),
                "code_level_optimizations": convert_to_summary_format(first_analysis.get("code_level_optimizations", [])),
                "instruction_level_optimizations": convert_to_summary_format(first_analysis.get("instruction_level_optimizations", [])),
                "timestamp": datetime.now().isoformat()
            }
            
            # ä¿å­˜åˆå§‹summary
            FileManager.save_content(summary_path, json.dumps(current_summary, ensure_ascii=False, indent=2))
            print(f"    âœ… åˆå§‹summaryå·²ä¿å­˜")
            
            # é€ä¸ªæ•´åˆåç»­çš„åˆ†æç»“æœ
            for i, analysis in enumerate(individual_analyses[1:], start=2):
                print(f"  ğŸ”„ æ•´åˆç¬¬{i}ä¸ªæ–‡ä»¶çš„åˆ†æç»“æœ...")
                
                summary_input = f"""å¢é‡æ•´åˆ{current_algo}ç®—å­çš„ä¼˜åŒ–ç­–ç•¥ï¼š

**å·²æœ‰çš„ä¼˜åŒ–ç­–ç•¥**ï¼š
ç®—æ³•è®¾è®¡å±‚æ¬¡: {json.dumps(current_summary.get('algorithm_level_optimizations', []), ensure_ascii=False, indent=2)}
ä»£ç ä¼˜åŒ–å±‚æ¬¡: {json.dumps(current_summary.get('code_level_optimizations', []), ensure_ascii=False, indent=2)}
ç‰¹æœ‰æŒ‡ä»¤å±‚æ¬¡: {json.dumps(current_summary.get('instruction_level_optimizations', []), ensure_ascii=False, indent=2)}

**æ–°çš„ç»“æ„åŒ–åˆ†æç»“æœ**ï¼š
ç®—æ³•è®¾è®¡å±‚æ¬¡: {json.dumps(analysis.get('algorithm_level_optimizations', []), ensure_ascii=False, indent=2)}
ä»£ç ä¼˜åŒ–å±‚æ¬¡: {json.dumps(analysis.get('code_level_optimizations', []), ensure_ascii=False, indent=2)}
ç‰¹æœ‰æŒ‡ä»¤å±‚æ¬¡: {json.dumps(analysis.get('instruction_level_optimizations', []), ensure_ascii=False, indent=2)}

**ä»»åŠ¡**ï¼šå°†ç»“æ„åŒ–åˆ†æç»“æœæ•´åˆåˆ°å·²æœ‰ç­–ç•¥ä¸­ï¼Œæç‚¼ç›¸è¿‘ç­–ç•¥å¹¶ç»Ÿä¸€å‘½åã€‚

**ç»“æ„åŒ–æ•°æ®å¤„ç†è¯´æ˜**ï¼š
- åˆ†æç»“æœä¸­æ¯ä¸ªç­–ç•¥åŒ…å«nameã€description_detailså’Œcode_context
- ä½ éœ€è¦æå–description_detailsçš„æ ¸å¿ƒå†…å®¹ï¼Œå½¢æˆunified_description
- åˆå¹¶ç›¸ä¼¼ç­–ç•¥æ—¶ï¼Œç»¼åˆå¤šä¸ªdescription_detailsçš„è¦ç‚¹

**æ•´åˆè§„åˆ™**ï¼š
- å¦‚æœæ–°ç­–ç•¥ä¸å·²æœ‰ç­–ç•¥ç›¸ä¼¼ï¼Œåˆå¹¶ä¸ºç»Ÿä¸€å‘½åçš„ç­–ç•¥ï¼ˆä¿æŒå·²æœ‰åç§°ï¼‰
- å¦‚æœæ–°ç­–ç•¥æ˜¯å…¨æ–°çš„ï¼Œç›´æ¥æ·»åŠ åˆ°ç­–ç•¥åˆ—è¡¨ä¸­
- ä¿æŒç­–ç•¥åç§°çš„è§„èŒƒåŒ–å’Œä¸€è‡´æ€§
- ä»è¯¦ç»†çš„description_detailsä¸­æå–æ ¸å¿ƒå†…å®¹ï¼Œå½¢æˆç®€æ´çš„unified_description

**è¾“å‡ºæ ¼å¼**ï¼šæ¯ä¸ªå±‚æ¬¡è¾“å‡ºç­–ç•¥åˆ—è¡¨ï¼Œæ ¼å¼ä¸ºï¼š
[{{"name": "ç»Ÿä¸€çš„ç­–ç•¥åç§°", "unified_description": "åˆå¹¶å¤šä¸ªç›¸ä¼¼ç­–ç•¥åçš„ç»Ÿä¸€æè¿°"}}]

è¾“å‡ºJSONæ ¼å¼ç»“æœï¼ŒåªåŒ…å«ä¸‰ä¸ªå­—æ®µï¼šalgorithm_level_optimizations, code_level_optimizations, instruction_level_optimizations"""
                
                result = self.individual_summarizer.invoke({"input": summary_input})
                time.sleep(2)
                
                # æå–JSONç»“æœ
                updated_summary = self._extract_json_from_result(result)
                
                # æ›´æ–°current_summary
                current_summary["algorithm_level_optimizations"] = updated_summary.get("algorithm_level_optimizations", current_summary["algorithm_level_optimizations"])
                current_summary["code_level_optimizations"] = updated_summary.get("code_level_optimizations", current_summary["code_level_optimizations"])
                current_summary["instruction_level_optimizations"] = updated_summary.get("instruction_level_optimizations", current_summary["instruction_level_optimizations"])
                current_summary["timestamp"] = datetime.now().isoformat()
                
                # å¢é‡ä¿å­˜
                FileManager.save_content(summary_path, json.dumps(current_summary, ensure_ascii=False, indent=2))
                print(f"    âœ… å·²æ•´åˆå¹¶ä¿å­˜ (è¿›åº¦: {i}/{len(individual_analyses)})")
            
            state["completed_tasks"].append(f"individual_summary_{current_algo}")
            print(f"âœ… Individual Summaryå®Œæˆ: {summary_path}")
            
        except Exception as e:
            print(f"âŒ Individual Summaryå¤±è´¥ ({current_algo}): {str(e)}")
            state["errors"].append(str(e))
        
        return state
    
    def final_summary_work(self, state: WorkState) -> WorkState:
        """Final Summaryå·¥ä½œ - è·¨ç®—å­å¢é‡æ•´åˆ"""
        algorithms = state["algorithms"]
        report_folder = state["report_folder"]
        
        try:
            final_path = self.file_mgr.get_final_summary_path(report_folder)
            
            if not algorithms:
                raise ValueError("æ²¡æœ‰å·²å®Œæˆçš„ç®—å­")
            
            # åˆå§‹åŒ–ï¼šå°†ç¬¬ä¸€ä¸ªç®—å­çš„summaryè½¬æ¢ä¸ºfinalæ ¼å¼
            print(f"  ğŸ”„ åˆå§‹åŒ–final summary (åŸºäºç¬¬1ä¸ªç®—å­: {algorithms[0]})")
            first_summary_path = self.file_mgr.get_individual_summary_path(report_folder, algorithms[0])
            with open(first_summary_path, 'r', encoding='utf-8') as f:
                first_summary = json.load(f)
            
            # è¾…åŠ©å‡½æ•°ï¼šå°†summaryæ ¼å¼è½¬æ¢ä¸ºfinalæ ¼å¼
            def convert_to_final_format(strategies):
                final_strategies = []
                for strategy in strategies:
                    if isinstance(strategy, dict) and "name" in strategy:
                        universal_desc = strategy.get("unified_description", strategy.get("description", ""))
                        final_strategies.append({
                            "name": strategy["name"],
                            "universal_description": universal_desc
                        })
                return final_strategies
            
            current_final = {
                "analyzed_algorithms": [algorithms[0]],
                "algorithm_level_optimizations": convert_to_final_format(first_summary.get("algorithm_level_optimizations", [])),
                "code_level_optimizations": convert_to_final_format(first_summary.get("code_level_optimizations", [])),
                "instruction_level_optimizations": convert_to_final_format(first_summary.get("instruction_level_optimizations", [])),
                "timestamp": datetime.now().isoformat()
            }
            
            # ä¿å­˜åˆå§‹final summary
            FileManager.save_content(final_path, json.dumps(current_final, ensure_ascii=False, indent=2))
            print(f"    âœ… åˆå§‹final summaryå·²ä¿å­˜")
            
            # é€ä¸ªæ•´åˆåç»­ç®—å­çš„summary
            for i, algo in enumerate(algorithms[1:], start=2):
                print(f"  ğŸ”„ æ•´åˆç¬¬{i}ä¸ªç®—å­: {algo}...")
                
                # è¯»å–å½“å‰ç®—å­çš„summary
                algo_summary_path = self.file_mgr.get_individual_summary_path(report_folder, algo)
                with open(algo_summary_path, 'r', encoding='utf-8') as f:
                    algo_summary = json.load(f)
                
                final_input = f"""å¢é‡æ•´åˆOpenBLASä¼˜åŒ–ç­–ç•¥åº“ï¼š

**å·²æœ‰çš„ä¼˜åŒ–ç­–ç•¥åº“**ï¼š
ç®—æ³•è®¾è®¡å±‚æ¬¡: {json.dumps(current_final.get('algorithm_level_optimizations', []), ensure_ascii=False, indent=2)}
ä»£ç ä¼˜åŒ–å±‚æ¬¡: {json.dumps(current_final.get('code_level_optimizations', []), ensure_ascii=False, indent=2)}
ç‰¹æœ‰æŒ‡ä»¤å±‚æ¬¡: {json.dumps(current_final.get('instruction_level_optimizations', []), ensure_ascii=False, indent=2)}

**æ–°ç®—å­({algo})çš„ä¼˜åŒ–ç­–ç•¥**ï¼š
ç®—æ³•è®¾è®¡å±‚æ¬¡: {json.dumps(algo_summary.get('algorithm_level_optimizations', []), ensure_ascii=False, indent=2)}
ä»£ç ä¼˜åŒ–å±‚æ¬¡: {json.dumps(algo_summary.get('code_level_optimizations', []), ensure_ascii=False, indent=2)}
ç‰¹æœ‰æŒ‡ä»¤å±‚æ¬¡: {json.dumps(algo_summary.get('instruction_level_optimizations', []), ensure_ascii=False, indent=2)}

**ä»»åŠ¡**ï¼šå°†æ–°ç®—å­çš„ä¼˜åŒ–ç­–ç•¥æ•´åˆåˆ°å·²æœ‰ç­–ç•¥åº“ä¸­ï¼Œæç‚¼è·¨ç®—å­çš„é€šç”¨ä¼˜åŒ–æ¨¡å¼å¹¶ç»Ÿä¸€å‘½åã€‚

**è·¨ç®—å­æ•´åˆè¯´æ˜**ï¼š
- è¾“å…¥çš„ç®—å­ç­–ç•¥æ ¼å¼ï¼šnameï¼ˆç»Ÿä¸€åç§°ï¼‰å’Œunified_descriptionï¼ˆç»Ÿä¸€æè¿°ï¼‰
- è¾“å‡ºçš„ç­–ç•¥åº“æ ¼å¼ï¼šnameï¼ˆé€šç”¨ç­–ç•¥åç§°ï¼‰å’Œuniversal_descriptionï¼ˆé€šç”¨æè¿°å’Œåº”ç”¨åœºæ™¯ï¼‰
- é‡ç‚¹è¯†åˆ«åœ¨å¤šä¸ªç®—å­ä¸­éƒ½å‡ºç°çš„ä¼˜åŒ–æ¨¡å¼
- æç‚¼é€‚ç”¨äºæ•´ä¸ªOpenBLASåº“çš„é€šç”¨ä¼˜åŒ–ç­–ç•¥

**æ•´åˆè§„åˆ™**ï¼š
- å¦‚æœæ–°ç­–ç•¥ä¸å·²æœ‰ç­–ç•¥ç›¸ä¼¼ï¼Œåˆå¹¶ä¸ºç»Ÿä¸€å‘½åçš„ç­–ç•¥ï¼ˆä¿æŒå·²æœ‰åç§°ï¼‰
- å¦‚æœæ–°ç­–ç•¥æ˜¯å…¨æ–°çš„ï¼Œç›´æ¥æ·»åŠ åˆ°ç­–ç•¥åˆ—è¡¨ä¸­
- ä¿æŒç­–ç•¥åç§°çš„è§„èŒƒåŒ–å’Œä¸€è‡´æ€§
- ä»å¤šä¸ªç®—å­çš„å…±æ€§ä¸­æç‚¼é€šç”¨çš„ä¼˜åŒ–è§„å¾‹

**ç­–ç•¥å‘½åè§„èŒƒ**ï¼š
- ä½¿ç”¨æ ‡å‡†æŠ€æœ¯æœ¯è¯­ï¼ˆå¦‚"SIMDå‘é‡åŒ–"ã€"åˆ†å—è®¡ç®—"ã€"å¹¶è¡Œè®¡ç®—"ï¼‰
- é¿å…å£è¯­åŒ–è¡¨è¾¾
- ä¿æŒå‘½åç®€ç»ƒå‡†ç¡®

**è¾“å‡ºæ ¼å¼**ï¼šæ¯ä¸ªå±‚æ¬¡è¾“å‡ºç­–ç•¥åˆ—è¡¨ï¼Œæ ¼å¼ä¸ºï¼š
[{{"name": "é€šç”¨çš„ç­–ç•¥åç§°", "universal_description": "é€šç”¨çš„ç­–ç•¥æè¿°å’Œè·¨ç®—å­åº”ç”¨åœºæ™¯"}}]

è¾“å‡ºJSONæ ¼å¼ç»“æœï¼ŒåªåŒ…å«ä¸‰ä¸ªå­—æ®µï¼šalgorithm_level_optimizations, code_level_optimizations, instruction_level_optimizations"""
                
                result = self.final_summarizer.invoke({"input": final_input})
                time.sleep(2)
                
                # æå–JSONç»“æœ
                updated_final = self._extract_json_from_result(result)
                
                # æ›´æ–°current_final
                current_final["algorithm_level_optimizations"] = updated_final.get("algorithm_level_optimizations", current_final["algorithm_level_optimizations"])
                current_final["code_level_optimizations"] = updated_final.get("code_level_optimizations", current_final["code_level_optimizations"])
                current_final["instruction_level_optimizations"] = updated_final.get("instruction_level_optimizations", current_final["instruction_level_optimizations"])
                current_final["analyzed_algorithms"].append(algo)
                current_final["timestamp"] = datetime.now().isoformat()
                
                # å¢é‡ä¿å­˜
                FileManager.save_content(final_path, json.dumps(current_final, ensure_ascii=False, indent=2))
                print(f"    âœ… å·²æ•´åˆå¹¶ä¿å­˜ (è¿›åº¦: {i}/{len(algorithms)})")
            
            state["completed_tasks"].append("final_summary")
            print(f"âœ… Final Summaryå®Œæˆ: {final_path}")
            
        except Exception as e:
            print(f"âŒ Final Summaryå¤±è´¥: {str(e)}")
            state["errors"].append(str(e))
        
        return state
    
    def _extract_json_from_result(self, result):
        """ä»Agentç»“æœä¸­æå–JSON"""
        if isinstance(result, dict) and "output" in result:
            output_content = result["output"]
            if "```json" in output_content:
                json_start = output_content.find("```json") + 7
                json_end = output_content.find("```", json_start)
                json_str = output_content[json_start:json_end].strip()
                try:
                    return json.loads(json_str)
                except:
                    return {"error": "JSONè§£æå¤±è´¥", "raw": json_str}
            elif "```" in output_content:
                json_start = output_content.find("```") + 3
                json_end = output_content.find("```", json_start)
                json_str = output_content[json_start:json_end].strip()
                try:
                    return json.loads(json_str)
                except:
                    return {"error": "JSONè§£æå¤±è´¥", "raw": json_str}
        elif isinstance(result, dict):
            return result
        
        return {"error": "æ— æ³•è§£æç»“æœ", "raw": str(result)}
    
    def _limit_files_for_quick_analysis(self, report_folder: str, algorithms: List[str]):
        """é™åˆ¶å¿«é€Ÿåˆ†ææ¨¡å¼ä¸‹æ¯ä¸ªç®—å­åªåˆ†æå‰5ä¸ªæ–‡ä»¶"""
        try:
            discovery_path = self.file_mgr.get_discovery_output_path(report_folder, "all_algorithms")
            with open(discovery_path, 'r', encoding='utf-8') as f:
                discovery_data = json.load(f)
            
            # ä¿®æ”¹æ¯ä¸ªç›®æ ‡ç®—å­çš„æ–‡ä»¶åˆ—è¡¨ï¼Œåªä¿ç•™å‰5ä¸ª
            modified = False
            for algo_info in discovery_data["algorithms"]:
                if algo_info["algorithm"] in algorithms:
                    original_count = len(algo_info["files"])
                    if original_count > 5:
                        algo_info["files"] = algo_info["files"][:5]
                        print(f"   {algo_info['algorithm']}: é™åˆ¶ä¸ºå‰5ä¸ªæ–‡ä»¶ (åŸæœ‰{original_count}ä¸ª)")
                        modified = True
                    else:
                        print(f"   {algo_info['algorithm']}: ä¿æŒ{original_count}ä¸ªæ–‡ä»¶")
            
            # å¦‚æœæœ‰ä¿®æ”¹ï¼Œä¿å­˜æ›´æ–°åçš„å‘ç°ç»“æœ
            if modified:
                FileManager.save_content(discovery_path, json.dumps(discovery_data, ensure_ascii=False, indent=2))
                print("âœ… å·²æ›´æ–°ç®—å­æ–‡ä»¶åˆ—è¡¨ä»¥é€‚é…å¿«é€Ÿåˆ†ææ¨¡å¼")
                
        except Exception as e:
            print(f"âš ï¸ é™åˆ¶æ–‡ä»¶æ•°é‡æ—¶å‡ºé”™: {str(e)}")
    
    def run(self, algorithms = None) -> dict:
        """è¿è¡Œå·¥ä½œæµ - æ”¯æŒå¿«é€Ÿåˆ†æå’Œå…¨éƒ¨åˆ†ææ¨¡å¼"""
        # åˆ›å»ºæŠ¥å‘Šæ–‡ä»¶å¤¹
        report_folder = f"results/{time.strftime('%Y%m%d_%H%M%S')}"
        self.file_mgr.ensure_directories(report_folder)
        
        print(f"ğŸ“ æŠ¥å‘Šæ–‡ä»¶å¤¹: {report_folder}")
        
        completed_algorithms = []
        all_errors = []
        
        # ç¬¬ä¸€æ­¥ï¼šScoutæ‰«ææ‰€æœ‰ç®—å­
        if algorithms is None or algorithms == "quick_analysis":
            scout_state = {
                "algorithms": [], 
                "current_algorithm_index": 0,
                "completed_tasks": [], 
                "report_folder": report_folder, 
                "errors": []
            }
            
            scout_result = self.scout_work(scout_state)
            if "scout_all" not in scout_result["completed_tasks"]:
                return {"success": False, "errors": scout_result["errors"]}
            
            # è·å–Scoutå‘ç°çš„ç®—å­åˆ—è¡¨
            discovered_algorithms = scout_result["algorithms"]
            
            # å¦‚æœæ˜¯å¿«é€Ÿåˆ†ææ¨¡å¼ï¼Œåªé€‰æ‹©æŒ‡å®šçš„ç®—å­
            if algorithms == "quick_analysis":
                target_algorithms = ['axpy', 'hemv', 'gemm']
                algorithms = [algo for algo in discovered_algorithms if algo in target_algorithms]
                print(f"ğŸš€ å¿«é€Ÿåˆ†ææ¨¡å¼ï¼šä» {len(discovered_algorithms)} ç§ç®—å­ä¸­é€‰æ‹© {len(algorithms)} ç§è¿›è¡Œåˆ†æ")
                print(f"   é€‰ä¸­çš„ç®—å­: {algorithms}")
                
                # é™åˆ¶æ¯ä¸ªç®—å­åªåˆ†æå‰5ä¸ªæ–‡ä»¶
                self._limit_files_for_quick_analysis(report_folder, algorithms)
            else:
                # å…¨éƒ¨åˆ†ææ¨¡å¼
                algorithms = discovered_algorithms
        
        # ç¬¬äºŒé˜¶æ®µï¼šé€ä¸ªå¤„ç†æ¯ä¸ªç®—æ³• (Analyzer -> Individual Summary)
        for i, algorithm in enumerate(algorithms):
            print(f"\nğŸ”„ åˆ†æç®—å­ {i+1}/{len(algorithms)}: {algorithm}")
            
            try:
                # ä¸ºæ¯ä¸ªç®—æ³•è¿è¡Œä¸¤ä¸ªé˜¶æ®µï¼ˆAnalyzer -> Individual Summaryï¼‰
                single_result = self.run_single_algorithm_phases(algorithm, report_folder, algorithms)
                
                if single_result["success"]:
                    completed_algorithms.append(algorithm)
                    print(f"âœ… {algorithm} åˆ†æå®Œæˆ")
                else:
                    print(f"âš ï¸ {algorithm} åˆ†ææœªå®Œå…¨å®Œæˆ")
                    all_errors.extend(single_result["errors"])
                    
            except Exception as e:
                error_msg = f"{algorithm} åˆ†æå¤±è´¥: {str(e)}"
                print(f"âŒ {error_msg}")
                all_errors.append(error_msg)
        
        # ç¬¬äºŒé˜¶æ®µï¼šå¦‚æœæœ‰ç®—æ³•æˆåŠŸå®Œæˆï¼Œè¿›è¡Œæœ€ç»ˆæ€»ç»“
        final_summary_completed = False
        if completed_algorithms:
            print(f"\nğŸ“ æœ€ç»ˆæ€»ç»“: æ•´åˆ {len(completed_algorithms)} ä¸ªç®—æ³•")
            
            try:
                # è¿è¡Œæœ€ç»ˆæ€»ç»“
                final_summary_result = self.run_final_summary(completed_algorithms, report_folder)
                final_summary_completed = final_summary_result["success"]
                
                if final_summary_completed:
                    print(f"âœ… æœ€ç»ˆæ€»ç»“å®Œæˆ")
                else:
                    print(f"âŒ æœ€ç»ˆæ€»ç»“å¤±è´¥")
                    all_errors.extend(final_summary_result["errors"])
                    
            except Exception as e:
                error_msg = f"æœ€ç»ˆæ€»ç»“å¤±è´¥: {str(e)}"
                print(f"âŒ {error_msg}")
                all_errors.append(error_msg)
        
        return {
            "success": len(completed_algorithms) > 0,
            "completed_algorithms": completed_algorithms,
            "final_summary_completed": final_summary_completed,
            "report_folder": report_folder,
            "errors": all_errors
        }
    
    def run_single_algorithm_phases(self, algorithm: str, report_folder: str, all_algorithms: List[str] = None) -> dict:
        """è¿è¡Œå•ä¸ªç®—æ³•çš„ä¸‰ä¸ªé˜¶æ®µï¼šScout -> Analyzer -> Individual Summary"""
        errors = []
        
        try:
            # ä½¿ç”¨ä¼ å…¥çš„ç®—å­åˆ—è¡¨ï¼Œè·³è¿‡Scouté˜¶æ®µï¼ˆå·²åœ¨runæ–¹æ³•ä¸­å®Œæˆï¼‰
            if all_algorithms is None:
                all_algorithms = [algorithm]
            
            # é˜¶æ®µ1ï¼šAnalyzeråˆ†æä»£ç 
            print(f"  ğŸ“Š åˆ†æä»£ç ...")
            analyzer_state = {
                "algorithms": all_algorithms,
                "current_algorithm_index": all_algorithms.index(algorithm),
                "completed_tasks": ["scout_all"],
                "report_folder": report_folder,
                "errors": []
            }
            analyzer_result = self.analyzer_work(analyzer_state)
            
            if f"analyze_{algorithm}" not in analyzer_result["completed_tasks"]:
                return {"success": False, "errors": analyzer_result["errors"]}
            
            # é˜¶æ®µ2ï¼šIndividual Summaryæ€»ç»“
            print(f"  ğŸ“ ç­–ç•¥æ€»ç»“...")
            summary_result = self.individual_summary_work(analyzer_result)
            
            if f"individual_summary_{algorithm}" not in summary_result["completed_tasks"]:
                return {"success": False, "errors": summary_result["errors"]}
            
            return {"success": True, "errors": []}
            
        except Exception as e:
            error_msg = f"{algorithm} é˜¶æ®µæ‰§è¡Œå¤±è´¥: {str(e)}"
            errors.append(error_msg)
            return {"success": False, "errors": errors}
    
    def run_final_summary(self, completed_algorithms: List[str], report_folder: str) -> dict:
        """è¿è¡Œæœ€ç»ˆæ€»ç»“é˜¶æ®µ"""
        try:
            print(f"ğŸ“ [Final Summary] æ•´åˆæ‰€æœ‰ç®—å­çš„ä¼˜åŒ–ç­–ç•¥...")
            
            # åˆ›å»ºæœ€ç»ˆæ€»ç»“çŠ¶æ€
            final_state = {
                "algorithms": completed_algorithms,
                "current_algorithm_index": len(completed_algorithms),  # è¡¨ç¤ºæ‰€æœ‰ç®—æ³•éƒ½å®Œæˆäº†
                "completed_tasks": [f"scout_{algo}" for algo in completed_algorithms] + 
                                 [f"analyze_{algo}" for algo in completed_algorithms] + 
                                 [f"individual_summary_{algo}" for algo in completed_algorithms],
                "report_folder": report_folder,
                "errors": []
            }
            
            final_result = self.final_summary_work(final_state)
            
            if "final_summary" in final_result["completed_tasks"]:
                return {"success": True, "errors": []}
            else:
                return {"success": False, "errors": final_result["errors"]}
                
        except Exception as e:
            error_msg = f"æœ€ç»ˆæ€»ç»“å¤±è´¥: {str(e)}"
            return {"success": False, "errors": [error_msg]}


def main():
    """ä¸»å‡½æ•°"""
    if not os.getenv("DASHSCOPE_API_KEY"):
        print("âŒ é”™è¯¯: è¯·è®¾ç½®DASHSCOPE_API_KEYç¯å¢ƒå˜é‡")
        return
    
    if not os.path.exists("./openblas-output/GENERIC/kernel"):
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ°openblas-output/GENERIC/kernelç›®å½•")
        return
    
    workflow = Workflow()
    
    print("ğŸ¯ OpenBLASä¼˜åŒ–åˆ†æ")
    print("1. å¿«é€Ÿåˆ†æ (æ‰«æååªåˆ†æaxpyã€hemvã€gemmå‰5ä¸ªæ–‡ä»¶)")
    print("2. å…¨éƒ¨åˆ†æ (æ‰«ækernelç›®å½•ä¸‹æ‰€æœ‰ç®—å­)")
    
    choice = input("è¯·é€‰æ‹© (1-2): ").strip()
    
    if choice == "1":
        print("é€‰æ‹©å¿«é€Ÿåˆ†ææ¨¡å¼ï¼Œå°†æ‰«ækernelç›®å½•ååªåˆ†æaxpyã€hemvã€gemmä¸‰ç§ç®—å­çš„å‰5ä¸ªæ–‡ä»¶")
        algorithms = "quick_analysis"  # ç‰¹æ®Šæ ‡è®°ï¼Œè¡¨ç¤ºå¿«é€Ÿåˆ†æ
    elif choice == "2":
        print("é€‰æ‹©å…¨éƒ¨åˆ†ææ¨¡å¼ï¼Œå°†æ‰«ækernelç›®å½•ä¸‹çš„æ‰€æœ‰ç®—å­ç§ç±»")
        algorithms = None  # è®©Scoutè‡ªåŠ¨å‘ç°
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©")
        return
    
    try:
        result = workflow.run(algorithms)
        
        print("\nğŸ“Š åˆ†æå®Œæˆ")
        
        completed_algorithms = result["completed_algorithms"]
        final_summary_completed = result["final_summary_completed"]
        report_folder = result["report_folder"]
        errors = result["errors"]
        
        if isinstance(algorithms, list):
            total_algorithms = len(algorithms)
        else:
            total_algorithms = len(completed_algorithms)
        
        print(f"\nâœ… å®Œæˆç®—æ³•: {len(completed_algorithms)}/{total_algorithms} ä¸ª")
        print(f"ğŸ¯ æœ€ç»ˆæ€»ç»“: {'âœ…' if final_summary_completed else 'âŒ'}")
        print(f"ğŸ“ æŠ¥å‘Šä½ç½®: {report_folder}")
        
        if len(completed_algorithms) > 0:
            print(f"ğŸ“‹ å·²åˆ†æç®—å­: {', '.join(completed_algorithms)}")
        
        if errors:
            print(f"\nâš ï¸ é”™è¯¯: {len(errors)} ä¸ª")
        
        if final_summary_completed:
            final_path = FileManager.get_final_summary_path(report_folder)
            print(f"\nğŸ‰ åˆ†æå®Œæˆï¼æŸ¥çœ‹ {final_path}")
        
    except Exception as e:
        print(f"\nâŒ å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {str(e)}")


if __name__ == "__main__":
    main()

