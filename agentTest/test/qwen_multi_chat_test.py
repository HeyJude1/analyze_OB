from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate, MessagesPlaceholder, ChatPromptTemplate
from langchain_core.runnables import RunnableSequence, RunnableLambda
from langchain.output_parsers import StructuredOutputParser, ResponseSchema
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from dotenv import load_dotenv
import os
import json

load_dotenv(override=True)
Qwen_api_key = os.getenv("DASHSCOPE_API_KEY")

model = ChatOpenAI(
    model="qwen-plus-2025-07-14",
    api_key=Qwen_api_key,
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    temperature=0.7,
    max_tokens=2048,
    top_p=0.9,
    logprobs=False
)

chat_prompt = ChatPromptTemplate.from_messages([
    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªç®—å­ç”Ÿæˆå™¨ï¼Œè¯·æ ¹æ®ä»¥ä¸‹ç®—å­åç§°ï¼Œè¡¥å……ç®—å­ä»£ç ä»¥åŠç®—å­æè¿°ï¼š"),
    MessagesPlaceholder("messages")
])

chat_chain = chat_prompt | model
messages_list=[]
print("è¯·è¾“å…¥exitï¼Œé€€å‡ºç¨‹åº")
while True:
    user_query = input("ğŸ‘¤ ä½ ï¼š")
    if user_query.lower() in {"exit", "quit"}:
        break
    
    messages_list.append(HumanMessage(content=user_query))

    ai_reply = chat_chain.invoke({"messages":messages_list})
    print("ğŸ¤– å°æ™ºï¼š", ai_reply.content)

    messages_list.append(AIMessage(content=ai_reply.content))

    messages_list=messages_list[-50:]

# gen_chain = gen_prompt | model

# schemas=[
#     ResponseSchema(name="operator_name", description="ç®—å­åç§°"),
#     ResponseSchema(name="operator_code", description="ç®—å­ä»£ç "),
#     ResponseSchema(name="operator_description", description="ç®—å­æè¿°"),
# ]

# parser = StructuredOutputParser.from_response_schemas(schemas)

# summary_prompt = PromptTemplate.from_template(
#     "\n\nè¯·æ ¹æ®ä»¥ä¸‹ç®—å­ä¿¡æ¯ï¼Œæå–å…³é”®ä¿¡æ¯ï¼Œä»¥JSONæ ¼å¼è¿”å›ï¼š\n\n{operator_info}\n\n{format_information}"
# )

# summary_chain = (
#     summary_prompt.partial(format_information=parser.get_format_instructions())
#     | model 
#     | parser
# )

# def gen_operator_info(operator_info):
#     print("ç®—å­ä¿¡æ¯ï¼š", operator_info)
#     return operator_info

# debug_node = RunnableLambda(gen_operator_info)

# # full_chain = gen_chain | summary_chain
# full_chain = gen_chain | debug_node | summary_chain 
# response = full_chain.invoke({"operator_name" : "gemm()"})

# print(json.dumps(response, indent=4, ensure_ascii=False))

