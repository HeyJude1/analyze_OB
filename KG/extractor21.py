#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
OpenBLASçŸ¥è¯†å›¾è°±å®ä½“æŠ½å–å™¨ - (V13 - Embeddingç”Ÿæˆé€»è¾‘æœ€ç»ˆä¿®å¤)
- "analysis_results_dir" ç°åœ¨æ˜¯åŸºå‡†ç›®å½•ã€‚
- JSONæ–‡ä»¶ä»åŸºå‡†ç›®å½•ä¸‹çš„ "analysis_results" å­ç›®å½•è¯»å–ã€‚
- è¾“å‡ºæ–‡ä»¶ (relations, checkpoints) ç›´æ¥ä¿å­˜åœ¨åŸºå‡†ç›®å½•ä¸‹ã€‚
- æ¯ä¸ªæå–çš„å®ä½“éƒ½è¢«è§†ä¸ºå…¨æ–°å®ä½“ï¼ŒUIDæ ¹æ®å…¶å®Œæ•´æ•°æ®ç”Ÿæˆã€‚
- --fresh å‚æ•°ç”¨äºå¼ºåˆ¶ä»å¤´å¼€å§‹å¤„ç†ã€‚
- ä¿®æ­£äº†æ‰€æœ‰å·²çŸ¥çš„bugã€‚
- æ–°å¢ï¼šä¸ºå…³ç³»å®ä½“è‡ªåŠ¨ç”Ÿæˆæè¿°ã€‚
- æ–°å¢ï¼šä¸ºç¡¬ä»¶ç‰¹å¾å®ä½“å¡«å……æ¶æ„ä¿¡æ¯ã€‚
- ä¿®æ­£ï¼šç¡®ä¿åœ¨ç”Ÿæˆembeddingæ—¶ï¼Œentity_dataä¸­ä¸åŒ…å«uidã€‚
- æ–°å¢ï¼šä¸°å¯Œoptimization_strategyå’Œcomputational_patternçš„entity_dataå­—æ®µã€‚
"""

import os
import json
import hashlib
from typing import Dict, List, Any
from pathlib import Path
from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType, utility
from langchain_community.embeddings import DashScopeEmbeddings
import argparse
from dotenv import load_dotenv

load_dotenv()


class KnowledgeGraphExtractor:
    """çŸ¥è¯†å›¾è°±å®ä½“æŠ½å–å™¨"""
    
    def __init__(self, config: Dict[str, Any], checkpoint_path: str):
        self.config = config
        self.milvus_config = self.config.get("milvus", {})
        self.embedding_config = self.config.get("dashscope_embeddings", {})
        self.data_source_config = self.config.get("data_source", {})
        
        self.embedding_model_name = self.embedding_config.get("name", "text-embedding-v3")
        api_key = os.getenv("DASHSCOPE_API_KEY")
        if not api_key:
            raise RuntimeError("DASHSCOPE_API_KEY environment variable is required")
        
        self.embedding_model = DashScopeEmbeddings(
            model=self.embedding_model_name, 
            dashscope_api_key=api_key
        )
        
        self._connect_milvus()
        self._create_collections()
        
        self.checkpoint_file = checkpoint_path
        self.processed_files = self._load_checkpoint()
        
        self.all_relations_for_txt = []
        self.all_relations_for_json = []

        self.code_counter = 1

        print("âœ… çŸ¥è¯†å›¾è°±æŠ½å–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    @staticmethod
    def _load_config(config_path: str) -> Dict[str, Any]:
        if not os.path.exists(config_path):
            return {
                "milvus": {"host": "localhost", "port": 19530, "database": "code_op"},
                "dashscope_embeddings": {"name": "text-embedding-v3", "dimension": 1024}
            }
        
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def _connect_milvus(self):
        host = self.milvus_config.get("host", "localhost")
        port = self.milvus_config.get("port", 19530)
        database = self.milvus_config.get("database", "code_op")
        
        connections.connect(alias="default", host=host, port=port, db_name=database)
        print(f"âœ… å·²è¿æ¥åˆ°Milvus: {host}:{port}/{database}")
    
    def _create_collections(self):
        dimension = self.embedding_config.get("dimension", 1024)
        collections_schema = {
            "optimization_strategy": [
                FieldSchema(name="uid", dtype=DataType.VARCHAR, max_length=100, is_primary=True),
                FieldSchema(name="name", dtype=DataType.VARCHAR, max_length=500),
                FieldSchema(name="level", dtype=DataType.VARCHAR, max_length=50),
                FieldSchema(name="rationale", dtype=DataType.VARCHAR, max_length=5000),
                FieldSchema(name="implementation", dtype=DataType.VARCHAR, max_length=5000),
                FieldSchema(name="impact", dtype=DataType.VARCHAR, max_length=2000),
                FieldSchema(name="trade_offs", dtype=DataType.VARCHAR, max_length=2000),
                FieldSchema(name="entity_data", dtype=DataType.VARCHAR, max_length=65535),
                FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=dimension)
            ],
            "computational_pattern": [
                FieldSchema(name="uid", dtype=DataType.VARCHAR, max_length=100, is_primary=True),
                FieldSchema(name="name", dtype=DataType.VARCHAR, max_length=100),
                FieldSchema(name="type", dtype=DataType.VARCHAR, max_length=100),
                FieldSchema(name="description", dtype=DataType.VARCHAR, max_length=5000),
                FieldSchema(name="code", dtype=DataType.VARCHAR, max_length=10000),
                FieldSchema(name="numeric_kind", dtype=DataType.VARCHAR, max_length=100),
                FieldSchema(name="numeric_precision", dtype=DataType.VARCHAR, max_length=100),
                FieldSchema(name="structural_properties", dtype=DataType.VARCHAR, max_length=500),
                FieldSchema(name="storage_layout", dtype=DataType.VARCHAR, max_length=500),
                FieldSchema(name="entity_data", dtype=DataType.VARCHAR, max_length=65535),
                FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=dimension)
            ],
            "hardware_feature": [
                FieldSchema(name="uid", dtype=DataType.VARCHAR, max_length=100, is_primary=True),
                FieldSchema(name="name", dtype=DataType.VARCHAR, max_length=500),
                FieldSchema(name="architecture", dtype=DataType.VARCHAR, max_length=100),
                FieldSchema(name="description", dtype=DataType.VARCHAR, max_length=2000),
                FieldSchema(name="entity_data", dtype=DataType.VARCHAR, max_length=65535),
                FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=dimension)
            ],
            "tunable_parameter": [
                FieldSchema(name="uid", dtype=DataType.VARCHAR, max_length=100, is_primary=True),
                FieldSchema(name="name", dtype=DataType.VARCHAR, max_length=500),
                FieldSchema(name="description", dtype=DataType.VARCHAR, max_length=2000),
                FieldSchema(name="impact", dtype=DataType.VARCHAR, max_length=2000),
                FieldSchema(name="value_in_code", dtype=DataType.VARCHAR, max_length=500),
                FieldSchema(name="typical_range", dtype=DataType.VARCHAR, max_length=500),
                FieldSchema(name="entity_data", dtype=DataType.VARCHAR, max_length=65535),
                FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=dimension)
            ],
            "code_example": [
                FieldSchema(name="uid", dtype=DataType.VARCHAR, max_length=100, is_primary=True),
                FieldSchema(name="name", dtype=DataType.VARCHAR, max_length=100),
                FieldSchema(name="snippet", dtype=DataType.VARCHAR, max_length=10000),
                FieldSchema(name="explanation", dtype=DataType.VARCHAR, max_length=5000),
                FieldSchema(name="source_file", dtype=DataType.VARCHAR, max_length=500),
                FieldSchema(name="entity_data", dtype=DataType.VARCHAR, max_length=65535),
                FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=dimension)
            ],
            "relation": [
                FieldSchema(name="relation_id", dtype=DataType.VARCHAR, max_length=100, is_primary=True),
                FieldSchema(name="relation_type", dtype=DataType.VARCHAR, max_length=100),
                FieldSchema(name="head_entity_uid", dtype=DataType.VARCHAR, max_length=100),
                FieldSchema(name="tail_entity_uid", dtype=DataType.VARCHAR, max_length=100),
                FieldSchema(name="head_name", dtype=DataType.VARCHAR, max_length=500),
                FieldSchema(name="tail_name", dtype=DataType.VARCHAR, max_length=500),
                FieldSchema(name="description", dtype=DataType.VARCHAR, max_length=2000),
                FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=dimension)
            ]
        }
        for collection_name, fields in collections_schema.items():
            if not utility.has_collection(collection_name):
                Collection(collection_name, CollectionSchema(fields, f"{collection_name} collection"))

    def _build_index_for_collection(self, collection_name: str):
        try:
            collection = Collection(collection_name)
            collection.flush()
            num_entities = collection.num_entities
            if num_entities == 0: return
            if not collection.has_index():
                if num_entities < 1000:
                    index_params = {"index_type": "FLAT", "metric_type": "L2"}
                else:
                    nlist = max(128, min(1024, int((num_entities ** 0.5) * 2)))
                    index_params = {"index_type": "IVF_FLAT", "metric_type": "L2", "params": {"nlist": nlist}}
                collection.create_index(field_name="embedding", index_params=index_params)
            collection.load()
        except Exception as e:
            print(f"âš ï¸ å¤„ç†é›†åˆ {collection_name} æ—¶å‡ºé”™: {e}")

    def _build_indexes_for_all_collections(self):
        collection_names = ["optimization_strategy", "computational_pattern", "hardware_feature", 
                            "tunable_parameter", "code_example", "relation"]
        for name in collection_names: self._build_index_for_collection(name)

    def _load_checkpoint(self) -> set:
        if os.path.exists(self.checkpoint_file):
            with open(self.checkpoint_file, 'r', encoding='utf-8') as f:
                return set(json.load(f).get("processed_files", []))
        return set()

    def _save_checkpoint(self):
        with open(self.checkpoint_file, 'w', encoding='utf-8') as f:
            json.dump({"processed_files": list(self.processed_files)}, f, ensure_ascii=False, indent=2)

    def _generate_uid_from_dict(self, data: Dict[str, Any]) -> str:
        dhash = hashlib.md5()
        encoded = json.dumps(data, sort_keys=True).encode('utf-8')
        dhash.update(encoded)
        return dhash.hexdigest()

    def _get_embedding(self, text: str) -> List[float]:
        try:
            return self.embedding_model.embed_query(text)
        except Exception as e:
            print(f"âš ï¸ å‘é‡åŒ–å¤±è´¥: {e}")
            return [0.0] * self.embedding_config.get("dimension", 1024)

    def _get_embedding_text(self, entity_data_without_uid: Dict[str, Any]) -> str:
        """ä»ä¸å«UIDçš„entity_dataå­—å…¸ç”Ÿæˆç”¨äºembeddingçš„æ–‡æœ¬"""
        return json.dumps(entity_data_without_uid, ensure_ascii=False, sort_keys=True)

    # <<< MODIFIED: Corrected the logic to remove uid BEFORE embedding
    def _save_entity(self, collection_name: str, entity_data: Dict[str, Any]) -> str:
        # 1. æå–UID
        uid = entity_data["uid"]
        
        # 2. åˆ›å»ºä¸€ä¸ªå¹²å‡€çš„å‰¯æœ¬ç”¨äºembeddingå’Œå­˜å‚¨
        data_for_processing = entity_data.copy()
        data_for_processing.pop("uid", None)
        
        # 3. åŸºäºå¹²å‡€çš„æ•°æ®ç”Ÿæˆembedding
        embedding_text = self._get_embedding_text(data_for_processing)
        embedding = self._get_embedding(embedding_text)
        
        # 4. å‡†å¤‡æ’å…¥æ•°æ®
        schema = Collection(collection_name).schema
        field_names = [field.name for field in schema.fields]
        
        insert_data = []
        for name in field_names:
            if name == "uid":
                insert_data.append([uid]) # ä½¿ç”¨åŸå§‹UID
            elif name == "embedding":
                insert_data.append([embedding])
            elif name == "entity_data":
                # å­˜å‚¨ä¸å«uidçš„entity_data
                insert_data.append([json.dumps(data_for_processing, ensure_ascii=False)])
            else:
                # ä»åŸå§‹çš„entity_dataä¸­è·å–å…¶ä»–å­—æ®µå€¼
                if name in ["numeric_kind", "numeric_precision", "structural_properties", "storage_layout"]:
                    value = entity_data.get("data_object_features", {}).get(name, "")
                else:
                    value = entity_data.get(name, "")
                insert_data.append([value])

        Collection(collection_name).insert(insert_data)
        print(f"    âœ“ ä¿å­˜æ–°å®ä½“ {collection_name}: {entity_data.get('name', '')} -> {uid[:8]}...")
        return uid
    
    def _save_relation(self, head_uid: str, tail_uid: str, relation_type: str,
                       head_name: str, tail_name: str):
        description = ""
        if relation_type == "OPTIMIZES_PATTERN":
            description = f"{head_name}å¯ä½¿ç”¨{tail_name}ä¼˜åŒ–"
        elif relation_type == "HAS_PARAMETER":
            description = "è¯¥ä¼˜åŒ–ç­–ç•¥åŒ…å«æ­¤å¯è°ƒå‚æ•°"
        elif relation_type == "IS_ILLUSTRATED_BY":
            description = "è¯¥ä»£ç ç¤ºä¾‹å±•ç¤ºäº†æ­¤ä¼˜åŒ–ç­–ç•¥"
        elif relation_type == "TARGETS":
            description = "è¯¥ä¼˜åŒ–ç­–ç•¥é’ˆå¯¹æ­¤ç¡¬ä»¶ç‰¹æ€§"
            
        relation_content = {"type": relation_type, "head": head_uid, "tail": tail_uid, "desc": description}
        relation_uid = self._generate_uid_from_dict(relation_content)

        embedding_text = f"{relation_type} from {head_name} to {tail_name}: {description}"
        embedding = self._get_embedding(embedding_text)
        
        Collection("relation").insert([
            [relation_uid], [relation_type], [head_uid], [tail_uid],
            [head_name], [tail_name], [description], [embedding]
        ])
        print(f"    âœ“ ä¿å­˜æ–°å…³ç³»: {relation_type} ({head_name} -> {tail_name})")

        self.all_relations_for_txt.append((head_name, relation_type, tail_name))
        self.all_relations_for_json.append({
            "relation_type": relation_type,
            "relation_id": relation_uid,
            "head": {"name": head_name, "uid": head_uid},
            "tail": {"name": tail_name, "uid": tail_uid}
        })

    def extract_from_file(self, file_path: str):
        if file_path in self.processed_files:
            print(f"â­ï¸ è·³è¿‡å·²å¤„ç†æ–‡ä»¶: {file_path}")
            return
        
        print(f"ğŸ“„ å¤„ç†: {os.path.basename(file_path)}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        source_algorithm = data.get("algorithm", "unknown")

        for analysis in data.get("individual_analyses", []):
            operator_name = analysis.get("file_path", "").split("/")[-1]
            architecture = analysis.get("architecture", "é€šç”¨")
            print(f"  ğŸ” ç®—å­: {operator_name} (ç®—æ³•: {source_algorithm}, æ¶æ„: {architecture})")
            
            entity_count = 0
            relation_count = 0
            
            pattern_map = {}
            for pattern in analysis.get("computational_patterns", []):
                pattern_entity = {
                    "name": pattern.get("name", ""),
                    "type": pattern.get("pattern_type", "") or pattern.get("type", ""),
                    "description": pattern.get("description", ""),
                    "code": pattern.get("code", ""),
                    "data_object_features": pattern.get("data_object_features", {}),
                    "source_algorithm": source_algorithm,
                    "source_file": operator_name,
                    "architecture": architecture
                }
                uid = self._generate_uid_from_dict(pattern_entity)
                pattern_entity["uid"] = uid
                self._save_entity("computational_pattern", pattern_entity)
                entity_count += 1
                if pattern_entity["type"]:
                    pattern_map[pattern_entity["type"]] = {"uid": uid, "name": pattern_entity["name"]}

            for level in ["algorithm_level_optimizations", "code_level_optimizations", "instruction_level_optimizations"]:
                for opt in analysis.get(level, []):
                    desc = opt.get('description', {})
                    strategy_entity = {
                        "name": opt.get("optimization_name", ""),
                        "level": opt.get("level", ""),
                        "rationale": desc.get("strategy_rationale", ""),
                        "implementation": desc.get("implementation_pattern", ""),
                        "impact": desc.get("performance_impact", ""),
                        "trade_offs": desc.get("trade_offs", ""),
                        "related_patterns": opt.get("related_patterns", []),
                        "source_algorithm": source_algorithm,
                        "source_file": operator_name,
                        "architecture": architecture
                    }
                    strategy_uid = self._generate_uid_from_dict(strategy_entity)
                    strategy_entity["uid"] = strategy_uid
                    self._save_entity("optimization_strategy", strategy_entity)
                    entity_count += 1
                    
                    for pattern_type in opt.get("related_patterns", []):
                        if pattern_type in pattern_map:
                            head_info = pattern_map[pattern_type]
                            self._save_relation(head_info["uid"], strategy_uid, "OPTIMIZES_PATTERN",
                                              head_name=head_info["name"], tail_name=strategy_entity["name"])
                            relation_count += 1
                    
                    hw_name = opt.get("target_hardware_feature_name")
                    if hw_name:
                        hw_entity = {
                            "name": hw_name, 
                            "architecture": architecture, 
                            "description": opt.get("target_hardware_feature", ""),
                            "source_algorithm": source_algorithm,
                            "source_file": operator_name
                        }
                        hw_uid = self._generate_uid_from_dict(hw_entity)
                        hw_entity["uid"] = hw_uid
                        self._save_entity("hardware_feature", hw_entity)
                        entity_count += 1
                        self._save_relation(strategy_uid, hw_uid, "TARGETS",
                                          head_name=strategy_entity["name"], tail_name=hw_name)
                        relation_count += 1
                    
                    code_examples = []
                    if 'code_example' in opt and isinstance(opt['code_example'], dict) and opt['code_example']:
                        code_examples.append(opt['code_example'])
                    elif 'code_examples' in opt and isinstance(opt['code_examples'], list) and opt['code_examples']:
                        code_examples.extend(opt['code_examples'])

                    for code_obj in code_examples:
                        code_entity = {
                            "name": f"code{self.code_counter}",
                            "snippet": code_obj.get("snippet", "") if isinstance(code_obj, dict) else str(code_obj),
                            "explanation": code_obj.get("explanation", "") if isinstance(code_obj, dict) else "",
                            "source_file": operator_name,
                            "source_algorithm": source_algorithm,
                            "architecture": architecture
                        }
                        code_uid = self._generate_uid_from_dict(code_entity)
                        code_entity["uid"] = code_uid
                        self._save_entity("code_example", code_entity)
                        entity_count += 1
                        self._save_relation(strategy_uid, code_uid, "IS_ILLUSTRATED_BY",
                                          head_name=strategy_entity["name"], tail_name=code_entity["name"])
                        relation_count += 1
                        self.code_counter += 1
                    
                    for param in opt.get("tunable_parameters", []):
                        param_name = param.get("parameter_name") if isinstance(param, dict) else str(param)
                        if not param_name: continue
                        
                        typical_range = param.get("typical_range", []) if isinstance(param, dict) else []
                        param_entity = {
                            "name": param_name,
                            "description": param.get("description", "") if isinstance(param, dict) else f"Tunable parameter: {param_name}",
                            "impact": param.get("impact", "") if isinstance(param, dict) else "",
                            "value_in_code": str(param.get("value_in_code", "")) if isinstance(param, dict) else "",
                            "typical_range": ",".join(map(str, typical_range)),
                            "source_algorithm": source_algorithm,
                            "source_file": operator_name,
                            "architecture": architecture
                        }
                        param_uid = self._generate_uid_from_dict(param_entity)
                        param_entity["uid"] = param_uid
                        self._save_entity("tunable_parameter", param_entity)
                        entity_count += 1
                        self._save_relation(strategy_uid, param_uid, "HAS_PARAMETER",
                                          head_name=strategy_entity["name"], tail_name=param_entity["name"])
                        relation_count += 1
            
            print(f"  ğŸ“Š å®Œæˆ: æ–°å¢å®ä½“={entity_count}, æ–°å¢å…³ç³»={relation_count}")
        
        self.processed_files.add(file_path)
        self._save_checkpoint()
        print("ğŸ’¾ æ–­ç‚¹å·²ä¿å­˜")

    def _write_relation_txt(self, output_directory: str):
        output_path = os.path.join(output_directory, "relation.txt")
        with open(output_path, 'w', encoding='utf-8') as f:
            for head, rel_type, tail in self.all_relations_for_txt:
                f.write(f"{head}\t{rel_type}\t{tail}\n")
        print(f"âœ… å…³ç³»æ–‡æœ¬æ–‡ä»¶å·²ä¿å­˜åˆ°: {output_path}")

    def _write_relation_entity_json(self, output_directory: str):
        output_path = os.path.join(output_directory, "relation_entity.json")
        grouped_relations = {}
        for relation in self.all_relations_for_json:
            rel_type = relation["relation_type"]
            if rel_type not in grouped_relations:
                grouped_relations[rel_type] = []
            grouped_relations[rel_type].append(relation)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(grouped_relations, f, ensure_ascii=False, indent=2)
        print(f"âœ… å…³ç³»JSONæ–‡ä»¶å·²ä¿å­˜åˆ°: {output_path}")

    def extract_from_directory(self, json_input_dir: str, base_output_dir: str):
        json_files = sorted(list(Path(json_input_dir).glob("*.json")))
        print(f"ğŸ“ å‘ç° {len(json_files)} ä¸ªJSONæ–‡ä»¶")
        
        for i, file_path in enumerate(json_files, 1):
            print(f"\n{'='*60}\nè¿›åº¦: {i}/{len(json_files)}\n{'='*60}")
            self.extract_from_file(str(file_path))
        
        print("\nğŸ”§ æ•°æ®æ’å…¥å®Œæˆï¼Œæ­£åœ¨åˆ·æ–°å’Œç´¢å¼•é›†åˆ...")
        self._build_indexes_for_all_collections()
        
        print("\nğŸ’¾ æ­£åœ¨å†™å…¥å…³ç³»æ–‡ä»¶...")
        self._write_relation_txt(base_output_dir)
        self._write_relation_entity_json(base_output_dir)
        
        print(f"\n{'='*60}\nğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
        total_entities, total_relations = 0, 0
        all_collections = utility.list_collections()
        for name in all_collections:
            try:
                count = Collection(name).num_entities
                if name != "relation": total_entities += count
                else: total_relations = count
                print(f"  âœ… {name}: {count} ä¸ª")
            except Exception as e:
                print(f"  âš ï¸ {name}: ç»Ÿè®¡å¤±è´¥ - {e}")
        
        print(f"\nğŸ“Š æ€»è®¡: å®ä½“={total_entities}, å…³ç³»={total_relations}")
        print(f"{'='*60}\nğŸ‰ å®Œæˆï¼")


def main():
    parser = argparse.ArgumentParser(description="OpenBLASçŸ¥è¯†å›¾è°±å®ä½“æŠ½å–å™¨")
    parser.add_argument("--config", type=str, default="kg_config.json", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--data_dir", type=str, default=None, help="åˆ†æç»“æœçš„åŸºå‡†ç›®å½•")
    parser.add_argument("--fresh", action="store_true", help="å¿½ç•¥æ–­ç‚¹æ–‡ä»¶ï¼Œä»å¤´å¼€å§‹å¤„ç†æ‰€æœ‰æ–‡ä»¶")
    args = parser.parse_args()
    
    config = KnowledgeGraphExtractor._load_config(args.config)
    
    base_dir = args.data_dir or config.get("data_source", {}).get("analysis_results_dir")
    if not base_dir:
        print("âŒ é”™è¯¯ï¼šæœªåœ¨é…ç½®æˆ–å‘½ä»¤è¡Œä¸­æŒ‡å®šåŸºå‡†ç›®å½• (analysis_results_dir)")
        return

    if not os.path.isabs(base_dir):
        script_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(script_dir)
        
        resolved_path = os.path.join(project_root, base_dir)
        
        if not os.path.exists(resolved_path):
            project_folder_name = os.path.basename(project_root)
            if project_folder_name in base_dir:
                try:
                    idx = base_dir.index(project_folder_name)
                    suffix = base_dir[idx:]
                    root_parent = os.path.dirname(project_root)
                    resolved_path = os.path.join(root_parent, suffix)
                except ValueError:
                    pass
        base_dir = os.path.abspath(resolved_path)

    json_input_dir = os.path.join(base_dir, "analysis_results")

    if not os.path.exists(json_input_dir):
        print(f"âŒ é”™è¯¯ï¼šJSONè¾“å…¥ç›®å½•ä¸å­˜åœ¨: {json_input_dir}")
        return
    
    checkpoints_dir = os.path.join(base_dir, "checkpoints")
    os.makedirs(checkpoints_dir, exist_ok=True)
    checkpoint_file_path = os.path.join(checkpoints_dir, "extraction_checkpoint.json")

    if args.fresh and os.path.exists(checkpoint_file_path):
        os.remove(checkpoint_file_path)
        print(f"ğŸ—‘ï¸ å·²åˆ é™¤æ—§çš„æ–­ç‚¹æ–‡ä»¶ '{checkpoint_file_path}'ï¼Œå°†ä»å¤´å¼€å§‹å¤„ç†ã€‚")
    
    extractor = KnowledgeGraphExtractor(config=config, checkpoint_path=checkpoint_file_path)
    
    print(f"ğŸ“ åŸºå‡†ç›®å½•: {base_dir}")
    print(f"ğŸ“‚ JSONè¾“å…¥ç›®å½•: {json_input_dir}")
    extractor.extract_from_directory(json_input_dir=json_input_dir, base_output_dir=base_dir)


if __name__ == "__main__":
    main()