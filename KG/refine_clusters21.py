#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®ä½“èšç±»ç²¾ç‚¼å™¨v10 (æœ€ç»ˆå¥å£®ç‰ˆ)
- è¾“å…¥: clusters_retrieved.json
- è¾“å‡º: clusters_retrieved_refined.json
- åŠŸèƒ½:
  1. å¯¹æ¯ä¸ªç°‡ï¼Œè°ƒç”¨LLMè¿›è¡Œè¯­ä¹‰åˆ†ç»„å¹¶å‘½åã€‚
  2. ç¨‹åºæ ¹æ®LLMçš„åˆ†ç»„ç»“æœï¼ŒæŒ‰ç…§æ˜ç¡®è§„åˆ™è®¾ç½®is_primaryæ ‡å¿—ã€‚
  3. ä¸¥æ ¼éµå¾ªè¾“å‡ºJSONæ ¼å¼ã€‚
  4. æ™ºèƒ½è·³è¿‡åªå«å•ä¸ªå®ä½“çš„ç°‡ï¼ŒèŠ‚çœAPIè°ƒç”¨ã€‚
  5. åœ¨æ¯æ¬¡LLMè°ƒç”¨å‰å¢åŠ å›ºå®šå»¶æ—¶ï¼Œä¸»åŠ¨é¿å…APIé€Ÿç‡é™åˆ¶ã€‚
  6. ä¿®å¤äº†LLMè°ƒç”¨å¤±è´¥å¯¼è‡´çš„ç¨‹åºå´©æºƒé—®é¢˜ã€‚
  7. æ–°å¢ï¼šé€šè¿‡Promptå¼•å¯¼å’Œä»£ç åå¤„ç†ï¼Œç¡®ä¿similar_groupsä¸­æ¯ç»„è‡³å°‘æœ‰2ä¸ªå®ä½“ã€‚
"""

import os
import json
import time
from typing import Dict, List, Any
from pathlib import Path
from pymilvus import connections, Collection, utility
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain.output_parsers import StructuredOutputParser, ResponseSchema
import argparse
from dotenv import load_dotenv

load_dotenv()


class EntityClusterRefiner:
    """å®ä½“èšç±»ç²¾ç‚¼å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        """åˆå§‹åŒ–èšç±»ç²¾ç‚¼å™¨"""
        self.config = config
        self.milvus_config = self.config.get("milvus", {})
        self.model_config = self.config.get("model", {})
        self.entity_types = ["hardware_feature", "optimization_strategy", "tunable_parameter"]
        
        self._connect_milvus()
        self._init_llm()
        
        print("âœ… å®ä½“èšç±»ç²¾ç‚¼å™¨åˆå§‹åŒ–å®Œæˆ")
    
    @staticmethod
    def _load_config(config_path: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if not os.path.exists(config_path):
            return {
                "milvus": {"host": "localhost", "port": 19530, "database": "code_op"},
                "model": {
                    "name": "qwen-max",
                    "temperature": 0.0,
                    "max_tokens": 8192,
                    "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1"
                }
            }
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def _load_clusters_data(self, input_file: str) -> Dict[str, Any]:
        """åŠ è½½ç”± retrieve_clusters.py ç”Ÿæˆçš„èšç±»ç»“æœ"""
        print(f"ğŸ“‚ æ­£åœ¨åŠ è½½èšç±»æ–‡ä»¶: {input_file}")
        with open(input_file, 'r', encoding='utf-8') as f:
            return json.load(f)

    def _connect_milvus(self):
        """è¿æ¥ Milvus"""
        host = self.milvus_config.get("host", "localhost")
        port = self.milvus_config.get("port", 19530)
        db_name = self.milvus_config.get("database", "code_op")
        connections.connect(alias="default", host=host, port=port, db_name=db_name)
        print(f"âœ… å·²è¿æ¥åˆ°Milvus: {host}:{port}/{db_name}")
    
    def _init_llm(self):
        """åˆå§‹åŒ– ChatOpenAI æ¨¡å‹"""
        self.llm = ChatOpenAI(
            model=self.model_config.get("name"),
            temperature=float(self.model_config.get("temperature", 0.0)),
            max_tokens=int(self.model_config.get("max_tokens", 8192)),
            api_key=os.getenv("DASHSCOPE_API_KEY"),
            base_url=self.model_config.get("base_url"),
        )

    def _fetch_entity_details(self, entity_type: str, uids: List[str]) -> Dict[str, Dict[str, Any]]:
        """ä» Milvus é«˜æ•ˆæŸ¥è¯¢ä¸€æ‰¹å®ä½“çš„è¯¦ç»†ä¿¡æ¯"""
        if not uids: return {}
        try:
            col = Collection(entity_type)
            col.load()
            expr = f'uid in {json.dumps(uids)}'
            output_fields = ["uid", "name"]
            if entity_type == "optimization_strategy":
                output_fields.extend(["level", "rationale", "implementation", "impact", "trade_offs"])
            elif entity_type == "hardware_feature":
                output_fields.extend(["architecture", "description"])
            elif entity_type == "tunable_parameter":
                output_fields.extend(["description", "impact", "value_in_code", "typical_range"])
            results = col.query(expr=expr, output_fields=output_fields, limit=len(uids))
            return {res['uid']: res for res in results}
        except Exception as e:
            print(f"  âš ï¸ æŸ¥è¯¢å®ä½“è¯¦æƒ…å¤±è´¥ ({entity_type}): {e}")
            return {}

    def _build_entity_summary(self, entity_type: str, entity_details: Dict[str, Any]) -> str:
        """æ ¹æ®å®ä½“è¯¦æƒ…æ„å»ºç”¨äºLLMåˆ¤æ–­çš„æ‘˜è¦"""
        name = entity_details.get("name", "æœªçŸ¥åç§°")
        
        if entity_type == "optimization_strategy":
            parts = [f"ç­–ç•¥åç§°: {name}", f"åŸç†: {entity_details.get('rationale', 'N/A')}", f"å®ç°: {entity_details.get('implementation', 'N/A')}", f"å½±å“: {entity_details.get('impact', 'N/A')}", f"æƒè¡¡: {entity_details.get('trade_offs', 'N/A')}"]
            return "ï¼›".join(p for p in parts if p.split(': ')[-1] not in ['N/A', ''])
        elif entity_type == "hardware_feature":
            return f"ç¡¬ä»¶ç‰¹æ€§: {name}ï¼›æè¿°: {entity_details.get('description', 'N/A')}"
        elif entity_type == "tunable_parameter":
            parts = [f"å¯è°ƒå‚æ•°: {name}", f"æè¿°: {entity_details.get('description', 'N/A')}", f"å½±å“: {entity_details.get('impact', 'N/A')}"]
            return "ï¼›".join(p for p in parts if p.split(': ')[-1] not in ['N/A', ''])
        return f"å®ä½“åç§°: {name}"

    def _create_refine_parser(self) -> StructuredOutputParser:
        """åˆ›å»ºç”¨äºè§£æLLMå“åº”çš„ç»“æ„åŒ–è§£æå™¨"""
        response_schemas = [
            ResponseSchema(name="similar_groups", description="ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä»£è¡¨ä¸€ä¸ªåº”åˆå¹¶çš„å®ä½“ç»„ã€‚æ¯ä¸ªç»„æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å« 'canonical_name' (å­—ç¬¦ä¸²) å’Œ 'entities' (ä¸€ä¸ªå®ä½“ä¸´æ—¶åç§°çš„åˆ—è¡¨)ã€‚"),
            ResponseSchema(name="remaining_entities", description="ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«é‚£äº›ä¸å±äºä»»ä½•ç»„çš„ç‹¬ç«‹å®ä½“çš„ä¸´æ—¶åç§°ã€‚"),
        ]
        return StructuredOutputParser.from_response_schemas(response_schemas)

    def _invoke_llm_with_retry(self, messages: List[Any], parser: StructuredOutputParser, retries: int = 3) -> Dict[str, Any]:
        """å¸¦é‡è¯•å’Œç»“æ„åŒ–è§£æçš„LLMè°ƒç”¨ï¼Œç¡®ä¿å§‹ç»ˆè¿”å›æœ‰æ•ˆå­—å…¸"""
        for attempt in range(retries):
            try:
                time.sleep(1) 
                response = self.llm.invoke(messages)
                content = response.content
                if not content:
                    raise ValueError("LLMè¿”å›ç©ºå†…å®¹")
                
                parsed_output = parser.parse(content)
                if not isinstance(parsed_output, dict):
                    raise ValueError(f"è§£æå™¨è¿”å›äº†éå­—å…¸ç±»å‹: {type(parsed_output)}")
                    
                return parsed_output
            except Exception as e:
                print(f"  - LLMè°ƒç”¨æˆ–è§£æå¤±è´¥ (å°è¯• {attempt + 1}/{retries}): {e}")
                if attempt < retries - 1:
                    wait_time = 5 * (attempt + 1)
                    print(f"    å°†åœ¨ {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    print("  - è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè¯¥ç°‡ç²¾ç‚¼å¤±è´¥ã€‚")
                    return {"similar_groups": [], "remaining_entities": []}
        return {"similar_groups": [], "remaining_entities": []}

    def _refine_one_cluster(self, entity_type: str, cluster_obj: Dict[str, Any]) -> Dict[str, Any]:
        """å¯¹å•ä¸ªç°‡è¿›è¡Œç²¾ç‚¼ï¼Œè¿”å›ç¬¦åˆæœ€ç»ˆæ ¼å¼çš„ analysis å­—å…¸"""
        center_uid = cluster_obj.get("center_uid")
        center_name = cluster_obj.get("center_name")
        members = cluster_obj.get("members", [])

        if not members:
            print("    -> ç°‡åªåŒ…å«ä¸­å¿ƒå®ä½“ï¼Œè·³è¿‡LLMè°ƒç”¨ã€‚")
            return {
                "status": "success",
                "analysis": {
                    "similar_groups": [],
                    "remaining_entities": [
                        {"name": center_name, "uid": center_uid, "is_primary": True}
                    ]
                }
            }

        all_uids = [center_uid] + [m['uid'] for m in members]
        details_map = self._fetch_entity_details(entity_type, all_uids)

        temp_id_map = {}
        llm_input_items = []
        name_counts = {}

        all_cluster_entities = [{"uid": center_uid, "name": cluster_obj.get("center_name")}] + members
        
        for entity in all_cluster_entities:
            uid = entity.get('uid')
            name = entity.get('name')
            if not uid or not name: continue
            
            details = details_map.get(uid)
            if not details: continue

            count = name_counts.get(name, 0)
            temp_name = f"{name}_{count}"
            name_counts[name] = count + 1
            temp_id_map[temp_name] = {"uid": uid, "name": name}
            
            is_center = (uid == center_uid)
            llm_input_items.append({
                "temp_name": temp_name,
                "summary": self._build_entity_summary(entity_type, details),
                "is_center": is_center
            })

        parser = self._create_refine_parser()
        # <<< MODIFIED: Added explicit rule for group size
        system_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªå®ä½“å¯¹é½ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æä¸€ä¸ªé¢„èšç±»ç°‡ä¸­çš„å®ä½“åˆ—è¡¨ï¼Œå¹¶å°†å®ƒä»¬ç²¾ç¡®åœ°åˆ†ç»„ã€‚\n"
            "æ¯ä¸ªå®ä½“éƒ½æœ‰ä¸€ä¸ªä¸´æ—¶çš„å”¯ä¸€åç§°ï¼ˆå¦‚ 'åç§°_åºå·'ï¼‰å’Œä¸€ä¸ªæ‘˜è¦ã€‚ç°‡çš„åŸå§‹ä¸­å¿ƒç”± 'is_center: true' æ ‡è®°ã€‚\n\n"
            "ä½ çš„è¾“å‡ºå¿…é¡»æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„JSONå¯¹è±¡ï¼ŒåŒ…å«ä¸¤ä¸ªé”®ï¼š'similar_groups' å’Œ 'remaining_entities'ã€‚\n"
            "1. `similar_groups`: ä¸€ä¸ªåˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ ä»£è¡¨ä¸€ä¸ªè¯­ä¹‰ä¸Šåº”åˆå¹¶çš„ç»„ã€‚æ¯ä¸ªç»„åŒ…å«ï¼š\n"
            "   - `canonical_name`: ä¸ºè¯¥ç»„æŒ‡å®šä¸€ä¸ªæœ€å‡†ç¡®ã€æœ€å…·ä»£è¡¨æ€§çš„è§„èŒƒåç§°ã€‚\n"
            "   - `entities`: ä¸€ä¸ªåˆ—è¡¨ï¼Œä»…åŒ…å«å±äºè¯¥ç»„çš„æ‰€æœ‰å®ä½“çš„**ä¸´æ—¶åç§°** (temp_name)ã€‚\n"
            "2. `remaining_entities`: ä¸€ä¸ªåˆ—è¡¨ï¼Œä»…åŒ…å«é‚£äº›ä¸å±äºä»»ä½•ç»„çš„ç‹¬ç«‹å®ä½“çš„**ä¸´æ—¶åç§°** (temp_name)ã€‚\n\n"
            "**ä¸¥æ ¼è¦æ±‚**:\n"
            "- **`similar_groups` ä¸­çš„æ¯ä¸ªç»„ï¼ˆ`entities` åˆ—è¡¨ï¼‰å¿…é¡»è‡³å°‘åŒ…å«2ä¸ªå®ä½“ã€‚å¦‚æœä¸€ä¸ªå®ä½“æ— æ³•ä¸å…¶ä»–ä»»ä½•å®ä½“åˆå¹¶ï¼Œè¯·å°†å…¶æ”¾å…¥ `remaining_entities`ã€‚**\n"
            "- æ‰€æœ‰è¾“å…¥çš„å®ä½“å¿…é¡»å‡ºç°åœ¨è¾“å‡ºä¸­ï¼Œä¸èƒ½é—æ¼æˆ–é‡å¤ã€‚\n"
            "- æœ€ç»ˆè¾“å‡ºä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ€§æ–‡å­—ï¼Œåªè¿”å›JSONå¯¹è±¡ã€‚\n"
            "{format_instructions}"
        )
        
        llm_input_payload = {"entities_to_group": llm_input_items}

        prompt_template = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "{input}")
        ])
        
        messages = prompt_template.format_messages(
            format_instructions=parser.get_format_instructions(),
            input=json.dumps(llm_input_payload, ensure_ascii=False)
        )
        
        llm_result = self._invoke_llm_with_retry(messages, parser)

        final_analysis = {"similar_groups": [], "remaining_entities": []}
        
        for group in llm_result.get("similar_groups", []):
            # <<< MODIFIED: Post-processing to enforce group size rule
            if len(group.get("entities", [])) < 2:
                for temp_name in group.get("entities", []):
                    llm_result.setdefault("remaining_entities", []).append(temp_name)
                continue

            final_group = {"canonical_name": group.get("canonical_name"), "entities": []}
            contains_center = False
            
            for temp_name in group.get("entities", []):
                original_info = temp_id_map.get(temp_name)
                if original_info:
                    final_group["entities"].append({"name": original_info["name"], "uid": original_info["uid"]})
                    if original_info["uid"] == center_uid:
                        contains_center = True
            
            if final_group["entities"]:
                if contains_center:
                    for entity in final_group["entities"]:
                        entity["is_primary"] = (entity["uid"] == center_uid)
                else:
                    final_group["entities"][0]["is_primary"] = True
                    for entity in final_group["entities"][1:]:
                        entity["is_primary"] = False
            final_analysis["similar_groups"].append(final_group)

        for temp_name in llm_result.get("remaining_entities", []):
            original_info = temp_id_map.get(temp_name)
            if original_info:
                final_analysis["remaining_entities"].append({
                    "name": original_info["name"],
                    "uid": original_info["uid"],
                    "is_primary": True
                })

        return {"status": "success", "analysis": final_analysis}

    def refine_all_clusters(self, input_file: str) -> Dict[str, Any]:
        """ç²¾ç‚¼æ‰€æœ‰èšç±»"""
        print(f"ğŸš€ å¼€å§‹ç²¾ç‚¼èšç±»æ–‡ä»¶: {input_file}")
        clusters_data = self._load_clusters_data(input_file)
        refined_results = {}

        for entity_type in self.entity_types:
            if entity_type not in clusters_data:
                continue

            print(f"\nğŸ“‹ å¼€å§‹å¤„ç†å®ä½“ç±»å‹: {entity_type}")
            refined_results[entity_type] = []
            
            clusters = clusters_data.get(entity_type, {})
            cluster_count = len(clusters)
            
            for i, (cluster_name, cluster_obj) in enumerate(clusters.items()):
                print(f"  -> æ­£åœ¨ç²¾ç‚¼ç°‡ {i + 1}/{cluster_count} ({cluster_name})...")
                analysis_result = self._refine_one_cluster(entity_type, cluster_obj)
                refined_results[entity_type].append(analysis_result)

        print("\nğŸ‰ èšç±»ç²¾ç‚¼å®Œæˆï¼")
        return refined_results
    
    def save_results(self, results: Dict[str, Any], output_file: str):
        """ä¿å­˜ç²¾ç‚¼ç»“æœ"""
        output_dir = os.path.dirname(output_file)
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
            
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ç²¾ç‚¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å®ä½“èšç±»ç²¾ç‚¼å™¨v10")
    parser.add_argument("--config", type=str, default="kg_config.json", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--data_dir", type=str, default=None, help="åˆ†æç»“æœçš„åŸºå‡†ç›®å½•ï¼Œç”¨äºç¡®å®šè¾“å…¥è¾“å‡ºä½ç½®")
    parser.add_argument("--input", type=str, default="clusters_retrieved.json", help="è¾“å…¥æ–‡ä»¶å")
    parser.add_argument("--output", type=str, default="clusters_retrieved_refined.json", help="è¾“å‡ºæ–‡ä»¶å")
    
    args = parser.parse_args()
    
    print("ğŸ”§ å®ä½“èšç±»ç²¾ç‚¼å™¨v10")
    print("=" * 50)

    config = EntityClusterRefiner._load_config(args.config)
    
    base_dir = args.data_dir or config.get("data_source", {}).get("analysis_results_dir")
    if not base_dir:
        print("âŒ é”™è¯¯ï¼šæœªåœ¨é…ç½®æˆ–å‘½ä»¤è¡Œä¸­æŒ‡å®šåŸºå‡†ç›®å½• (analysis_results_dir)")
        return

    if not os.path.isabs(base_dir):
        script_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(script_dir)
        
        resolved_path = os.path.join(project_root, base_dir)
        
        if not os.path.exists(resolved_path):
            project_folder_name = os.path.basename(project_root)
            if project_folder_name in base_dir:
                try:
                    idx = base_dir.index(project_folder_name)
                    suffix = base_dir[idx:]
                    root_parent = os.path.dirname(project_root)
                    resolved_path = os.path.join(root_parent, suffix)
                except ValueError:
                    pass
        base_dir = os.path.abspath(resolved_path)

    if not os.path.exists(base_dir):
        print(f"âŒ é”™è¯¯ï¼šåŸºå‡†ç›®å½•ä¸å­˜åœ¨: {base_dir}")
        return

    input_file_path = os.path.join(base_dir, args.input)
    output_file_path = os.path.join(base_dir, args.output)

    if not os.path.exists(input_file_path):
        print(f"âŒ é”™è¯¯ï¼šè¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file_path}")
        return

    refiner = EntityClusterRefiner(config)
    results = refiner.refine_all_clusters(input_file_path)
    refiner.save_results(results, output_file_path)


if __name__ == "__main__":
    main()