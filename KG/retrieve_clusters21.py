#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®ä½“èšç±»æ£€ç´¢å™¨v4 (MilvusæŸ¥è¯¢ä¿®å¤)
- å¯¹ hardware_feature, optimization_strategy, tunable_parameter è¿›è¡Œèšç±»ã€‚
- ä¸¥æ ¼éµå¾ªâ€œä¸€ä¸ªå®ä½“åªå±äºä¸€ä¸ªç°‡â€çš„è§„åˆ™ã€‚
- ç°‡ä¸­å¿ƒæŒ‰é¡ºåºä»æœªèšç±»çš„å®ä½“ä¸­é€‰å–ã€‚
- è¾“å‡ºæ–‡ä»¶ clusters_retrieved.json ä¿å­˜åˆ° analysis_results_dirã€‚
- ä¿®å¤äº† Milvus collection.query() çš„ expr è¯­æ³•é—®é¢˜ã€‚
"""

import os
import json
from typing import Dict, List, Any
from pymilvus import connections, Collection, utility
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import argparse

class EntityClusterRetriever:
    """å®ä½“èšç±»æ£€ç´¢å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        """åˆå§‹åŒ–èšç±»æ£€ç´¢å™¨"""
        self.config = config
        self.milvus_config = self.config.get("milvus", {})
        self.clustering_config = self.config.get("clustering", {})
        
        self.similarity_threshold = self.clustering_config.get("similarity_threshold", 0.85)
        
        self._connect_milvus()
        
        print("âœ… å®ä½“èšç±»æ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆ")
        self.entity_types_to_export = [
            "hardware_feature",
            "optimization_strategy",
            "tunable_parameter",
        ]
    
    @staticmethod
    def _load_config(config_path: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if not os.path.exists(config_path):
            return {
                "milvus": {"host": "localhost", "port": 19530, "database": "code_op"},
                "clustering": {"similarity_threshold": 0.85}
            }
        
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def _connect_milvus(self):
        """è¿æ¥Milvusæ•°æ®åº“"""
        host = self.milvus_config.get("host", "localhost")
        port = self.milvus_config.get("port", 19530)
        database = self.milvus_config.get("database", "code_op")
        
        connections.connect(alias="default", host=host, port=port, db_name=database)
        print(f"âœ… å·²è¿æ¥åˆ°Milvus: {host}:{port}/{database}")
    
    # <<< MODIFIED: Corrected the query expression
    def _get_all_entities(self, collection_name: str) -> List[Dict[str, Any]]:
        """è·å–é›†åˆä¸­çš„æ‰€æœ‰å®ä½“"""
        if not utility.has_collection(collection_name):
            print(f"  âš ï¸ é›†åˆ {collection_name} ä¸å­˜åœ¨ï¼Œè·³è¿‡ã€‚")
            return []
            
        collection = Collection(collection_name)
        collection.load()
        
        num_entities = collection.num_entities
        if num_entities == 0:
            return []

        # åŠ¨æ€è·å–ä¸»é”®å­—æ®µå
        primary_key_field = next((f.name for f in collection.schema.fields if f.is_primary), None)
        if not primary_key_field:
            raise ValueError(f"é›†åˆ {collection_name} ä¸­æœªæ‰¾åˆ°ä¸»é”®å­—æ®µã€‚")

        # ä½¿ç”¨ä¸€ä¸ªå§‹ç»ˆä¸ºçœŸçš„è¡¨è¾¾å¼ï¼Œä¾‹å¦‚ "pk_field != ''"
        query_expr = f'{primary_key_field} != ""'

        results = collection.query(
            expr=query_expr,
            output_fields=["*"],
            limit=16384 # Milvus's max limit per query
        )
        print(f"ğŸ“Š {collection_name}: åŠ è½½ {len(results)} ä¸ªå®ä½“")
        return results
    
    def _calculate_similarity_matrix(self, embeddings: List[List[float]]) -> np.ndarray:
        """è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ"""
        embeddings_array = np.array(embeddings)
        similarity_matrix = cosine_similarity(embeddings_array)
        return similarity_matrix
    
    def _cluster_entities(self, entities: List[Dict[str, Any]], entity_type: str) -> List[List[int]]:
        """
        å¯¹å®ä½“è¿›è¡Œèšç±»ã€‚
        ä¸¥æ ¼éµå¾ªè§„åˆ™ï¼šä¸€ä¸ªå®ä½“ï¼ˆæ— è®ºæ˜¯ä¸­å¿ƒè¿˜æ˜¯æˆå‘˜ï¼‰åªèƒ½å±äºä¸€ä¸ªç°‡ã€‚
        """
        if not entities:
            return []
        
        embeddings = [entity["embedding"] for entity in entities]
        similarity_matrix = self._calculate_similarity_matrix(embeddings)
        
        num_entities = len(entities)
        unclustered_indices = set(range(num_entities))
        clusters = []

        while unclustered_indices:
            center_idx = sorted(list(unclustered_indices))[0]
            new_cluster = [center_idx]
            unclustered_indices.remove(center_idx)
            
            potential_members = list(unclustered_indices)
            for member_idx in potential_members:
                if similarity_matrix[center_idx][member_idx] >= self.similarity_threshold:
                    new_cluster.append(member_idx)
                    unclustered_indices.remove(member_idx)
            
            clusters.append(new_cluster)
        
        print(f"  ğŸ” {entity_type}: {num_entities} ä¸ªå®ä½“ -> {len(clusters)} ä¸ªç°‡")
        return clusters

    def _format_clusters_map(self, entities: List[Dict[str, Any]], clusters: List[List[int]]) -> Dict[str, Dict[str, Any]]:
        """æ ¼å¼åŒ–ä¸º { cluster_k: { center_uid, center_name, members: [ {uid,name,score} ] } }"""
        if not entities:
            return {}
            
        embeddings = np.array([e["embedding"] for e in entities])
        clusters_map: Dict[str, Dict[str, Any]] = {}
        
        for cluster_idx, idx_list in enumerate(clusters):
            if not idx_list:
                continue
            
            center_idx = idx_list[0]
            center_entity = entities[center_idx]
            center_vec = embeddings[center_idx].reshape(1, -1)
            
            members = []
            for member_idx in idx_list[1:]:
                sim = float(cosine_similarity(center_vec, embeddings[member_idx].reshape(1, -1))[0][0])
                member_entity = entities[member_idx]
                members.append({
                    "uid": member_entity["uid"],
                    "name": member_entity.get("name", member_entity.get("type", "")),
                    "score": sim
                })
            
            members.sort(key=lambda x: x["score"], reverse=True)

            clusters_map[f"cluster_{cluster_idx}"] = {
                "center_uid": center_entity["uid"],
                "center_name": center_entity.get("name", center_entity.get("type", "")),
                "members": members
            }
        return clusters_map
    
    def retrieve_and_cluster(self) -> Dict[str, Any]:
        """æ£€ç´¢æ‰€æœ‰æŒ‡å®šå®ä½“ç±»å‹çš„èšç±»"""
        print("ğŸš€ å¼€å§‹å®ä½“èšç±»æ£€ç´¢")
        
        final_clusters: Dict[str, Any] = {}
        
        for entity_type in self.entity_types_to_export:
            print(f"\nğŸ“‹ å¤„ç†å®ä½“ç±»å‹: {entity_type}")
            
            try:
                entities = self._get_all_entities(entity_type)
                
                if not entities:
                    print(f"  âš ï¸ {entity_type}: æ— å®ä½“æ•°æ®ï¼Œè·³è¿‡ã€‚")
                    final_clusters[entity_type] = {}
                    continue
                
                clusters = self._cluster_entities(entities, entity_type)
                
                clusters_map = self._format_clusters_map(entities, clusters)
                final_clusters[entity_type] = clusters_map
                
            except Exception as e:
                print(f"  âŒ åœ¨å¤„ç† {entity_type} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                import traceback
                traceback.print_exc()
                final_clusters[entity_type] = {}
        
        print(f"\nğŸ‰ èšç±»æ£€ç´¢å®Œæˆ")
        return final_clusters
    
    def save_results(self, results: Dict[str, Any], output_file: str):
        """ä¿å­˜èšç±»ç»“æœ"""
        output_dir = os.path.dirname(output_file)
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
            
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ èšç±»ç»“æœå·²ä¿å­˜: {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å®ä½“èšç±»æ£€ç´¢å™¨v4")
    parser.add_argument("--config", type=str, default="kg_config.json", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--data_dir", type=str, default=None, help="åˆ†æç»“æœçš„åŸºå‡†ç›®å½•ï¼Œç”¨äºç¡®å®šè¾“å‡ºä½ç½®")
    parser.add_argument("--output", type=str, default="clusters_retrieved.json", help="è¾“å‡ºæ–‡ä»¶å")
    
    args = parser.parse_args()
    
    print("ğŸ” å®ä½“èšç±»æ£€ç´¢å™¨v4")
    print("=" * 50)

    config = EntityClusterRetriever._load_config(args.config)
    
    base_dir = args.data_dir or config.get("data_source", {}).get("analysis_results_dir")
    if not base_dir:
        print("âŒ é”™è¯¯ï¼šæœªåœ¨é…ç½®æˆ–å‘½ä»¤è¡Œä¸­æŒ‡å®šåŸºå‡†ç›®å½• (analysis_results_dir)")
        return

    if not os.path.isabs(base_dir):
        script_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(script_dir)
        
        resolved_path = os.path.join(project_root, base_dir)
        
        if not os.path.exists(resolved_path):
            project_folder_name = os.path.basename(project_root)
            if project_folder_name in base_dir:
                try:
                    idx = base_dir.index(project_folder_name)
                    suffix = base_dir[idx:]
                    root_parent = os.path.dirname(project_root)
                    resolved_path = os.path.join(root_parent, suffix)
                except ValueError:
                    pass
        base_dir = os.path.abspath(resolved_path)

    if not os.path.exists(base_dir):
        print(f"âŒ é”™è¯¯ï¼šåŸºå‡†ç›®å½•ä¸å­˜åœ¨: {base_dir}")
        return

    output_file_path = os.path.join(base_dir, args.output)
    
    retriever = EntityClusterRetriever(config)
    results = retriever.retrieve_and_cluster()
    retriever.save_results(results, output_file_path)


if __name__ == "__main__":
    main()