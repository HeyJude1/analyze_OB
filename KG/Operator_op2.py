#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
OpenBLASä¼˜åŒ–ç­–ç•¥æ£€ç´¢ä¸è¯„åˆ†ç³»ç»Ÿv3 (é«˜çº§è¯„åˆ†ç‰ˆ)
- ä¸¥æ ¼éµå¾ª agent23 çš„å››é˜¶æ®µè®¡ç®—æµç¨‹è¯†åˆ«ã€‚
- å¢åŠ  Milvus ç›¸ä¼¼åº¦æ£€ç´¢ä¸æœ€é«˜åˆ†ç­›é€‰ã€‚
- æ ¹æ®å…³è”å…³ç³»æŸ¥æ‰¾ä¼˜åŒ–ç­–ç•¥ã€‚
- å®ç°æ–°çš„ã€åŸºäºä¸Šä¸‹æ–‡çš„è¯„åˆ†ä¸ç­›é€‰é€»è¾‘ã€‚
"""

import os
import json
from typing import Dict, List, Any
from pymilvus import connections, Collection, utility
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain.output_parsers import StructuredOutputParser, ResponseSchema
from langchain_community.embeddings import DashScopeEmbeddings
import argparse
from dotenv import load_dotenv

# åŠ¨æ€å¯¼å…¥ agent23 ä¸­çš„ AgentFactory
from agent23 import AgentFactory

load_dotenv()


class OptimizationStrategyOperator:
    """ä¼˜åŒ–ç­–ç•¥æ“ä½œå™¨"""
    
    def __init__(self, config_path: str = "kg_config.json"):
        """åˆå§‹åŒ–æ“ä½œå™¨"""
        self.config = self._load_config(config_path)
        self.milvus_config = self.config.get("milvus", {})
        self.model_config = self.config.get("model", {})
        self.embedding_config = self.config.get("dashscope_embeddings", {})
        
        self._connect_milvus()
        self._init_llm()
        self._init_embedding_model()
        
        # AgentFactory for pattern detection
        self.agent_factory = AgentFactory()
        
        print("âœ… ä¼˜åŒ–ç­–ç•¥æ“ä½œå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if not os.path.exists(config_path):
            return {
                "milvus": {"host": "localhost", "port": 19530, "database": "code_op"},
                "model": {
                    "name": "qwen-max",
                    "temperature": 0.0,
                    "max_tokens": 8192,
                    "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1"
                },
                "dashscope_embeddings": {"name": "text-embedding-v3"}
            }
        
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def _connect_milvus(self):
        """è¿æ¥Milvusæ•°æ®åº“"""
        host = self.milvus_config.get("host", "localhost")
        port = self.milvus_config.get("port", 19530)
        database = self.milvus_config.get("database", "code_op")
        
        connections.connect(alias="default", host=host, port=port, db_name=database)
        print(f"âœ… å·²è¿æ¥åˆ°Milvus: {host}:{port}/{database}")

    def _init_llm(self):
        """åˆå§‹åŒ– ChatOpenAI æ¨¡å‹"""
        self.llm = ChatOpenAI(
            model=self.model_config.get("name"),
            temperature=float(self.model_config.get("temperature", 0.0)),
            max_tokens=int(self.model_config.get("max_tokens", 8192)),
            api_key=os.getenv("DASHSCOPE_API_KEY"),
            base_url=self.model_config.get("base_url"),
        )
        
    def _init_embedding_model(self):
        """åˆå§‹åŒ– Embedding æ¨¡å‹"""
        api_key = os.getenv("DASHSCOPE_API_KEY")
        if not api_key:
            raise RuntimeError("DASHSCOPE_API_KEY is required for embedding model")
        
        self.embedding_model = DashScopeEmbeddings(
            model=self.embedding_config.get("name", "text-embedding-v3"), 
            dashscope_api_key=api_key
        )

    def _detect_computational_patterns(self, source_code: str) -> List[Dict[str, Any]]:
        """ä½¿ç”¨AgentFactoryæŒ‰å››ä¸ªé˜¶æ®µæ£€æµ‹è®¡ç®—æµç¨‹æ¨¡å¼"""
        all_patterns = []
        stages = ["prep", "transform", "core", "post"]
        
        for stage in stages:
            print(f"  -> æ­£åœ¨è¯†åˆ« {stage} é˜¶æ®µçš„è®¡ç®—æµç¨‹...")
            try:
                patterns = self.agent_factory.analyze_patterns_stage(source_code, "unknown", stage)
                if patterns:
                    all_patterns.extend(patterns)
                    print(f"    âœ… {stage} é˜¶æ®µè¯†åˆ«åˆ° {len(patterns)} ä¸ªæ¨¡å¼")
            except Exception as e:
                print(f"    âŒ {stage} é˜¶æ®µè¯†åˆ«å¤±è´¥: {e}")
        
        return all_patterns

    def _search_similar_patterns(self, detected_patterns: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """åœ¨Milvusä¸­æ£€ç´¢ä¸æ£€æµ‹åˆ°çš„è®¡ç®—æµç¨‹ç›¸ä¼¼çš„å®ä½“"""
        if not detected_patterns:
            return []

        collection = Collection("computational_pattern")
        collection.load()
        
        # ä¸ºæ£€æµ‹åˆ°çš„æ¨¡å¼ç”Ÿæˆå‘é‡
        embedding_texts = [json.dumps(p, ensure_ascii=False) for p in detected_patterns]
        vectors_to_search = self.embedding_model.embed_documents(embedding_texts)
        
        search_params = {"metric_type": "L2", "params": {"nprobe": 10}}
        all_hits = []

        results = collection.search(
            data=vectors_to_search,
            anns_field="embedding",
            param=search_params,
            limit=10, # æ¯ä¸ªæ£€æµ‹åˆ°çš„æ¨¡å¼æ£€ç´¢10ä¸ªæœ€ç›¸ä¼¼çš„
            output_fields=["uid", "name", "type"]
        )
        
        for i, hits in enumerate(results):
            for hit in hits:
                if hit.distance <= 0.2: # ç›¸ä¼¼åº¦é˜ˆå€¼ (1 - L2è·ç¦»)ï¼Œ0.2è¡¨ç¤ºéå¸¸ç›¸ä¼¼
                    all_hits.append({
                        "uid": hit.entity.get("uid"),
                        "name": hit.entity.get("name"),
                        "type": hit.entity.get("type"),
                        "similarity": 1 - hit.distance,
                        "query_pattern": detected_patterns[i]['name']
                    })
        return all_hits

    def _filter_top_patterns(self, similar_patterns: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ä»ç›¸ä¼¼ç»“æœä¸­ä¸ºæ¯ç§ç±»å‹ç­›é€‰å‡ºå¾—åˆ†æœ€é«˜çš„å®ä½“"""
        top_patterns = {}
        for pattern in similar_patterns:
            ptype = pattern['type']
            if ptype not in top_patterns or pattern['similarity'] > top_patterns[ptype]['similarity']:
                top_patterns[ptype] = pattern
        return list(top_patterns.values())

    def _find_related_strategies(self, top_patterns: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ ¹æ®è®¡ç®—æµç¨‹æŸ¥æ‰¾å…³è”çš„ä¼˜åŒ–ç­–ç•¥"""
        if not top_patterns:
            return []
            
        pattern_uids = [p['uid'] for p in top_patterns]
        relation_col = Collection("relation")
        strategy_col = Collection("optimization_strategy")
        
        expr = f'head_entity_uid in {json.dumps(pattern_uids)} and relation_type == "OPTIMIZES_PATTERN"'
        relations = relation_col.query(expr, output_fields=["tail_entity_uid"])
        
        strategy_uids = list({rel['tail_entity_uid'] for rel in relations})
        if not strategy_uids:
            return []
            
        strategies = strategy_col.query(f'uid in {json.dumps(strategy_uids)}', output_fields=["*"])
        return strategies

    def _score_and_filter_strategies(self, strategies: List[Dict[str, Any]], detected_pattern_types: List[str]) -> List[Dict[str, Any]]:
        """æ ¹æ®è‡ªå®šä¹‰è¯„åˆ†å…¬å¼ç­›é€‰ç­–ç•¥"""
        scored_strategies = []
        w_context = 0.5
        
        for strategy in strategies:
            try:
                entity_data = json.loads(strategy.get("entity_data", "{}"))
                context = entity_data.get("optimization_context", {})
                core_patterns = context.get("core_patterns", [])
                contextual_patterns = context.get("contextual_patterns", {})
                
                # æ¡ä»¶1: æ ¸å¿ƒæ¨¡å¼å¿…é¡»æ˜¯æ£€æµ‹åˆ°æ¨¡å¼çš„å­é›†
                if not set(core_patterns).issubset(set(detected_pattern_types)):
                    continue

                # è®¡ç®—Score_core
                score_core = len(set(core_patterns) & set(detected_pattern_types)) / len(detected_pattern_types) if detected_pattern_types else 0

                # è®¡ç®—Score_context
                score_context = 0.0
                for pattern, freq in contextual_patterns.items():
                    if pattern in detected_pattern_types:
                        score_context += freq
                
                # è®¡ç®—æ€»åˆ†
                score_total = score_core + w_context * score_context
                
                # æ¡ä»¶2: æ€»åˆ† >= 0.5
                if score_total >= 0.5:
                    strategy_info = {
                        "strategy_uid": strategy['uid'],
                        "strategy_name": strategy['name'],
                        "level": strategy['level'],
                        "overview": entity_data.get('rationale', ''),
                        "when_to_use": entity_data.get('applicability_conditions', ''),
                        "hardware": entity_data.get('target_hardware_feature', ''),
                        "key_actions": (entity_data.get('implementation_pattern', '') or '').split('\n'),
                        "code_examples": [], # This would require another query if needed
                        "parameters": entity_data.get('tunable_parameters', []),
                        "cautions": entity_data.get('trade_offs', ''),
                        "related_patterns": entity_data.get('related_patterns', []),
                        "optimization_context": context,
                        "score": score_total
                    }
                    scored_strategies.append(strategy_info)
            except Exception as e:
                print(f"  âš ï¸ è¯„åˆ†ç­–ç•¥ {strategy.get('uid')} å¤±è´¥: {e}")
                
        # æŒ‰åˆ†æ•°é™åºæ’åˆ—
        scored_strategies.sort(key=lambda x: x['score'], reverse=True)
        return scored_strategies

    def process_source_code(self, source_file: str) -> Dict[str, Any]:
        """å¤„ç†æºä»£ç æ–‡ä»¶ï¼Œæ‰§è¡Œå®Œæ•´çš„æ£€ç´¢å’Œè¯„åˆ†æµç¨‹"""
        print(f"ğŸš€ å¼€å§‹å¤„ç†æºä»£ç : {source_file}")
        
        if not os.path.exists(source_file):
            return {"error": f"æºæ–‡ä»¶ä¸å­˜åœ¨: {source_file}"}
        
        with open(source_file, 'r', encoding='utf-8') as f:
            source_code = f.read()
        
        # 1. è¯†åˆ«è®¡ç®—æµç¨‹
        patterns_detected_full = self._detect_computational_patterns(source_code)
        patterns_detected_types = [p['pattern_type'] for p in patterns_detected_full]
        print(f"âœ… æ­¥éª¤1å®Œæˆ: æ£€æµ‹åˆ° {len(patterns_detected_types)} ä¸ªè®¡ç®—æµç¨‹: {patterns_detected_types}")
        
        # 2. æ£€ç´¢ç›¸ä¼¼è®¡ç®—æµç¨‹
        similar_patterns = self._search_similar_patterns(patterns_detected_full)
        print(f"âœ… æ­¥éª¤2å®Œæˆ: æ£€ç´¢åˆ° {len(similar_patterns)} ä¸ªç›¸ä¼¼è®¡ç®—æµç¨‹ (ç›¸ä¼¼åº¦ > 0.8)")

        # 3. ç­›é€‰æ¯ç§ç±»å‹çš„æœ€é«˜åˆ†
        top_patterns = self._filter_top_patterns(similar_patterns)
        print(f"âœ… æ­¥éª¤3å®Œæˆ: ç­›é€‰å‡º {len(top_patterns)} ä¸ªæœ€é«˜åˆ†è®¡ç®—æµç¨‹")

        # 4. æŸ¥æ‰¾å…³è”çš„ä¼˜åŒ–ç­–ç•¥
        search_strategies = self._find_related_strategies(top_patterns)
        print(f"âœ… æ­¥éª¤4å®Œæˆ: æ‰¾åˆ° {len(search_strategies)} ä¸ªå…³è”çš„ä¼˜åŒ–ç­–ç•¥")

        # 5. è¯„åˆ†å’Œç­›é€‰
        scored_strategies = self._score_and_filter_strategies(search_strategies, patterns_detected_types)
        print(f"âœ… æ­¥éª¤5å®Œæˆ: æœ€ç»ˆç­›é€‰å‡º {len(scored_strategies)} ä¸ªé«˜åˆ†ç­–ç•¥")

        result = {
            "source_file": source_file,
            "patterns_detected": patterns_detected_full,
            "similar_patterns_found": similar_patterns,
            "top_patterns_per_type": top_patterns,
            "search_strategies": [s['name'] for s in search_strategies],
            "scored_strategies": scored_strategies
        }
        return result
    
    def save_results(self, results: Dict[str, Any], output_file: str):
        """ä¿å­˜å¤„ç†ç»“æœ"""
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜: {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ä¼˜åŒ–ç­–ç•¥æ£€ç´¢ä¸è¯„åˆ†ç³»ç»Ÿv3")
    parser.add_argument("--source", type=str, required=True, help="æºä»£ç æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", type=str, default="opinfo2.json", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--config", type=str, default="kg_config.json", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    print("âš–ï¸ ä¼˜åŒ–ç­–ç•¥æ£€ç´¢ä¸è¯„åˆ†ç³»ç»Ÿv3")
    print("=" * 50)
    
    config = OptimizationStrategyOperator._load_config(args.config)
    operator = OptimizationStrategyOperator(config)
    results = operator.process_source_code(args.source)
    operator.save_results(results, args.output)


if __name__ == "__main__":
    main()