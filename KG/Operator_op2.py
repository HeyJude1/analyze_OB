#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
OpenBLASä¼˜åŒ–ç­–ç•¥æ£€ç´¢ä¸è¯„åˆ†ç³»ç»Ÿv5 (TypeErrorä¿®å¤ç‰ˆ)
- å°† agent23 çš„å››é˜¶æ®µè®¡ç®—æµç¨‹è¯†åˆ«é€»è¾‘å®Œå…¨é›†æˆåˆ°æœ¬æ–‡ä»¶ä¸­ã€‚
- ç§»é™¤å¯¹ agent23.py çš„å¤–éƒ¨ä¾èµ–ã€‚
- ä¿æŒç›¸ä¼¼åº¦æ£€ç´¢ã€å…³è”ç­–ç•¥æŸ¥æ‰¾å’Œé«˜çº§è¯„åˆ†é€»è¾‘ä¸å˜ã€‚
- è¾“å…¥æ–‡ä»¶ç¡¬ç¼–ç ä¸ºåŒç›®å½•ä¸‹çš„ gemm.txtã€‚
- è¾“å‡ºæ–‡ä»¶è·¯å¾„æ ¹æ®é…ç½®æ–‡ä»¶è‡ªåŠ¨ç¡®å®šã€‚
- ä¿®å¤äº†å› é”™è¯¯è°ƒç”¨å®ä¾‹æ–¹æ³•å¯¼è‡´çš„TypeErrorã€‚
"""

import os
import json
import time
from typing import Dict, List, Any, Optional
from pathlib import Path
from pymilvus import connections, Collection, utility
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain.output_parsers import StructuredOutputParser, ResponseSchema
from langchain_community.embeddings import DashScopeEmbeddings
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain_core.tools import tool
import argparse
from dotenv import load_dotenv

load_dotenv()


# ===== åŸºç¡€å·¥å…· (ä¸ºAgentæä¾›) =====
@tool
def read_source_file(file_path: str) -> str:
    """(æ­¤å·¥å…·ä»…ä¸ºAgentå†…éƒ¨ä½¿ç”¨) è¯»å–æºä»£ç æ–‡ä»¶ã€‚"""
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read(15000)
        return f"æ–‡ä»¶è·¯å¾„: {file_path}\nå†…å®¹:\n{content}\n..."
    except Exception as e:
        return f"è¯»å–å¤±è´¥: {str(e)}"


class OptimizationStrategyOperator:
    """ä¼˜åŒ–ç­–ç•¥æ“ä½œå™¨"""
    
    # <<< MODIFIED: __init__ now accepts the config dictionary directly
    def __init__(self, config: Dict[str, Any]):
        """åˆå§‹åŒ–æ“ä½œå™¨"""
        self.config = config
        self.milvus_config = self.config.get("milvus", {})
        self.model_config = self.config.get("model", {})
        self.embedding_config = self.config.get("dashscope_embeddings", {})
        
        self._connect_milvus()
        self._init_llm()
        self._init_embedding_model()
        
        print("âœ… ä¼˜åŒ–ç­–ç•¥æ“ä½œå™¨åˆå§‹åŒ–å®Œæˆ")
    
    # <<< MODIFIED: Changed to a staticmethod
    @staticmethod
    def _load_config(config_path: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if not os.path.exists(config_path):
            return {
                "milvus": {"host": "localhost", "port": 19530, "database": "code_op"},
                "model": {
                    "name": "qwen-max",
                    "temperature": 0.0,
                    "max_tokens": 8192,
                    "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1"
                },
                "dashscope_embeddings": {"name": "text-embedding-v3"}
            }
        
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def _connect_milvus(self):
        """è¿æ¥Milvusæ•°æ®åº“"""
        host = self.milvus_config.get("host", "localhost")
        port = self.milvus_config.get("port", 19530)
        database = self.milvus_config.get("database", "code_op")
        
        connections.connect(alias="default", host=host, port=port, db_name=database)
        print(f"âœ… å·²è¿æ¥åˆ°Milvus: {host}:{port}/{database}")

    def _init_llm(self):
        """åˆå§‹åŒ– ChatOpenAI æ¨¡å‹"""
        self.llm = ChatOpenAI(
            model=self.model_config.get("name"),
            temperature=float(self.model_config.get("temperature", 0.0)),
            max_tokens=int(self.model_config.get("max_tokens", 8192)),
            api_key=os.getenv("DASHSCOPE_API_KEY"),
            base_url=self.model_config.get("base_url"),
        )
        
    def _init_embedding_model(self):
        """åˆå§‹åŒ– Embedding æ¨¡å‹"""
        api_key = os.getenv("DASHSCOPE_API_KEY")
        if not api_key:
            raise RuntimeError("DASHSCOPE_API_KEY is required for embedding model")
        
        self.embedding_model = DashScopeEmbeddings(
            model=self.embedding_config.get("name", "text-embedding-v3"), 
            dashscope_api_key=api_key
        )

    # ===== START: AgentFactory Logic Integration =====
    
    def _create_pattern_parser(self) -> StructuredOutputParser:
        schemas = [
            ResponseSchema(name="computational_patterns", description=(
                "è®¡ç®—æµç¨‹åˆ—è¡¨ã€‚æ¯é¡¹åŒ…å«: pattern_type(æµç¨‹ç±»å‹æ ‡ç­¾), name(æµç¨‹ä¸­æ–‡åç§°), "
                "description(å¯¹æµç¨‹çš„ç®€è¦è¯´æ˜), code(è¯¥æµç¨‹æœ€ç›¸å…³çš„å®Œæ•´ä»£ç ç‰‡æ®µ), "
                "data_object_features(å¯¹è±¡ï¼Œå« numeric_kind, numeric_precision, structural_properties, storage_layout å››é”®)"
            )),
        ]
        return StructuredOutputParser.from_response_schemas(schemas)

    def create_prep_pattern_agent(self) -> AgentExecutor:
        tools = [read_source_file]
        parser = self._create_pattern_parser()
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯"ç»†ç²’åº¦è®¡ç®—æµç¨‹"è¯†åˆ«ä¸“å®¶ã€‚ä»…è¯†åˆ«"é˜¶æ®µä¸€ï¼šè®¡ç®—å‡†å¤‡ (Computation Preparation)"ä¸­çš„æµç¨‹ï¼Œå¦‚æœè¯†åˆ«åˆ°ä»¥ä¸‹æŸä¸ªè®¡ç®—æµç¨‹ï¼Œåˆ™éœ€è¦å¹¶ä¸¥æ ¼æŒ‰æ¨¡æ¿è¾“å‡ºã€‚

ã€é˜¶æ®µä¸€ç›®å½•ã€‘
1) prep.parameter_validationï¼ˆå‚æ•°åˆæ³•æ€§æ ¡éªŒï¼‰
   - description: æ ¹æ®æ‰€ç»™çš„å®é™…ä»£ç ç”Ÿæˆå…·ä½“æè¿°ï¼Œæ£€æŸ¥ n/m/k/inc_x/inc_y/lda ç­‰è¾¹ç•Œå¹¶å¯æå‰é€€å‡ºï¼Œå…¸å‹å½¢æ€ if (...) returnã€‚
   - code: è´´å‡ºå®ç°æ ¡éªŒä¸æ—©é€€çš„å®Œæ•´ä»£ç ç‰‡æ®µã€‚
   - data_object_features:
     -numeric_kind=N/A
     -numeric_precision=N/A
     -structural_properties=N/A
     -storage_layout=N/A

2) prep.index_pointer_initï¼ˆç´¢å¼•ä¸æŒ‡é’ˆåˆå§‹åŒ–ï¼‰
   - description: æ ¹æ®æ‰€ç»™çš„å®é™…ä»£ç ç”Ÿæˆå…·ä½“æè¿°ï¼Œè¯´æ˜åˆå§‹åŒ–äº†å“ªäº›å…·ä½“å˜é‡ã€å˜é‡çš„ä½œç”¨å’Œåˆå§‹å€¼ã€‚è¯†åˆ«æ ‡å‡†ï¼Œåˆå§‹åŒ–å¾ªç¯å˜é‡/ç´¯åŠ å™¨/æŒ‡é’ˆèµ·ç‚¹ï¼ˆå¦‚ i=0, sum=0, ptr=a ç­‰ï¼‰ã€‚
   - code: è´´å‡ºåˆå§‹åŒ–ç›¸å…³çš„å®Œæ•´ä»£ç ç‰‡æ®µã€‚
   - data_object_features: 
     -numeric_kind=å®æ•°/å¤æ•°/ä¸é€‚ç”¨åˆ™ç”¨N/Aï¼ˆåˆ¤æ–­ä¾æ®æ˜¯æ£€æŸ¥ç´¯åŠ å™¨ç±»å‹æ˜¯å¦ä¸º`_Complex`æˆ–ç»“æ„ä½“ï¼Œè‹¥ä»…ä¸ºç´¢å¼•/æŒ‡é’ˆï¼Œåˆ™ä¸º"N/A"ï¼‰
     -numeric_precision= å•ç²¾åº¦/åŒç²¾åº¦/ä¸é€‚ç”¨åˆ™ç”¨N/Aï¼ˆåˆ¤æ–­ä¾æ®æ˜¯æ£€æŸ¥ç´¯åŠ å™¨æˆ–æŒ‡é’ˆçš„ç±»å‹æ˜¯`float`è¿˜æ˜¯`double`ï¼Œè‹¥ä»…ä¸ºæ•´æ•°ç´¢å¼•ï¼Œåˆ™ä¸º"N/A"ï¼‰
     -structural_properties=N/A
     -storage_layout=N/A

3) prep.loop_invariant_calcï¼ˆå¾ªç¯ä¸å˜é‡è®¡ç®—ï¼‰ 
   - description: æ ¹æ®æ‰€ç»™çš„å®é™…ä»£ç ç”Ÿæˆå…·ä½“æè¿°ï¼Œè¯´æ˜è®¡ç®—äº†å“ªäº›å…·ä½“çš„ä¸å˜é‡ã€è®¡ç®—å…¬å¼å’Œç”¨é€”ã€‚è¯†åˆ«æ ‡å‡†ï¼Œåœ¨å¾ªç¯å¤–è®¡ç®—å¹¶åœ¨å¾ªç¯å†…å¤ç”¨çš„ä¸å˜é‡ï¼ˆå¦‚ inc_x2=2*inc_x, lda2=2*lda ç­‰ï¼‰ã€‚
   - code: è´´å‡ºç›¸å…³èµ‹å€¼ç‰‡æ®µã€‚
   - data_object_features: 
     -numeric_kind=å®æ•°/å¤æ•°ï¼ˆåˆ¤æ–­ä¾æ®æ˜¯å¦‚ 2*inc_x ç”¨äºå¤æ•°äº¤é”™æ—¶æ ‡è®°å¤æ•°ï¼‰
     -numeric_precision=N/A
     -structural_properties=N/A
     -storage_layout=è·¨æ­¥

ã€è¾“å‡ºè¦æ±‚ã€‘
- âš ï¸ é‡è¦ï¼šåªæœ‰åœ¨ä»£ç ä¸­æ˜ç¡®æ‰¾åˆ°ç›¸åº”è®¡ç®—æµç¨‹æ—¶æ‰è¾“å‡ºè¯¥æµç¨‹ï¼å¦‚æœä»£ç ä¸­æ²¡æœ‰æŸä¸ªè®¡ç®—æµç¨‹ï¼Œåˆ™å®Œå…¨ä¸è¾“å‡ºè¯¥æµç¨‹çš„JSONå¯¹è±¡ã€‚
- ä¸¥æ ¼è¾“å‡º JSON æ•°ç»„ã€‚æ•°ç»„ä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªå¯¹è±¡ï¼Œä¸”å¿…é¡»åŒ…å«ä»¥ä¸‹äº”ä¸ªå­—æ®µï¼š
  - pattern_type, name, description, code, data_object_featuresã€‚
- ç‰¹åˆ«è¯´æ˜ï¼š
  - å¯¹äº prep.parameter_validationï¼šåªæœ‰å½“ä»£ç ä¸­å­˜åœ¨ if(...) return å½¢å¼çš„å‚æ•°æ£€æŸ¥æ—¶æ‰è¾“å‡º
  - å¯¹äº prep.index_pointer_initï¼šåªæœ‰å½“ä»£ç ä¸­å­˜åœ¨å˜é‡åˆå§‹åŒ–è¯­å¥æ—¶æ‰è¾“å‡º  
  - å¯¹äº prep.loop_invariant_calcï¼šåªæœ‰å½“ä»£ç ä¸­å­˜åœ¨å¾ªç¯å¤–çš„é¢„è®¡ç®—èµ‹å€¼æ—¶æ‰è¾“å‡º
- data_object_features å¿…é¡»æ˜¯å¯¹è±¡ï¼ŒåŒ…å«å››é”®ï¼šnumeric_kind, numeric_precision, structural_properties, storage_layoutï¼›å€¼ä»ä¸Šè¿°è¯´æ˜ä¸­é€‰æ‹©ï¼›ä¸é€‚ç”¨åˆ™ç”¨ "N/A"ã€‚
- "åˆ¤æ–­ä¾æ®"ä»…ä¾›ç†è§£ï¼ŒJSONè¾“å‡ºä¸­data_object_featuresåªåŒ…å«å…·ä½“çš„å€¼ï¼Œå¦‚"å®æ•°"ã€"å•ç²¾åº¦"ç­‰ï¼Œä¸å¾—åŒ…å«"åˆ¤æ–­ä¾æ®"ã€‚
- ä¸å¾—å‘æ˜æ–°æ ‡ç­¾ï¼›åªåšæµç¨‹è¯†åˆ«ã€‚
- å¦‚æœæŸä¸ªé˜¶æ®µçš„æ‰€æœ‰è®¡ç®—æµç¨‹éƒ½ä¸å­˜åœ¨ï¼Œåˆ™è¿”å›ç©ºæ•°ç»„ []ã€‚

{format_instructions}
ï¼ˆæç¤ºï¼šä¸Šé¢çš„æ ¼å¼è¯´æ˜æè¿°äº†â€œå•ä¸ªæµç¨‹å¯¹è±¡â€çš„å­—æ®µï¼Œè¯·å°†å®ƒä½œä¸ºæ•°ç»„å…ƒç´ çš„å¯¹è±¡ç»“æ„ã€‚ï¼‰"""),
            ("human", "{input}"),
            ("placeholder", "{agent_scratchpad}"),
        ])
        formatted = prompt.partial(format_instructions=parser.get_format_instructions())
        agent = create_openai_tools_agent(self.llm, tools, formatted)
        return AgentExecutor(agent=agent, tools=tools, verbose=False, max_iterations=10)

    def create_transform_pattern_agent(self) -> AgentExecutor:
        tools = [read_source_file]
        parser = self._create_pattern_parser()
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯"ç»†ç²’åº¦è®¡ç®—æµç¨‹"è¯†åˆ«ä¸“å®¶ã€‚ä»…è¯†åˆ«"é˜¶æ®µäºŒï¼šæ•°æ®è½¬æ¢ä¸é¢„å¤„ç† (Data Transformation & Pre-processing)"ä¸­çš„æµç¨‹ï¼Œå¹¶ä¸¥æ ¼æŒ‰æ¨¡æ¿è¾“å‡ºã€‚

ã€é˜¶æ®µäºŒç›®å½•ã€‘
1) transform.packingï¼ˆè¿ç»­åŒ–æ‹·è´/æ‰“åŒ…ï¼‰
   - description: æ ¹æ®å®é™…ä»£ç ç”Ÿæˆå…·ä½“æè¿°ï¼Œè¯´æ˜ä»å“ªé‡Œæ‹·è´åˆ°å“ªé‡Œã€ä½¿ç”¨äº†ä»€ä¹ˆæ–¹æ³•ã€æºå’Œç›®æ ‡çš„è®¿é—®æ¨¡å¼ã€‚è¯†åˆ«æ ‡å‡†ï¼Œå°†è·¨æ­¥æºæ•°æ®å¤åˆ¶åˆ°è¿ç»­ç¼“å†²åŒºï¼Œå¸¸è§ memcpy æˆ–æ˜¾å¼å¾ªç¯ï¼Œæºç´¢å¼•å«ä¹˜æ³•(â€¦*lda/inc_x)ï¼Œç›®æ ‡ç®€å•é€’å¢ã€‚
   - code: è´´å‡ºæ‹·è´/æ‰“åŒ…å®ç°çš„å®Œæ•´ä»£ç ç‰‡æ®µã€‚
   - data_object_features: 
     -numeric_kind=å®æ•°/å¤æ•°
     -numeric_precision=å•ç²¾åº¦/åŒç²¾åº¦ï¼ˆåˆ¤æ–­ä¾æ®æ˜¯æ£€æŸ¥æº/ç›®æ ‡æŒ‡é’ˆçš„å˜é‡ç±»å‹ï¼‰
     -structural_properties=é€šç”¨
     -storage_layout=(æºæŒ‡é’ˆ)è·¨æ­¥ -> (ç›®æ ‡æŒ‡é’ˆ)è¿ç»­ï¼ˆåˆ¤æ–­ä¾æ®æ˜¯æ£€æŸ¥æºåœ°å€è®¡ç®—æ˜¯å¦å«"*lda"æˆ–"*inc_x"ï¼Œç›®æ ‡åœ°å€æ˜¯å¦ä¸ºç®€å•é€’å¢ï¼‰

2) transform.unpacking_specialï¼ˆç‰¹æ®Šç»“æ„è§£åŒ…/å±•å¼€ï¼‰
   - description: æ ¹æ®å®é™…ä»£ç ç”Ÿæˆå…·ä½“æè¿°ï¼Œè¯´æ˜å¤„ç†äº†å“ªç§ç‰¹æ®Šç»“æ„ã€å¦‚ä½•è¿›è¡Œå±•å¼€ã€æ¶‰åŠçš„åˆ†æ”¯é€»è¾‘ã€‚è¯†åˆ«æ ‡å‡†ï¼Œå°†å¯¹ç§°/å„ç±³ç‰¹/ä¸‰è§’ç­‰ç‰¹æ®Šå­˜å‚¨å±•å¼€ä¸ºé€šç”¨å¸ƒå±€ï¼Œå¸¸å« uplo/diag åˆ†æ”¯ä¸å¤æ‚åœ°å€è®¡ç®—ã€‚
   - code: è´´å‡ºåˆ†æ”¯ä¸è§£åŒ…é€»è¾‘çš„å®Œæ•´ç‰‡æ®µã€‚
   - data_object_features: 
     -numeric_kind=å®æ•°/å¤æ•°
     -numeric_precision=å•/åŒç²¾åº¦
     -structural_properties=å¯¹ç§°/å„ç±³ç‰¹/ä¸‰è§’
     -storage_layout=æ‰“åŒ… -> è¿ç»­ / è·¨æ­¥ -> è¿ç»­

3) transform.transposeï¼ˆæ•°æ®è½¬ç½®ï¼‰
   - description: æ ¹æ®å®é™…ä»£ç ç”Ÿæˆå…·ä½“æè¿°ï¼Œè¯´æ˜å¦‚ä½•å®ç°è½¬ç½®ã€æ¶‰åŠçš„å¾ªç¯ç»“æ„å’Œç´¢å¼•å˜æ¢ã€‚è¯†åˆ«æ ‡å‡†ï¼Œè¡Œåˆ—äº’æ¢ä»¥æ”¹å˜è®¿å­˜å±€éƒ¨æ€§ï¼Œå¸¸è§åŒå±‚å¾ªç¯ B[j][i] = A[i][j]ã€‚
   - code: è´´å‡ºè½¬ç½®å®ç°çš„å®Œæ•´ä»£ç ç‰‡æ®µã€‚
   - data_object_features: 
     -numeric_kind=å®æ•°/å¤æ•°
     -numeric_precision=å•/åŒç²¾åº¦
     -structural_properties=é€šç”¨
     -storage_layout=è·¨æ­¥ï¼ˆåˆ¤æ–­ä¾æ®æ˜¯å¸ƒå±€å‘ç”Ÿå˜åŒ–ï¼‰

ã€è¾“å‡ºè¦æ±‚ã€‘
- âš ï¸ é‡è¦ï¼šåªæœ‰åœ¨ä»£ç ä¸­æ˜ç¡®æ‰¾åˆ°ç›¸åº”è®¡ç®—æµç¨‹æ—¶æ‰è¾“å‡ºè¯¥æµç¨‹ï¼å¦‚æœä»£ç ä¸­æ²¡æœ‰æŸä¸ªè®¡ç®—æµç¨‹ï¼Œåˆ™å®Œå…¨ä¸è¾“å‡ºè¯¥æµç¨‹çš„JSONå¯¹è±¡ã€‚
- ä¸¥æ ¼è¾“å‡º JSON æ•°ç»„ã€‚æ•°ç»„ä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªå¯¹è±¡ï¼Œä¸”å¿…é¡»åŒ…å«ä»¥ä¸‹äº”ä¸ªå­—æ®µï¼š
  - pattern_type, name, description, code, data_object_featuresã€‚
- data_object_features é”®ä¸ºï¼šnumeric_kind, numeric_precision, structural_properties, storage_layoutï¼›ä¸é€‚ç”¨ç”¨ "N/A"ã€‚
- "åˆ¤æ–­ä¾æ®"ä»…ä¾›ç†è§£ï¼ŒJSONè¾“å‡ºä¸­data_object_featuresåªåŒ…å«å…·ä½“çš„å€¼ï¼Œå¦‚"å®æ•°"ã€"å•ç²¾åº¦"ç­‰ï¼Œä¸å¾—åŒ…å«"åˆ¤æ–­ä¾æ®"ã€‚
- ä¸å¾—å‘æ˜æ–°æ ‡ç­¾ï¼›åªåšæµç¨‹è¯†åˆ«ã€‚
- å¦‚æœæŸä¸ªé˜¶æ®µçš„æ‰€æœ‰è®¡ç®—æµç¨‹éƒ½ä¸å­˜åœ¨ï¼Œåˆ™è¿”å›ç©ºæ•°ç»„ []ã€‚

{format_instructions}
ï¼ˆæç¤ºï¼šä¸Šé¢çš„æ ¼å¼è¯´æ˜æè¿°äº†â€œå•ä¸ªæµç¨‹å¯¹è±¡â€çš„å­—æ®µï¼Œè¯·å°†å®ƒä½œä¸ºæ•°ç»„å…ƒç´ çš„å¯¹è±¡ç»“æ„ã€‚ï¼‰"""),
            ("human", "{input}"),
            ("placeholder", "{agent_scratchpad}"),
        ])
        formatted = prompt.partial(format_instructions=parser.get_format_instructions())
        agent = create_openai_tools_agent(self.llm, tools, formatted)
        return AgentExecutor(agent=agent, tools=tools, verbose=False, max_iterations=10)

    def create_core_pattern_agent(self) -> AgentExecutor:
        tools = [read_source_file]
        parser = self._create_pattern_parser()
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯"ç»†ç²’åº¦è®¡ç®—æµç¨‹"è¯†åˆ«ä¸“å®¶ã€‚ä»…è¯†åˆ«"é˜¶æ®µä¸‰ï¼šæ ¸å¿ƒè®¡ç®— (Core Computation)"ä¸­çš„æµç¨‹ï¼Œå¹¶ä¸¥æ ¼æŒ‰æ¨¡æ¿è¾“å‡ºã€‚

ã€é˜¶æ®µä¸‰ç›®å½•ã€‘
1) core.vector_reductionï¼ˆå‘é‡å½’çº¦ï¼‰
   - description: æ ¹æ®å®é™…ä»£ç ç”Ÿæˆå…·ä½“æè¿°ï¼Œè¯´æ˜è¿›è¡Œäº†ä»€ä¹ˆç±»å‹çš„å½’çº¦æ“ä½œã€å…·ä½“çš„è®¡ç®—é€»è¾‘å’Œç´¯ç§¯æ–¹å¼ã€‚è¯†åˆ«æ ‡å‡†ï¼Œç´¯åŠ /æå€¼/èŒƒæ•°ç­‰ï¼›å¸¸è§ sum+= / max/minï¼Œå¯èƒ½å« inc_xã€‚
   - code: è´´å‡ºæ ¸å¿ƒå¾ªç¯å®Œæ•´ç‰‡æ®µã€‚
   - data_object_features: 
     -numeric_kind=å®æ•°/å¤æ•°
     -numeric_precision=å•/åŒç²¾åº¦
     -structural_properties=é€šç”¨
     -storage_layout=è¿ç»­/è·¨æ­¥

2) core.elementwise_updateï¼ˆå…ƒç´ çº§å‘é‡æ›´æ–°ï¼‰
   - description: æ ¹æ®å®é™…ä»£ç ç”Ÿæˆå…·ä½“æè¿°ï¼Œæè¿°å¯¹å‘é‡è¿›è¡Œé€å…ƒç´ è®¡ç®—å’Œæ›´æ–°çš„æ“ä½œï¼Œå¯èƒ½ä¸ºå¤æ•°ä¹˜åŠ ã€‚
   - code: è´´å‡ºæ›´æ–°å¾ªç¯çš„å®Œæ•´ç‰‡æ®µã€‚
   - data_object_features: 
     -numeric_kind=å®æ•°/å¤æ•°
     -numeric_precision=å•/åŒç²¾åº¦
     -structural_properties=é€šç”¨
     -storage_layout=è·¨æ­¥

3) core.gemv_likeï¼ˆçŸ©é˜µ-å‘é‡ä¹˜ï¼‰
   - description: æ ¹æ®å®é™…ä»£ç ç”Ÿæˆå…·ä½“æè¿°ï¼Œè¯´æ˜çŸ©é˜µå’Œå‘é‡çš„ä¹˜æ³•å®ç°æ–¹å¼ã€å¾ªç¯ç»“æ„å’Œè®¡ç®—é€»è¾‘ã€‚è¯†åˆ«æ ‡å‡†ï¼ŒåŒå±‚å¾ªç¯ï¼Œå†…å±‚ç‚¹ç§¯ sum+=A[i,j]*x[j]ã€‚
   - code: è´´å‡º GEMV-like å®ç°çš„å®Œæ•´ç‰‡æ®µã€‚
   - data_object_features: 
     -numeric_kind=å®æ•°/å¤æ•°
     -numeric_precision=å•/åŒç²¾åº¦
     -structural_properties=é€šç”¨/å¸¦çŠ¶/ä¸‰è§’
     -storage_layout=è·¨æ­¥

4) core.rank1_updateï¼ˆç§©-1 æ›´æ–°ï¼‰
   - description: æ ¹æ®å®é™…ä»£ç ç”Ÿæˆå…·ä½“æè¿°ï¼Œè¯´æ˜å¦‚ä½•å®ç°ç§©-1æ›´æ–°ã€æ¶‰åŠçš„å‘é‡å¤–ç§¯è®¡ç®—å’ŒçŸ©é˜µæ›´æ–°æ–¹å¼ã€‚è¯†åˆ«æ ‡å‡†ï¼ŒA+=alpha*x*y^T å¤–ç§¯æ›´æ–°ã€‚
   - code: è´´å‡ºåŒå±‚å¾ªç¯å®Œæ•´ç‰‡æ®µã€‚
   - data_object_features: 
     -numeric_kind=å®æ•°/å¤æ•°
     -numeric_precision=å•/åŒç²¾åº¦
     -structural_properties=é€šç”¨/å¯¹ç§°/å„ç±³ç‰¹
     -storage_layout=è·¨æ­¥

5) core.mm_microkernelï¼ˆçŸ©é˜µä¹˜æ³•å¾®å†…æ ¸ï¼‰
   - description: æ ¹æ®å®é™…ä»£ç ç”Ÿæˆå…·ä½“æè¿°ï¼Œè¯´æ˜å¾®å†…æ ¸çš„å®ç°æ–¹å¼ã€å¯„å­˜å™¨ä½¿ç”¨å’ŒFMAæŒ‡ä»¤åºåˆ—ã€‚è¯†åˆ«æ ‡å‡†ï¼Œå›ºå®šå°ºå¯¸å¯„å­˜å™¨çº§FMAå±•å¼€ï¼›å®Œå…¨å±•å¼€ã€å¯„å­˜å™¨ç´¯åŠ å™¨ã€è§„å¾‹æ€§load/FMAã€‚
   - code: è´´å‡ºå¾®å†…æ ¸çš„å®Œæ•´ç‰‡æ®µï¼ˆåŒ…å«ç´¯åŠ å™¨ä¸FMAåºåˆ—ï¼‰ã€‚
   - data_object_features: 
     -numeric_kind=å®æ•°/å¤æ•°
     -numeric_precision=å•/åŒç²¾åº¦
     -structural_properties=é€šç”¨
     -storage_layout=è¿ç»­

6) core.tiled_loopï¼ˆåˆ†å—çŸ©é˜µå¤„ç†å¾ªç¯ï¼‰
   - description: æ ¹æ®å®é™…ä»£ç ç”Ÿæˆå…·ä½“æè¿°ï¼Œè¯´æ˜åˆ†å—å¾ªç¯çš„å®ç°æ–¹å¼ã€å—å¤§å°å’Œå¾ªç¯åµŒå¥—ç»“æ„ã€‚è¯†åˆ«æ ‡å‡†ï¼Œå¤–å±‚éå†å—ã€é©±åŠ¨å¾®å†…æ ¸çš„ä¸‰å±‚ ijk å¾ªç¯ã€‚
   - code: è´´å‡ºå—å¾ªç¯æ¡†æ¶å®Œæ•´ç‰‡æ®µã€‚
   - data_object_features: 
     -numeric_kind=å®æ•°/å¤æ•°
     -numeric_precision=å•/åŒç²¾åº¦
     -structural_properties=é€šç”¨
     -storage_layout=è·¨æ­¥ï¼ˆåˆ¤æ–­ä¾æ®æ˜¯æ“ä½œåŸå§‹å¤§çŸ©é˜µçš„å­å—ï¼‰

7) core.triangular_solveï¼ˆä¸‰è§’æ±‚è§£/å›ä»£ï¼‰
   - description: æ ¹æ®å®é™…ä»£ç ç”Ÿæˆå…·ä½“æè¿°ï¼Œè¯´æ˜ä¸‰è§’æ±‚è§£çš„å®ç°æ–¹å¼ã€æ±‚è§£é¡ºåºå’Œå›ä»£æ›´æ–°é€»è¾‘ã€‚è¯†åˆ«æ ‡å‡†ï¼Œå°å‹ä¸‰è§’ç³»ç»Ÿæ±‚è§£ä¸å›ä»£æ›´æ–°ï¼›å¸¸è§é€’å¢/é€’å‡å¾ªç¯ã€é™¤æ³•ä¸AXPYæ ·æ›´æ–°ã€‚
   - code: è´´å‡ºç›¸å…³å¾ªç¯å®Œæ•´ç‰‡æ®µã€‚
   - data_object_features: 
     -numeric_kind=å®æ•°/å¤æ•°
     -numeric_precision=å•/åŒç²¾åº¦
     -structural_properties=ä¸‰è§’
     -storage_layout=è·¨æ­¥

ã€è¾“å‡ºè¦æ±‚ã€‘
- âš ï¸ é‡è¦ï¼šåªæœ‰åœ¨ä»£ç ä¸­æ˜ç¡®æ‰¾åˆ°ç›¸åº”è®¡ç®—æµç¨‹æ—¶æ‰è¾“å‡ºè¯¥æµç¨‹ï¼å¦‚æœä»£ç ä¸­æ²¡æœ‰æŸä¸ªè®¡ç®—æµç¨‹ï¼Œåˆ™å®Œå…¨ä¸è¾“å‡ºè¯¥æµç¨‹çš„JSONå¯¹è±¡ã€‚
- ä¸¥æ ¼è¾“å‡º JSON æ•°ç»„ã€‚æ•°ç»„ä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªå¯¹è±¡ï¼Œä¸”å¿…é¡»åŒ…å«ä»¥ä¸‹äº”ä¸ªå­—æ®µï¼š
  - pattern_type, name, description, code, data_object_featuresã€‚
- data_object_features å¿…é¡»æ˜¯å¯¹è±¡ï¼ŒåŒ…å«å››é”®ï¼šnumeric_kind, numeric_precision, structural_properties, storage_layoutï¼›ä¸é€‚ç”¨ç”¨ "N/A"ã€‚
- "åˆ¤æ–­ä¾æ®"ä»…ä¾›ç†è§£ï¼ŒJSONè¾“å‡ºä¸­data_object_featuresåªåŒ…å«å…·ä½“çš„å€¼ï¼Œå¦‚"å®æ•°"ã€"å•ç²¾åº¦"ç­‰ï¼Œä¸å¾—åŒ…å«"åˆ¤æ–­ä¾æ®"ã€‚
- ä¸å¾—å‘æ˜æ–°æ ‡ç­¾ï¼›åªåšæµç¨‹è¯†åˆ«ã€‚
- å¦‚æœæŸä¸ªé˜¶æ®µçš„æ‰€æœ‰è®¡ç®—æµç¨‹éƒ½ä¸å­˜åœ¨ï¼Œåˆ™è¿”å›ç©ºæ•°ç»„ []ã€‚

{format_instructions}
ï¼ˆæç¤ºï¼šä¸Šé¢çš„æ ¼å¼è¯´æ˜æè¿°äº†â€œå•ä¸ªæµç¨‹å¯¹è±¡â€çš„å­—æ®µï¼Œè¯·å°†å®ƒä½œä¸ºæ•°ç»„å…ƒç´ çš„å¯¹è±¡ç»“æ„ã€‚ï¼‰"""),
            ("human", "{input}"),
            ("placeholder", "{agent_scratchpad}"),
        ])
        formatted = prompt.partial(format_instructions=parser.get_format_instructions())
        agent = create_openai_tools_agent(self.llm, tools, formatted)
        return AgentExecutor(agent=agent, tools=tools, verbose=False, max_iterations=10)

    def create_post_pattern_agent(self) -> AgentExecutor:
        tools = [read_source_file]
        parser = self._create_pattern_parser()
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯"ç»†ç²’åº¦è®¡ç®—æµç¨‹"è¯†åˆ«ä¸“å®¶ã€‚ä»…è¯†åˆ«"é˜¶æ®µå››ï¼šåå¤„ç†ä¸å†™å› (Post-processing & Write-back)"ä¸­çš„æµç¨‹ï¼Œå¹¶ä¸¥æ ¼æŒ‰æ¨¡æ¿è¾“å‡ºã€‚

ã€é˜¶æ®µå››ç›®å½•ã€‘
1) post.scale_accumulationï¼ˆç»“æœç¼©æ”¾ä¸ç´¯åŠ ï¼‰
   - description: æ ¹æ®å®é™…ä»£ç ç”Ÿæˆå…·ä½“æè¿°ï¼Œè¯´æ˜å¦‚ä½•è¿›è¡Œç»“æœç¼©æ”¾å’Œç´¯åŠ ã€æ¶‰åŠçš„ç³»æ•°å¤„ç†å’Œå†™å›æ–¹å¼ã€‚è¯†åˆ«æ ‡å‡†ï¼ŒC=alpha*Temp + beta*C å†™å›ï¼›åŒ…å« alpha/beta åˆ†æ”¯ä¸ç›®æ ‡å†…å­˜å†™å›ã€‚
   - code: è´´å‡ºå†™å›é€»è¾‘çš„å®Œæ•´ç‰‡æ®µã€‚
   - data_object_features: 
     -numeric_kind=å®æ•°/å¤æ•°
     -numeric_precision=å•ç²¾åº¦/åŒç²¾åº¦
     -structural_properties=é€šç”¨
     -storage_layout=è·¨æ­¥

ã€è¾“å‡ºè¦æ±‚ã€‘
- âš ï¸ é‡è¦ï¼šåªæœ‰åœ¨ä»£ç ä¸­æ˜ç¡®æ‰¾åˆ°ç›¸åº”è®¡ç®—æµç¨‹æ—¶æ‰è¾“å‡ºè¯¥æµç¨‹ï¼å¦‚æœä»£ç ä¸­æ²¡æœ‰æŸä¸ªè®¡ç®—æµç¨‹ï¼Œåˆ™å®Œå…¨ä¸è¾“å‡ºè¯¥æµç¨‹çš„JSONå¯¹è±¡ã€‚
- ä¸¥æ ¼è¾“å‡º JSON æ•°ç»„ã€‚æ•°ç»„ä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªå¯¹è±¡ï¼Œä¸”å¿…é¡»åŒ…å«ä»¥ä¸‹äº”ä¸ªå­—æ®µï¼š
  - pattern_type, name, description, code, data_object_featuresã€‚
- data_object_features å¿…é¡»æ˜¯å¯¹è±¡ï¼ŒåŒ…å«å››é”®ï¼šnumeric_kind, numeric_precision, structural_properties, storage_layoutï¼›ä¸é€‚ç”¨ç”¨ "N/A"ã€‚
- "åˆ¤æ–­ä¾æ®"ä»…ä¾›ç†è§£ï¼ŒJSONè¾“å‡ºä¸­data_object_featuresåªåŒ…å«å…·ä½“çš„å€¼ï¼Œå¦‚"å®æ•°"ã€"å•ç²¾åº¦"ç­‰ï¼Œä¸å¾—åŒ…å«"åˆ¤æ–­ä¾æ®"ã€‚
- ä¸å¾—å‘æ˜æ–°æ ‡ç­¾ï¼›åªåšæµç¨‹è¯†åˆ«ã€‚
- å¦‚æœæŸä¸ªé˜¶æ®µçš„æ‰€æœ‰è®¡ç®—æµç¨‹éƒ½ä¸å­˜åœ¨ï¼Œåˆ™è¿”å›ç©ºæ•°ç»„ []ã€‚

{format_instructions}
ï¼ˆæç¤ºï¼šä¸Šé¢çš„æ ¼å¼è¯´æ˜æè¿°äº†â€œå•ä¸ªæµç¨‹å¯¹è±¡â€çš„å­—æ®µï¼Œè¯·å°†å®ƒä½œä¸ºæ•°ç»„å…ƒç´ çš„å¯¹è±¡ç»“æ„ã€‚ï¼‰"""),
            ("human", "{input}"),
            ("placeholder", "{agent_scratchpad}"),
        ])
        formatted = prompt.partial(format_instructions=parser.get_format_instructions())
        agent = create_openai_tools_agent(self.llm, tools, formatted)
        return AgentExecutor(agent=agent, tools=tools, verbose=False, max_iterations=10)

    def _extract_json_from_output(self, output: str) -> Optional[Dict]:
        if not output: return None
        try:
            return json.loads(output)
        except json.JSONDecodeError: pass
        if "```json" in output:
            s = output.find("```json") + 7
            e = output.find("```", s)
            if e > s:
                try: return json.loads(output[s:e].strip())
                except json.JSONDecodeError: return None
        if "```" in output:
            s = output.find("```") + 3
            e = output.find("```", s)
            if e > s:
                try: return json.loads(output[s:e].strip())
                except json.JSONDecodeError: return None
        return None

    def _invoke_with_retry(self, agent: AgentExecutor, payload: Dict[str, Any], label: str, retries: int = 3) -> Dict[str, Any]:
        attempt = 0
        delay_seq = [3, 6, 12]
        while True:
            try:
                return agent.invoke(payload)
            except Exception as e:
                if attempt >= retries: raise e
                wait = delay_seq[attempt] if attempt < len(delay_seq) else delay_seq[-1]
                print(f"  - {label} å¤±è´¥ï¼Œç¬¬ {attempt+1} æ¬¡é‡è¯•å‰ç­‰å¾… {wait}sï¼š{e}")
                time.sleep(wait)
                attempt += 1

    # ===== END: AgentFactory Logic Integration =====
    
    def _detect_computational_patterns(self, source_code: str) -> List[Dict[str, Any]]:
        """ä½¿ç”¨é›†æˆçš„AgentæŒ‰å››ä¸ªé˜¶æ®µæ£€æµ‹è®¡ç®—æµç¨‹æ¨¡å¼"""
        all_patterns = []
        stages = ["prep", "transform", "core", "post"]
        
        agent_map = {
            "prep": self.create_prep_pattern_agent(),
            "transform": self.create_transform_pattern_agent(),
            "core": self.create_core_pattern_agent(),
            "post": self.create_post_pattern_agent()
        }
        
        for stage in stages:
            print(f"  -> æ­£åœ¨è¯†åˆ« {stage} é˜¶æ®µçš„è®¡ç®—æµç¨‹...")
            try:
                agent = agent_map[stage]
                stage_input = f"è¯·åˆ†æä»¥ä¸‹æºç ï¼Œè¯†åˆ«â€˜{stage}â€™é˜¶æ®µçš„ç»†ç²’åº¦è®¡ç®—æµç¨‹ã€‚\n\næºç :\n{source_code}"
                result = self._invoke_with_retry(agent, {"input": stage_input}, f"è®¡ç®—æµç¨‹({stage})")
                output_raw = self._extract_json_from_output(result.get("output", "")) or {}
                
                if isinstance(output_raw, list):
                    patterns = output_raw
                elif isinstance(output_raw, dict):
                    patterns = output_raw.get("computational_patterns", [])
                else:
                    patterns = []

                if patterns:
                    all_patterns.extend(patterns)
                    print(f"    âœ… {stage} é˜¶æ®µè¯†åˆ«åˆ° {len(patterns)} ä¸ªæ¨¡å¼")
            except Exception as e:
                print(f"    âŒ {stage} é˜¶æ®µè¯†åˆ«å¤±è´¥: {e}")
        
        return all_patterns

    def _search_similar_patterns(self, detected_patterns: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """åœ¨Milvusä¸­æ£€ç´¢ä¸æ£€æµ‹åˆ°çš„è®¡ç®—æµç¨‹ç›¸ä¼¼çš„å®ä½“"""
        if not detected_patterns:
            return []

        collection = Collection("computational_pattern")
        collection.load()

        # å½’ä¸€åŒ–æœç´¢å‘é‡ï¼Œé…åˆ COSINE æ£€ç´¢
        def _normalize(v: List[float]) -> List[float]:
            try:
                s = sum(x * x for x in v)
                if s <= 0:
                    return v
                inv = 1.0 / (s ** 0.5)
                return [x * inv for x in v]
            except Exception:
                return v

        embedding_texts = [json.dumps(p, ensure_ascii=False, sort_keys=True) for p in detected_patterns]
        vectors_to_search_raw = self.embedding_model.embed_documents(embedding_texts)
        vectors_to_search = [_normalize(v) for v in vectors_to_search_raw]

        # ä½¿ç”¨ COSINE åº¦é‡ï¼›è¿™é‡Œç›´æ¥æŠŠè¿”å›çš„ score(distance å­—æ®µ)ä½œä¸ºç›¸ä¼¼åº¦ä½¿ç”¨
        search_params = {"metric_type": "COSINE", "params": {"nprobe": 10}}
        all_hits = []

        results = collection.search(
            data=vectors_to_search,
            anns_field="embedding",
            param=search_params,
            limit=50,
            output_fields=["uid", "name", "type"]
        )
        
        for i, hits in enumerate(results):
            for rank, hit in enumerate(hits):
                # ç›´æ¥ä½¿ç”¨ Milvus è¿”å›çš„ scoreï¼ˆpymilvus æš´éœ²ä¸º distance å­—æ®µï¼‰
                similarity = float(hit.distance)
                # Top-2 æ¨¡å¼ï¼ˆä¿ç•™æ³¨é‡Šï¼‰ï¼š
                # if rank < 2:
                #     all_hits.append({...})
                # å½“å‰é‡‡ç”¨ï¼šç›¸ä¼¼åº¦é˜ˆå€¼æ¨¡å¼ï¼ˆ>= 0.8ï¼‰
                if similarity >= 0.8:
                    all_hits.append({
                        "uid": hit.entity.get("uid"),
                        "name": hit.entity.get("name"),
                        "type": hit.entity.get("type"),
                        "similarity": similarity,
                        "query_pattern": detected_patterns[i]['name']
                    })
        return all_hits

    def _filter_top_patterns(self, similar_patterns: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ä»ç›¸ä¼¼ç»“æœä¸­ä¸ºæ¯ç§ç±»å‹ç­›é€‰å‡ºå¾—åˆ†æœ€é«˜çš„å®ä½“"""
        top_patterns = {}
        for pattern in similar_patterns:
            ptype = pattern['type']
            if ptype not in top_patterns or pattern['similarity'] > top_patterns[ptype]['similarity']:
                top_patterns[ptype] = pattern
        return list(top_patterns.values())

    def _find_related_strategies(self, top_patterns: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ ¹æ®è®¡ç®—æµç¨‹æŸ¥æ‰¾å…³è”çš„ä¼˜åŒ–ç­–ç•¥"""
        if not top_patterns:
            return []
            
        pattern_uids = [p['uid'] for p in top_patterns]
        relation_col = Collection("relation")
        strategy_col = Collection("optimization_strategy")
        
        expr = f'head_entity_uid in {json.dumps(pattern_uids)} and relation_type == "OPTIMIZES_PATTERN"'
        relations = relation_col.query(expr, output_fields=["tail_entity_uid"])
        
        strategy_uids = list({rel['tail_entity_uid'] for rel in relations})
        if not strategy_uids:
            return []
            
        strategies = strategy_col.query(f'uid in {json.dumps(strategy_uids)}', output_fields=["*"])
        return strategies

    def _find_related_strategy_uids(self, top_patterns: List[Dict[str, Any]]) -> List[str]:
        """æ ¹æ® top_patternsï¼ˆè®¡ç®—æµç¨‹ï¼‰é€šè¿‡å…³ç³»é›†åˆæ‰¾åˆ°å…³è”çš„ä¼˜åŒ–ç­–ç•¥UID"""
        if not top_patterns:
            return []
            
        pattern_uids = [p['uid'] for p in top_patterns]
        relation_col = Collection("relation")
        
        expr = f'head_entity_uid in {json.dumps(pattern_uids)} and relation_type == "OPTIMIZES_PATTERN"'
        relations = relation_col.query(expr, output_fields=["tail_entity_uid"])
        strategy_uids = list({rel['tail_entity_uid'] for rel in relations})
        return strategy_uids
    
    def _load_strategy_context(self, base_dir: Path) -> List[Dict[str, Any]]:
        """åŠ è½½ relation_refine/optimization_strategy_context_3.jsonï¼ˆä½äº analysis_results_dir ä¸‹ï¼‰"""
        ctx_path = base_dir / "relation_refine" / "optimization_strategy_context_3.json"
        if not ctx_path.exists():
            print(f"âš ï¸ æœªæ‰¾åˆ°ä¼˜åŒ–ç­–ç•¥ä¸Šä¸‹æ–‡æ–‡ä»¶: {ctx_path}")
            return []
        try:
            with open(ctx_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                if isinstance(data, list):
                    return data
                print(f"âš ï¸ ä¼˜åŒ–ç­–ç•¥ä¸Šä¸‹æ–‡æ–‡ä»¶ç»“æ„å¼‚å¸¸ï¼ˆæœŸæœ›listï¼‰: {ctx_path}")
                return []
        except Exception as e:
            print(f"âš ï¸ åŠ è½½ä¼˜åŒ–ç­–ç•¥ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return []
    
    def _score_and_select_final(self, context_by_uid: Dict[str, Dict[str, Any]], detected_pattern_types: List[str], candidate_uids: List[str], w_context: float = 0.5) -> List[Dict[str, Any]]:
        """æŒ‰ç»™å®šå…¬å¼è®¡ç®—å¾—åˆ†å¹¶ç­›é€‰ï¼ˆå¾—åˆ†>=0.5ï¼‰ä¸º final_strategies"""
        detected_set = set(detected_pattern_types)
        denom = float(len(detected_pattern_types)) if detected_pattern_types else 1.0
        finals: List[Dict[str, Any]] = []
        
        for uid in candidate_uids:
            entry = context_by_uid.get(uid)
            if not entry:
                continue
            core_patterns = entry.get("core_patterns", []) or []
            contextual_patterns = entry.get("contextual_patterns", {}) or {}
            
            # çº¦æŸï¼šcore_patterns å¿…é¡»å®Œå…¨åŒ…å«äºæ‰€ç»™ä»£ç çš„è®¡ç®—æµç¨‹ï¼ˆpatterns_detectedï¼‰
            if core_patterns:
                if not set(core_patterns).issubset(detected_set):
                    continue
            
            # Score_core = len(S_core âˆ© P_code) / len(P_code)
            s_core = set(core_patterns) & detected_set
            score_core = (len(s_core) / denom) if denom > 0 else 0.0
            
            # Score_context = sum(freq for matched contextual patterns)
            score_context = 0.0
            for pattern, freq_str in contextual_patterns.items():
                if pattern in detected_set:
                    try:
                        score_context += float(freq_str)
                    except Exception:
                        # å¿½ç•¥ä¸å¯è§£æçš„é¢‘ç‡
                        pass
            
            score_total = score_core + w_context * score_context
            # æœ€ç»ˆé˜ˆå€¼ï¼šå¤§äºç­‰äº 0.5
            if score_total >= 0.5:
                # è¾“å‡ºæ¡ç›®åŸºäºä¸Šä¸‹æ–‡æ•°æ®ï¼Œé™„åŠ  score
                out = {k: v for k, v in entry.items() if k != "members"}
                out["score"] = score_total
                finals.append(out)
        
        finals.sort(key=lambda x: x.get("score", 0.0), reverse=True)
        return finals

    def process_source_code(self, source_file: str) -> Dict[str, Any]:
        """å¤„ç†æºä»£ç æ–‡ä»¶ï¼Œæ‰§è¡Œå®Œæ•´çš„æ£€ç´¢å’Œè¯„åˆ†æµç¨‹"""
        print(f"ğŸš€ å¼€å§‹å¤„ç†æºä»£ç : {source_file}")
        
        if not os.path.exists(source_file):
            return {"error": f"æºæ–‡ä»¶ä¸å­˜åœ¨: {source_file}"}
        
        with open(source_file, 'r', encoding='utf-8') as f:
            source_code = f.read()
        
        patterns_detected_full = self._detect_computational_patterns(source_code)
        patterns_detected_types = [p['pattern_type'] for p in patterns_detected_full]
        print(f"âœ… æ­¥éª¤1å®Œæˆ: æ£€æµ‹åˆ° {len(patterns_detected_types)} ä¸ªè®¡ç®—æµç¨‹: {patterns_detected_types}")
        
        similar_patterns = self._search_similar_patterns(patterns_detected_full)
        print(f"âœ… æ­¥éª¤2å®Œæˆ: æ£€ç´¢åˆ° {len(similar_patterns)} ä¸ªç›¸ä¼¼è®¡ç®—æµç¨‹ (ç›¸ä¼¼åº¦ >= 0.8)")

        top_patterns = self._filter_top_patterns(similar_patterns)
        print(f"âœ… æ­¥éª¤3å®Œæˆ: ç­›é€‰å‡º {len(top_patterns)} ä¸ªæœ€é«˜åˆ†è®¡ç®—æµç¨‹")

        # æ­¥éª¤4ï¼šé€šè¿‡å…³ç³»æŸ¥æ‰¾å…³è”ç­–ç•¥UIDï¼Œå¹¶ä»ä¼˜åŒ–ä¸Šä¸‹æ–‡æ–‡ä»¶ä¸­æ„å»º search_strategies
        related_strategy_uids = self._find_related_strategy_uids(top_patterns)
        print(f"âœ… æ­¥éª¤4å®Œæˆ: æ‰¾åˆ° {len(related_strategy_uids)} ä¸ªå…³è”çš„ä¼˜åŒ–ç­–ç•¥UID")
        
        # é‡æ–°è§£æ base_dirï¼ˆä¸ main ä¸­é€»è¾‘ä¿æŒä¸€è‡´ï¼‰
        script_dir = os.path.dirname(os.path.abspath(__file__))
        cfg_data_source = self.config.get("data_source", {})
        base_dir_str = cfg_data_source.get("analysis_results_dir", "")
        # ä¸ main ä¸­è§£æä¸€è‡´
        base_dir = Path(base_dir_str)
        if not base_dir.is_absolute():
            project_root = Path(script_dir).parent
            resolved_path = project_root / base_dir
            if not resolved_path.exists():
                project_folder_name = project_root.name
                if project_folder_name in base_dir_str:
                    try:
                        idx = base_dir_str.index(project_folder_name)
                        suffix = base_dir_str[idx:]
                        root_parent = project_root.parent
                        resolved_path = root_parent / suffix
                    except ValueError:
                        pass
            base_dir = resolved_path.resolve()
        
        context_list = self._load_strategy_context(base_dir)
        context_by_uid = {e.get("strategy_uid"): e for e in context_list if isinstance(e, dict) and e.get("strategy_uid")}
        
        # search_strategies: ä»ä¸Šä¸‹æ–‡ä¸­æŒ‘å‡ºå…³è”UIDçš„æ¡ç›®ï¼Œç§»é™¤ members å­—æ®µ
        search_strategies = []
        for uid in related_strategy_uids:
            entry = context_by_uid.get(uid)
            if not entry:
                continue
            filtered = {k: v for k, v in entry.items() if k != "members"}
            # ä¸º search_strategies è®¡ç®—å¹¶æ·»åŠ  scoreï¼ˆä¸è¿›è¡Œ core å­é›†çº¦æŸï¼Œä»…è¯„åˆ†ï¼‰
            try:
                detected_set = set(patterns_detected_types)
                denom = float(len(patterns_detected_types)) if patterns_detected_types else 1.0
                core_patterns = entry.get("core_patterns", []) or []
                contextual_patterns = entry.get("contextual_patterns", {}) or {}
                
                s_core = set(core_patterns) & detected_set
                score_core = (len(s_core) / denom) if denom > 0 else 0.0
                
                score_context = 0.0
                for pattern, freq_str in contextual_patterns.items():
                    if pattern in detected_set:
                        try:
                            score_context += float(freq_str)
                        except Exception:
                            pass
                filtered["score"] = score_core + 0.5 * score_context
            except Exception:
                filtered["score"] = 0.0
            search_strategies.append(filtered)
        print(f"âœ… æ­¥éª¤4.1å®Œæˆ: ç»„è£… {len(search_strategies)} ä¸ªä¸Šä¸‹æ–‡ç­–ç•¥ï¼ˆå»é™¤ membersï¼‰")
        
        # æ­¥éª¤5ï¼šè®¡ç®—å¾—åˆ†å¹¶ç­›é€‰ final_strategies
        final_strategies = self._score_and_select_final(context_by_uid, patterns_detected_types, related_strategy_uids, w_context=0.5)
        print(f"âœ… æ­¥éª¤5å®Œæˆ: æœ€ç»ˆç­›é€‰å‡º {len(final_strategies)} ä¸ªé«˜åˆ†ç­–ç•¥")

        result = {
            "source_file": source_file,
            "patterns_detected": patterns_detected_full,
            "similar_patterns_found": similar_patterns,
            "top_patterns_per_type": top_patterns,
            "search_strategies": search_strategies,
            "final_strategies": final_strategies
        }
        return result
    
    def save_results(self, results: Dict[str, Any], output_file: str):
        """ä¿å­˜å¤„ç†ç»“æœ"""
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜: {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ä¼˜åŒ–ç­–ç•¥æ£€ç´¢ä¸è¯„åˆ†ç³»ç»Ÿv5")
    parser.add_argument("--config", type=str, default="kg_config.json", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    print("âš–ï¸ ä¼˜åŒ–ç­–ç•¥æ£€ç´¢ä¸è¯„åˆ†ç³»ç»Ÿv5")
    print("=" * 50)
    
    # <<< MODIFIED: Pass config dictionary instead of path
    config = OptimizationStrategyOperator._load_config(args.config)
    operator = OptimizationStrategyOperator(config=config)
    
    script_dir = os.path.dirname(os.path.abspath(__file__))
    source_file = os.path.join(script_dir, "gemm.txt")

    base_dir_str = config.get("data_source", {}).get("analysis_results_dir")
    if not base_dir_str:
        print("âŒ é”™è¯¯: æœªèƒ½åœ¨ kg_config.json ä¸­æ‰¾åˆ° 'analysis_results_dir'ã€‚")
        return

    base_dir = Path(base_dir_str)
    if not base_dir.is_absolute():
        project_root = Path(script_dir).parent
        resolved_path = project_root / base_dir
        if not resolved_path.exists():
             project_folder_name = project_root.name
             if project_folder_name in base_dir_str:
                 try:
                     idx = base_dir_str.index(project_folder_name)
                     suffix = base_dir_str[idx:]
                     root_parent = project_root.parent
                     resolved_path = root_parent / suffix
                 except ValueError: pass
        base_dir = resolved_path.resolve()

    if not base_dir.exists():
        print(f"âŒ é”™è¯¯ï¼šåŸºå‡†ç›®å½•ä¸å­˜åœ¨: {base_dir}")
        return

    output_file = os.path.join(base_dir, "opinfo2.json")
    
    results = operator.process_source_code(source_file)
    operator.save_results(results, output_file)


if __name__ == "__main__":
    main()