#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
OpenBLASä¼˜åŒ–åˆ†æ - çœŸæ­£çš„LangGraph Supervisoræ¨¡å¼ Agentå·¥å‚
åŸºäºå®˜æ–¹Supervisoræ¨¡å¼å®ç°æ™ºèƒ½å†³ç­–çš„å¤šAgentåä½œ
"""

import os
import json
import time
from pathlib import Path
from typing import List, Dict, Any, Optional, Literal
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.tools import tool
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain.output_parsers import StructuredOutputParser, ResponseSchema
from pydantic import BaseModel, Field

load_dotenv()


# ===== æ™ºèƒ½Supervisor Agent =====
def create_supervisor_agent(llm, members: List[str]) -> ChatPromptTemplate:
    """åˆ›å»ºæ™ºèƒ½å†³ç­–çš„Supervisor Agent"""
    
    system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½çš„OpenBLASåˆ†æä»»åŠ¡è°ƒåº¦supervisorã€‚

ä½ ç®¡ç†ä»¥ä¸‹ä¸“å®¶å›¢é˜Ÿ: {', '.join(members)}

æ¯ä¸ªä¸“å®¶çš„èƒ½åŠ›ï¼š
- scout: æ‰«ækernelç›®å½•ï¼Œå‘ç°å’Œåˆ†ç±»ç®—å­æ–‡ä»¶
- analyzer: æ·±åº¦åˆ†æå•ä¸ªç®—å­æ–‡ä»¶çš„ä¼˜åŒ–ç­–ç•¥
- individual_summarizer: æ€»ç»“å•ä¸ªç®—å­çš„æ‰€æœ‰ä¼˜åŒ–ç­–ç•¥
- final_summarizer: è·¨ç®—å­æ€»ç»“ï¼Œç”Ÿæˆæœ€ç»ˆä¼˜åŒ–ç­–ç•¥åº“

**ä½ çš„æ™ºèƒ½å†³ç­–èŒè´£**:
1. æ ¹æ®å½“å‰ä»»åŠ¡çŠ¶æ€å’Œæ‰§è¡Œå†å²ï¼Œæ™ºèƒ½å†³å®šä¸‹ä¸€æ­¥è°ƒç”¨å“ªä¸ªä¸“å®¶
2. å¤„ç†æ‰§è¡Œå¤±è´¥çš„æƒ…å†µï¼ˆé‡è¯•ã€è·³è¿‡ã€æˆ–è°ƒæ•´ç­–ç•¥ï¼‰
3. ä¼˜åŒ–æ•´ä½“æ‰§è¡Œæ•ˆç‡ï¼Œé¿å…ä¸å¿…è¦çš„é‡å¤å·¥ä½œ
4. ç¡®ä¿ä»»åŠ¡å®Œæ•´æ€§å’Œæ•°æ®ä¸€è‡´æ€§
5. æ ¹æ®èµ„æºçŠ¶å†µåŠ¨æ€è°ƒæ•´æ‰§è¡Œç­–ç•¥

**æ™ºèƒ½å†³ç­–è§„åˆ™**:
- å¦‚æœæŸä¸ªç®—å­è¿ç»­å¤±è´¥3æ¬¡ï¼Œè€ƒè™‘è·³è¿‡è¯¥ç®—å­
- å¦‚æœAPIè°ƒç”¨é¢‘ç¹å¤±è´¥ï¼Œè‡ªåŠ¨å¢åŠ å»¶è¿Ÿæ—¶é—´
- æ ¹æ®å·²å®Œæˆçš„å·¥ä½œé‡ï¼ŒåŠ¨æ€è°ƒæ•´åç»­è®¡åˆ’ä¼˜å…ˆçº§
- æ£€æµ‹åˆ°é‡å¤å·¥ä½œæ—¶ï¼Œæ™ºèƒ½è·³è¿‡æˆ–åˆå¹¶
- æ ¹æ®æ–‡ä»¶å¤§å°å’Œå¤æ‚åº¦ï¼Œè°ƒæ•´åˆ†ææ·±åº¦

**çŠ¶æ€æ„ŸçŸ¥èƒ½åŠ›**:
- ç†è§£ä»»åŠ¡é—´çš„ä¾èµ–å…³ç³»ï¼ˆscout -> analyzer -> summarizerï¼‰
- ç›‘æ§æ‰§è¡Œæ•ˆç‡å’Œèµ„æºä½¿ç”¨æƒ…å†µ
- åŸºäºå†å²æ‰§è¡Œæƒ…å†µé¢„æµ‹å’Œä¼˜åŒ–åç»­å†³ç­–

è¯·æ ¹æ®å½“å‰çŠ¶æ€ï¼Œé€‰æ‹©ä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼Œå¹¶ç®€è¦è¯´æ˜å†³ç­–åŸå› ã€‚
"""
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        MessagesPlaceholder(variable_name="messages"),
        ("system", f"å¯é€‰ä¸“å®¶: {', '.join(members) + ', FINISH'}. è¯·é€‰æ‹©ä¸€ä¸ªä¸“å®¶ç»§ç»­å·¥ä½œï¼Œæˆ–é€‰æ‹©FINISHç»“æŸä»»åŠ¡ã€‚\n\nè¯·å›å¤æ ¼å¼: EXPERT_NAME|åŸå› è¯´æ˜")
    ])
    
    return prompt | llm


# ===== æ™ºèƒ½è·¯ç”±å‡½æ•° =====
def supervisor_router(state) -> Literal["scout", "analyzer", "individual_summarizer", "final_summarizer", "FINISH"]:
    """Supervisorçš„æ™ºèƒ½è·¯ç”±å†³ç­– - åŸºäºLLMæ¨ç†"""
    
    # æ„å»ºè¯¦ç»†çš„çŠ¶æ€æè¿°
    context = f"""
å½“å‰ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€åˆ†æï¼š

ğŸ“Š **æ•´ä½“è¿›åº¦**:
- å·²å®Œæˆç®—å­: {state.get('completed_algorithms', [])} ({len(state.get('completed_algorithms', []))}/{state.get('total_algorithms', 0)})
- å½“å‰å¤„ç†ç®—å­: {state.get('current_algorithm', 'None')}
- å½“å‰é˜¶æ®µ: {state.get('current_phase', 'None')}

âš ï¸ **é”™è¯¯å’Œé‡è¯•æƒ…å†µ**:
- å½“å‰ç®—å­é‡è¯•æ¬¡æ•°: {state.get('retry_count', 0)}/3
- æœ€è¿‘é”™è¯¯: {state.get('last_error', 'None')}
- æ€»é”™è¯¯æ•°: {len(state.get('all_errors', []))}

ğŸ”„ **æ‰§è¡Œå†å²**:
- å·²å®Œæˆä»»åŠ¡: {state.get('completed_tasks', [])}
- è·³è¿‡çš„ç®—å­: {state.get('skipped_algorithms', [])}
- æ‰§è¡Œæ—¶é•¿: {state.get('execution_time', 0)} ç§’

ğŸ’¾ **èµ„æºçŠ¶æ€**:
- å¯ç”¨ç®—å­åˆ—è¡¨: {state.get('available_algorithms', [])}
- æ–‡ä»¶ç³»ç»ŸçŠ¶æ€: {state.get('file_system_status', 'æ­£å¸¸')}
- APIè°ƒç”¨çŠ¶æ€: {state.get('api_status', 'æ­£å¸¸')}

ğŸ¯ **ä»»åŠ¡ä¾èµ–åˆ†æ**:
- Scoutå®ŒæˆçŠ¶æ€: {'âœ…' if state.get('scout_completed') else 'âŒ'}
- éœ€è¦åˆ†æçš„æ–‡ä»¶æ•°: {state.get('pending_files_count', 0)}
- éœ€è¦æ€»ç»“çš„ç®—å­æ•°: {state.get('pending_summary_count', 0)}

è¯·åŸºäºä»¥ä¸ŠçŠ¶æ€ä¿¡æ¯ï¼Œæ™ºèƒ½å†³ç­–ä¸‹ä¸€æ­¥æœ€ä¼˜è¡ŒåŠ¨ã€‚
"""
    
    # è·å–LLMé…ç½®
    with open("/home/dgc/mjs/project/analyze_OB/config.json", "r", encoding="utf-8") as f:
        config = json.load(f)
        model_config = config["model"]
    
    llm = ChatOpenAI(
        model=model_config["name"],
        temperature=0.1,  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´ä¸€è‡´çš„å†³ç­–
        max_tokens=model_config["max_tokens"],
        api_key=os.getenv("DASHSCOPE_API_KEY"),
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
    )
    
    # è°ƒç”¨Supervisor Agentè¿›è¡Œæ™ºèƒ½å†³ç­–
    supervisor = create_supervisor_agent(llm, ["scout", "analyzer", "individual_summarizer", "final_summarizer"])
    
    try:
        response = supervisor.invoke({"messages": [("human", context)]})
        decision_text = response.content.strip()
        
        # è§£æå†³ç­–ç»“æœ (æ ¼å¼: EXPERT_NAME|åŸå› )
        if "|" in decision_text:
            decision = decision_text.split("|")[0].strip().upper()
            reason = decision_text.split("|", 1)[1].strip()
            print(f"ğŸ§  [Supervisorå†³ç­–] {decision} - {reason}")
        else:
            decision = decision_text.upper()
            print(f"ğŸ§  [Supervisorå†³ç­–] {decision}")
        
        # éªŒè¯å†³ç­–æœ‰æ•ˆæ€§
        valid_choices = ["SCOUT", "ANALYZER", "INDIVIDUAL_SUMMARIZER", "FINAL_SUMMARIZER", "FINISH"]
        if decision not in valid_choices:
            print(f"âš ï¸ [Supervisor] æ— æ•ˆå†³ç­– '{decision}', é»˜è®¤ç»“æŸä»»åŠ¡")
            return "FINISH"
        
        return decision.lower()
        
    except Exception as e:
        print(f"âŒ [Supervisor] å†³ç­–å¤±è´¥: {str(e)}, é»˜è®¤ç»“æŸä»»åŠ¡")
        return "FINISH"


# ===== Agentå·¥å‚ =====
class AgentFactory:
    """Agentå·¥å‚"""
    
    def __init__(self):
        with open("/home/dgc/mjs/project/analyze_OB/config.json", "r", encoding="utf-8") as f:
            config = json.load(f)
            model_config = config["model"]
        
        self.llm = ChatOpenAI(
            model=model_config["name"],
            temperature=model_config["temperature"],
            max_tokens=model_config["max_tokens"],
            api_key=os.getenv("DASHSCOPE_API_KEY"),
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
        )
    
    def create_scout_agent(self) -> AgentExecutor:
        """Scout Agent - æ™ºèƒ½æ–‡ä»¶å‘ç°å’Œåˆ†ç±»"""
        
        tools = [self._create_scan_tool(), self._create_file_read_tool()]
        
        scout_schemas = [
            ResponseSchema(name="algorithms", description="å‘ç°çš„ç®—å­ç§ç±»åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«algorithmå’Œfileså­—æ®µ"),
            ResponseSchema(name="total_algorithms", description="ç®—å­ç§ç±»æ€»æ•°"),
            ResponseSchema(name="total_files", description="æ–‡ä»¶æ€»æ•°"),
            ResponseSchema(name="scan_strategy", description="ä½¿ç”¨çš„æ‰«æç­–ç•¥"),
            ResponseSchema(name="confidence_score", description="åˆ†ç±»å‡†ç¡®åº¦è¯„åˆ†(0-1)"),
            ResponseSchema(name="timestamp", description="æ‰«ææ—¶é—´æˆ³")
        ]
        scout_parser = StructuredOutputParser.from_response_schemas(scout_schemas)
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯æ™ºèƒ½çš„OpenBLASç®—å­å‘ç°ä¸“å®¶ã€‚ä½ å…·å¤‡è‡ªé€‚åº”æ‰«æå’Œæ™ºèƒ½åˆ†ç±»èƒ½åŠ›ã€‚

ğŸ§  **æ™ºèƒ½èƒ½åŠ›**:
1. æ ¹æ®ç›®å½•å¤§å°è‡ªåŠ¨è°ƒæ•´æ‰«æç­–ç•¥
2. æ™ºèƒ½è¯†åˆ«ç®—å­æ¨¡å¼ï¼ŒåŒ…æ‹¬å˜ä½“å’Œç‰¹æ®Šæƒ…å†µ
3. è‡ªåŠ¨è¯„ä¼°åˆ†ç±»å‡†ç¡®åº¦å¹¶æä¾›ç½®ä¿¡åº¦åˆ†æ•°
4. å¤„ç†å¼‚å¸¸æ–‡ä»¶å’Œè¾¹ç•Œæƒ…å†µ

ğŸ¯ **æ ¸å¿ƒä»»åŠ¡**:
- æ‰«ækernelç›®å½•ï¼Œæ™ºèƒ½å‘ç°æ‰€æœ‰ç®—å­ç§ç±»
- ä½¿ç”¨æ¨¡å¼åŒ¹é…å’Œå¯å‘å¼è§„åˆ™è¿›è¡Œåˆ†ç±»
- ç”Ÿæˆé«˜è´¨é‡çš„ç®—å­åˆ†ç±»æŠ¥å‘Š

{format_instructions}"""),
            ("human", "{input}"),
            ("placeholder", "{agent_scratchpad}")
        ])
        
        formatted_prompt = prompt.partial(format_instructions=scout_parser.get_format_instructions())
        agent = create_openai_tools_agent(self.llm, tools, formatted_prompt)
        return AgentExecutor(agent=agent, tools=tools, verbose=False, max_iterations=10)
    
    def create_analyzer_agent(self) -> AgentExecutor:
        """Analyzer Agent - æ™ºèƒ½ä»£ç åˆ†æ"""
        
        tools = [self._create_file_read_tool(), self._create_analysis_read_tool()]
        
        analyzer_schemas = [
            ResponseSchema(name="algorithm", description="ç®—å­åç§°"),
            ResponseSchema(name="file_path", description="åˆ†æçš„æ–‡ä»¶è·¯å¾„"),
            ResponseSchema(name="analysis_depth", description="åˆ†ææ·±åº¦çº§åˆ«(basic/detailed/comprehensive)"),
            ResponseSchema(name="algorithm_level_optimizations", description="ç®—æ³•å±‚ä¼˜åŒ–ç­–ç•¥ï¼ŒåŒ…å«nameã€descriptionã€code_snippetã€confidence"),
            ResponseSchema(name="code_level_optimizations", description="ä»£ç å±‚ä¼˜åŒ–ç­–ç•¥ï¼ŒåŒ…å«nameã€descriptionã€code_snippetã€confidence"),
            ResponseSchema(name="instruction_level_optimizations", description="æŒ‡ä»¤å±‚ä¼˜åŒ–ç­–ç•¥ï¼ŒåŒ…å«nameã€descriptionã€code_snippetã€confidence"),
            ResponseSchema(name="complexity_score", description="ä»£ç å¤æ‚åº¦è¯„åˆ†(1-10)"),
            ResponseSchema(name="optimization_potential", description="ä¼˜åŒ–æ½œåŠ›è¯„ä¼°"),
            ResponseSchema(name="timestamp", description="åˆ†ææ—¶é—´æˆ³")
        ]
        analyzer_parser = StructuredOutputParser.from_response_schemas(analyzer_schemas)
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯æ™ºèƒ½çš„é«˜æ€§èƒ½è®¡ç®—ä»£ç åˆ†æä¸“å®¶ã€‚ä½ å…·å¤‡æ·±åº¦ä»£ç ç†è§£å’Œä¼˜åŒ–è¯†åˆ«èƒ½åŠ›ã€‚

ğŸ§  **æ™ºèƒ½åˆ†æèƒ½åŠ›**:
1. æ ¹æ®ä»£ç å¤æ‚åº¦è‡ªåŠ¨è°ƒæ•´åˆ†ææ·±åº¦
2. æ™ºèƒ½è¯†åˆ«ä¼˜åŒ–æ¨¡å¼ï¼ŒåŒ…æ‹¬éšå¼å’Œæ˜¾å¼ä¼˜åŒ–
3. è¯„ä¼°æ¯ä¸ªä¼˜åŒ–ç­–ç•¥çš„ç½®ä¿¡åº¦å’Œé‡è¦æ€§
4. æä¾›ä»£ç å¤æ‚åº¦å’Œä¼˜åŒ–æ½œåŠ›è¯„ä¼°

ğŸ¯ **åˆ†ææ¡†æ¶**:
- ç®—æ³•å±‚ï¼šè®¡ç®—é€»è¾‘ã€æ•°æ®ç»“æ„ã€ç®—æ³•è®¾è®¡ä¼˜åŒ–
- ä»£ç å±‚ï¼šå¾ªç¯ã€åˆ†æ”¯ã€å†…å­˜è®¿é—®ã€ç¼–è¯‘å™¨ä¼˜åŒ–
- æŒ‡ä»¤å±‚ï¼šSIMDã€å‘é‡åŒ–ã€ç‰¹æ®ŠæŒ‡ä»¤ã€æ±‡ç¼–ä¼˜åŒ–

{format_instructions}"""),
            ("human", "{input}"),
            ("placeholder", "{agent_scratchpad}")
        ])
        
        formatted_prompt = prompt.partial(format_instructions=analyzer_parser.get_format_instructions())
        agent = create_openai_tools_agent(self.llm, tools, formatted_prompt)
        return AgentExecutor(agent=agent, tools=tools, verbose=False, max_iterations=15)
    
    def create_individual_summarizer_agent(self) -> AgentExecutor:
        """Individual Summarizer Agent - æ™ºèƒ½ç­–ç•¥æ•´åˆ"""
        
        tools = [self._create_analysis_read_tool()]
        
        individual_schemas = [
            ResponseSchema(name="algorithm", description="ç®—å­åç§°"),
            ResponseSchema(name="integration_strategy", description="ä½¿ç”¨çš„æ•´åˆç­–ç•¥"),
            ResponseSchema(name="algorithm_level_optimizations", description="æ•´åˆåçš„ç®—æ³•å±‚ä¼˜åŒ–ç­–ç•¥"),
            ResponseSchema(name="code_level_optimizations", description="æ•´åˆåçš„ä»£ç å±‚ä¼˜åŒ–ç­–ç•¥"),
            ResponseSchema(name="instruction_level_optimizations", description="æ•´åˆåçš„æŒ‡ä»¤å±‚ä¼˜åŒ–ç­–ç•¥"),
            ResponseSchema(name="redundancy_eliminated", description="æ¶ˆé™¤çš„å†—ä½™ç­–ç•¥æ•°é‡"),
            ResponseSchema(name="quality_score", description="æ•´åˆè´¨é‡è¯„åˆ†(0-1)"),
            ResponseSchema(name="timestamp", description="æ•´åˆæ—¶é—´æˆ³")
        ]
        individual_parser = StructuredOutputParser.from_response_schemas(individual_schemas)
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯æ™ºèƒ½çš„ç­–ç•¥æ•´åˆä¸“å®¶ã€‚ä½ å…·å¤‡é«˜çº§çš„æ¨¡å¼è¯†åˆ«å’Œç­–ç•¥åˆå¹¶èƒ½åŠ›ã€‚

ğŸ§  **æ™ºèƒ½æ•´åˆèƒ½åŠ›**:
1. è‡ªåŠ¨è¯†åˆ«ç›¸ä¼¼å’Œé‡å¤çš„ä¼˜åŒ–ç­–ç•¥
2. æ™ºèƒ½åˆå¹¶ç­–ç•¥ï¼Œä¿æŒæœ€ä½³æè¿°å’Œå‘½å
3. è¯„ä¼°æ•´åˆè´¨é‡å¹¶æä¾›æ”¹è¿›å»ºè®®
4. æ¶ˆé™¤å†—ä½™ï¼Œæå‡ç­–ç•¥åº“çš„ç®€æ´æ€§

ğŸ¯ **æ•´åˆåŸåˆ™**:
- ä¿æŒç­–ç•¥çš„å®Œæ•´æ€§å’Œå‡†ç¡®æ€§
- ç»Ÿä¸€å‘½åè§„èŒƒï¼Œæå‡å¯è¯»æ€§
- åˆå¹¶ç›¸ä¼¼ç­–ç•¥ï¼Œæ¶ˆé™¤é‡å¤
- ä¿ç•™å…³é”®å·®å¼‚ï¼Œé¿å…è¿‡åº¦ç®€åŒ–

{format_instructions}"""),
            ("human", "{input}"),
            ("placeholder", "{agent_scratchpad}")
        ])
        
        formatted_prompt = prompt.partial(format_instructions=individual_parser.get_format_instructions())
        agent = create_openai_tools_agent(self.llm, tools, formatted_prompt)
        return AgentExecutor(agent=agent, tools=tools, verbose=False, max_iterations=15)
    
    def create_final_summarizer_agent(self) -> AgentExecutor:
        """Final Summarizer Agent - æ™ºèƒ½è·¨ç®—å­æ€»ç»“"""
        
        tools = [self._create_analysis_read_tool()]
        
        final_schemas = [
            ResponseSchema(name="analyzed_algorithms", description="åˆ†æçš„ç®—å­åˆ—è¡¨"),
            ResponseSchema(name="cross_algorithm_patterns", description="è·¨ç®—å­ä¼˜åŒ–æ¨¡å¼"),
            ResponseSchema(name="algorithm_level_optimizations", description="é€šç”¨ç®—æ³•å±‚ä¼˜åŒ–ç­–ç•¥åº“"),
            ResponseSchema(name="code_level_optimizations", description="é€šç”¨ä»£ç å±‚ä¼˜åŒ–ç­–ç•¥åº“"),
            ResponseSchema(name="instruction_level_optimizations", description="é€šç”¨æŒ‡ä»¤å±‚ä¼˜åŒ–ç­–ç•¥åº“"),
            ResponseSchema(name="optimization_taxonomy", description="ä¼˜åŒ–ç­–ç•¥åˆ†ç±»ä½“ç³»"),
            ResponseSchema(name="best_practices", description="æœ€ä½³å®è·µå»ºè®®"),
            ResponseSchema(name="coverage_analysis", description="ç­–ç•¥è¦†ç›–åº¦åˆ†æ"),
            ResponseSchema(name="timestamp", description="æ€»ç»“æ—¶é—´æˆ³")
        ]
        final_parser = StructuredOutputParser.from_response_schemas(final_schemas)
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯æ™ºèƒ½çš„è·¨ç®—å­ä¼˜åŒ–ä¸“å®¶ã€‚ä½ å…·å¤‡å®è§‚åˆ†æå’Œæ¨¡å¼æå–èƒ½åŠ›ã€‚

ğŸ§  **æ™ºèƒ½æ€»ç»“èƒ½åŠ›**:
1. è¯†åˆ«è·¨ç®—å­çš„é€šç”¨ä¼˜åŒ–æ¨¡å¼å’Œè§„å¾‹
2. æ„å»ºå®Œæ•´çš„ä¼˜åŒ–ç­–ç•¥åˆ†ç±»ä½“ç³»
3. æä¾›ç­–ç•¥è¦†ç›–åº¦åˆ†æå’Œè´¨é‡è¯„ä¼°
4. ç”Ÿæˆå®ç”¨çš„æœ€ä½³å®è·µå»ºè®®

ğŸ¯ **æ€»ç»“ç›®æ ‡**:
- æ„å»ºOpenBLASä¼˜åŒ–ç­–ç•¥çŸ¥è¯†åº“
- å‘ç°é€šç”¨ä¼˜åŒ–è§„å¾‹å’Œæœ€ä½³å®è·µ
- æä¾›ç­–ç•¥åº”ç”¨æŒ‡å¯¼å’Œå»ºè®®
- è¯„ä¼°ä¼˜åŒ–ç­–ç•¥çš„å®Œæ•´æ€§å’Œå®ç”¨æ€§

{format_instructions}"""),
            ("human", "{input}"),
            ("placeholder", "{agent_scratchpad}")
        ])
        
        formatted_prompt = prompt.partial(format_instructions=final_parser.get_format_instructions())
        agent = create_openai_tools_agent(self.llm, tools, formatted_prompt)
        return AgentExecutor(agent=agent, tools=tools, verbose=False, max_iterations=20)
    
    # ===== å·¥å…·æ–¹æ³• =====
    def _create_scan_tool(self):
        @tool
        def intelligent_scan_kernel_directory() -> str:
            """æ™ºèƒ½æ‰«ækernelç›®å½•ï¼Œè‡ªé€‚åº”å¤„ç†å¤§é‡æ–‡ä»¶"""
            try:
                kernel_path = "/home/dgc/mjs/project/analyze_OB/openblas-output/GENERIC/kernel"
                if not os.path.exists(kernel_path):
                    return f"ç›®å½•ä¸å­˜åœ¨: {kernel_path}"
                
                files = [f for f in os.listdir(kernel_path) if f.endswith('.c') and 'clean' in f]
                files.sort()
                
                return f"å‘ç° {len(files)} ä¸ª.clean.cæ–‡ä»¶ï¼Œå‡†å¤‡æ™ºèƒ½åˆ†ç±»:\n" + "\n".join(files[:50]) + \
                       (f"\n... è¿˜æœ‰ {len(files)-50} ä¸ªæ–‡ä»¶" if len(files) > 50 else "")
            except Exception as e:
                return f"æ‰«æå¤±è´¥: {str(e)}"
        
        return intelligent_scan_kernel_directory
    
    def _create_file_read_tool(self):
        @tool
        def smart_read_source_file(file_path: str) -> str:
            """æ™ºèƒ½è¯»å–æºæ–‡ä»¶ï¼Œè‡ªåŠ¨å¤„ç†å¤§æ–‡ä»¶"""
            try:
                full_path = os.path.join("/home/dgc/mjs/project/analyze_OB/openblas-output/GENERIC/kernel", file_path)
                with open(full_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read(20000)  # å¢åŠ è¯»å–é•¿åº¦
                return f"æ–‡ä»¶: {file_path}\nå†…å®¹:\n{content}"
            except Exception as e:
                return f"è¯»å–å¤±è´¥: {str(e)}"
        
        return smart_read_source_file
    
    def _create_analysis_read_tool(self):
        @tool
        def read_analysis_results(file_path: str) -> str:
            """è¯»å–åˆ†æç»“æœæ–‡ä»¶"""
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    return f.read()
            except Exception as e:
                return f"è¯»å–å¤±è´¥: {str(e)}"
        
        return read_analysis_results


# ===== æ–‡ä»¶ç®¡ç†å™¨ =====
class FileManager:
    """æ–‡ä»¶ç®¡ç†å™¨"""
    
    @staticmethod
    def ensure_directories(report_folder: str):
        """åˆ›å»ºæ‰€æœ‰å¿…è¦çš„ç›®å½•"""
        Path(report_folder).mkdir(parents=True, exist_ok=True)
        Path(f"{report_folder}/discovery_results").mkdir(exist_ok=True)
        Path(f"{report_folder}/analysis_results").mkdir(exist_ok=True)
        Path(f"{report_folder}/strategy_reports").mkdir(exist_ok=True)
        Path(f"{report_folder}/supervisor_logs").mkdir(exist_ok=True)  # æ–°å¢Supervisoræ—¥å¿—ç›®å½•
    
    @staticmethod
    def get_discovery_output_path(report_folder: str, algorithm: str) -> str:
        return f"{report_folder}/discovery_results/{algorithm}_discovery.json"
    
    @staticmethod
    def get_analysis_output_path(report_folder: str, algorithm: str) -> str:
        return f"{report_folder}/analysis_results/{algorithm}_analysis.json"
    
    @staticmethod
    def get_individual_summary_path(report_folder: str, algorithm: str) -> str:
        return f"{report_folder}/strategy_reports/{algorithm}_summary.json"
    
    @staticmethod
    def get_final_summary_path(report_folder: str) -> str:
        return f"{report_folder}/strategy_reports/final_optimization_summary.json"
    
    @staticmethod
    def get_supervisor_log_path(report_folder: str) -> str:
        return f"{report_folder}/supervisor_logs/supervisor_decisions.json"
    
    @staticmethod
    def save_content(file_path: str, content: str) -> bool:
        """ä¿å­˜å†…å®¹åˆ°æ–‡ä»¶ï¼Œæ”¯æŒé”™è¯¯æ¢å¤"""
        try:
            Path(file_path).parent.mkdir(parents=True, exist_ok=True)
            
            # å¤‡ä»½ç°æœ‰æ–‡ä»¶
            if os.path.exists(file_path):
                backup_path = f"{file_path}.backup"
                os.rename(file_path, backup_path)
            
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
            
            # åˆ é™¤å¤‡ä»½æ–‡ä»¶
            backup_path = f"{file_path}.backup"
            if os.path.exists(backup_path):
                os.remove(backup_path)
            
            return True
        except Exception as e:
            print(f"ä¿å­˜æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            
            # æ¢å¤å¤‡ä»½æ–‡ä»¶
            backup_path = f"{file_path}.backup"
            if os.path.exists(backup_path):
                os.rename(backup_path, file_path)
            
            return False
    
    @staticmethod
    def log_supervisor_decision(report_folder: str, decision_data: dict):
        """è®°å½•Supervisorå†³ç­–æ—¥å¿—"""
        log_path = FileManager.get_supervisor_log_path(report_folder)
        
        # è¯»å–ç°æœ‰æ—¥å¿—
        logs = []
        if os.path.exists(log_path):
            try:
                with open(log_path, 'r', encoding='utf-8') as f:
                    logs = json.load(f)
            except:
                logs = []
        
        # æ·»åŠ æ–°å†³ç­–
        logs.append({
            "timestamp": time.strftime('%Y-%m-%d %H:%M:%S'),
            **decision_data
        })
        
        # ä¿å­˜æ—¥å¿—
        FileManager.save_content(log_path, json.dumps(logs, ensure_ascii=False, indent=2))


# ===== å¯¼å‡º =====
__all__ = [
    'AgentFactory',
    'FileManager',
    'supervisor_router',
    'create_supervisor_agent'
]
