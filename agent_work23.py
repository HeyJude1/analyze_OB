#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
OpenBLASä¼˜åŒ–åˆ†æ - LangGraphå·¥ä½œæµï¼ˆagent_work23.pyï¼‰

è¯´æ˜ï¼š
- åŸºäº agent_work22.py æ¼”è¿›ï¼šå°†â€œè®¡ç®—æµç¨‹è¯†åˆ«â€æ‹†åˆ†ä¸ºå››ä¸ªé˜¶æ®µï¼ˆprep/transform/core/postï¼‰ä¸²è¡Œæ‰§è¡Œ
- æ¯ä¸ªé˜¶æ®µç‹¬ç«‹æé—®ä¸ä¿å­˜è¿”å›ç»“æœï¼›å®Œæˆå››é˜¶æ®µåå†è¿›è¡Œä¸‰å±‚ä¼˜åŒ–ç­–ç•¥åˆ†æ
- ç»§ç»­æ”¯æŒï¼šæ–­ç‚¹ç»­è·‘ã€æŒ‰ç®—å­/æ–‡ä»¶é™åˆ¶ã€å¤±è´¥é‡è¯•ã€analysis_only/full ä¸¤ç§æ¨¡å¼
"""

import os
import time
import json
import argparse
from typing import Dict, List, Literal, Any, Optional
from typing_extensions import TypedDict
from dotenv import load_dotenv

from langgraph.graph import StateGraph, START, END

from agent23 import AgentFactory, FileManager


load_dotenv()


class WorkState(TypedDict, total=False):
    mode: Literal['analysis_only', 'full']
    report_folder: str
    algorithms: List[str]
    files_per_algorithm: int
    current_algorithm: str
    completed_analysis: List[str]
    completed_individual_summaries: List[str]
    final_summary_done: bool
    errors: List[str]


class Workflow23:
    def __init__(self, files_per_algorithm: int | None = None):
        self.factory = AgentFactory()
        self.file_mgr = FileManager()
        self.files_per_algorithm = files_per_algorithm if (isinstance(files_per_algorithm, int) and files_per_algorithm > 0) else None

        # æ„å»ºLangGraphå·¥ä½œæµ
        self.graph = self._build_graph()

        # é¢„åˆ›å»ºæ€»ç»“å™¨ï¼ˆä¸ 21/22 ä¸€è‡´çš„èŒè´£ï¼‰
        self.individual_summarizer = self.factory.create_individual_summarizer()
        self.final_summarizer = self.factory.create_final_summarizer()

    def _build_graph(self):
        g = StateGraph(WorkState)
        g.add_node("start_analysis", self.start_analysis_node)
        g.add_node("analyzer_work", self.analyzer_work_node)
        g.add_node("individual_summary_work", self.individual_summary_work_node)
        g.add_node("final_summary_work", self.final_summary_work_node)

        g.set_entry_point("start_analysis")
        g.add_conditional_edges(
            "start_analysis", self._route_after_start,
            {"continue": "analyzer_work", END: END},
        )
        g.add_conditional_edges(
            "analyzer_work", self._route_after_analyzer,
            {"continue_summary": "individual_summary_work", "next_algo": "analyzer_work", END: END},
        )
        g.add_conditional_edges(
            "individual_summary_work", self._route_after_individual_summary,
            {"next_algo": "analyzer_work", "do_final_summary": "final_summary_work"},
        )
        g.add_edge("final_summary_work", END)
        return g.compile()

    # --- è·¯ç”±é€»è¾‘ ---
    def _route_after_start(self, state: WorkState) -> str:
        if state.get("algorithms"):
            return "continue"
        return END

    def _route_after_analyzer(self, state: WorkState) -> str:
        completed = state.get("completed_analysis", [])
        if state.get("mode") == 'analysis_only':
            return "next_algo" if len(completed) < len(state.get("algorithms", [])) else END
        return "continue_summary"

    def _route_after_individual_summary(self, state: WorkState) -> str:
        completed = state.get("completed_individual_summaries", [])
        return "next_algo" if len(completed) < len(state.get("algorithms", [])) else "do_final_summary"

    # --- èŠ‚ç‚¹ ---
    def start_analysis_node(self, state: WorkState) -> WorkState:
        print(f"â–¶ï¸  å·¥ä½œæµå¯åŠ¨ï¼Œæ¨¡å¼: {state.get('mode')}")
        state["completed_analysis"] = []
        state["completed_individual_summaries"] = []
        state["final_summary_done"] = False

        # ç”Ÿæˆ discovery æ–‡ä»¶ï¼ˆæœ¬åœ°æ‰«æï¼Œä¸ä¾èµ– LLMï¼‰
        report_folder = state["report_folder"]
        all_algorithms_map = self._scan_and_classify_files_locally()
        discovery_path = self.file_mgr.get_discovery_output_path(report_folder, "all_algorithms")
        final_discovery = {
            "algorithms": list(all_algorithms_map.values()),
            "total_algorithms": len(all_algorithms_map),
            "total_files": sum(len(info["files"]) for info in all_algorithms_map.values()),
        }
        FileManager.save_content(discovery_path, json.dumps(final_discovery, ensure_ascii=False, indent=2))
        print(f"âœ… å‘ç° {len(all_algorithms_map)} ç§ç®—å­ï¼Œdiscovery å·²å†™å…¥ã€‚")
        return state

    def analyzer_work_node(self, state: WorkState) -> WorkState:
        completed = state.get("completed_analysis", [])
        algorithms = state.get("algorithms", [])
        idx = len(completed)
        if idx >= len(algorithms):
            return state
        current_algo = algorithms[idx]
        state["current_algorithm"] = current_algo
        report_folder = state["report_folder"]
        print(f"\nğŸ”¬ å¼€å§‹åˆ†æç®—å­: {current_algo}")

        try:
            # è¯»å– discovery è·å–è¯¥ç®—å­çš„æ–‡ä»¶åˆ—è¡¨
            discovery_path = self.file_mgr.get_discovery_output_path(report_folder, "all_algorithms")
            with open(discovery_path, 'r', encoding='utf-8') as f:
                all_algos = json.load(f).get("algorithms", [])
            input_files = next((a.get("files", []) for a in all_algos if a.get("algorithm") == current_algo), [])
            if not input_files:
                raise ValueError(f"æœªåœ¨discoveryä¸­æ‰¾åˆ° {current_algo} çš„æ–‡ä»¶åˆ—è¡¨")

            analysis_path = self.file_mgr.get_analysis_output_path(report_folder, current_algo)
            existing_analyses: List[Dict] = []
            if os.path.exists(analysis_path):
                try:
                    with open(analysis_path, 'r', encoding='utf-8') as rf:
                        existing = json.load(rf)
                        if isinstance(existing, dict) and isinstance(existing.get("individual_analyses"), list):
                            existing_analyses = existing["individual_analyses"]
                except Exception:
                    existing_analyses = []

            # è·³è¿‡å·²å®Œå…¨åˆ†æçš„æ–‡ä»¶ï¼Œæ”¯æŒæ–­ç‚¹ç»­è·‘
            processed_names = set()
            for ea in existing_analyses:
                if isinstance(ea, dict):
                    name = ea.get("file_path") or ea.get("file") or ea.get("filename")
                    # è®¤ä¸ºè®¡ç®—æµç¨‹ä¸ä¸‰å±‚ä¼˜åŒ–éƒ½å·²æœ‰åˆ™è§†ä¸ºå¤„ç†å®Œæˆ
                    if isinstance(name, str) and name and all(
                        k in ea for k in [
                            "computational_patterns",
                            "algorithm_level_optimizations",
                            "code_level_optimizations",
                            "instruction_level_optimizations",
                        ]
                    ):
                        processed_names.add(name)

            if processed_names:
                input_files = [fi for fi in input_files if fi.get("name") not in processed_names]

            # é™åˆ¶æ¯ä¸ªç®—å­æ–‡ä»¶æ•°é‡
            if self.files_per_algorithm:
                input_files = input_files[: self.files_per_algorithm]

            # é€æ–‡ä»¶ä¸²è¡Œåˆ†æå¹¶åˆ†é˜¶æ®µå¢é‡ä¿å­˜
            for i, file_info in enumerate(input_files, 1):
                file_name = file_info.get("name", "")
                if not file_name:
                    continue
                print(f"  ğŸ“„ åˆ†ææ–‡ä»¶ {i}/{len(input_files)}: {file_name}")

                # åœ¨å·¥ä½œæµä¸­è¯»å–æ–‡ä»¶
                source_code = self._read_source(file_name)

                # æ‰¾åˆ°æˆ–åˆ›å»ºè¯¥æ–‡ä»¶çš„åˆ†ææ¡ç›®
                entry = self._find_or_create_entry(existing_analyses, current_algo, file_name)
                
                # å¦‚æœè¯¥æ–‡ä»¶å·²å®Œå…¨åˆ†æï¼Œè·³è¿‡
                if entry.get("computational_patterns") and entry.get("algorithm_level_optimizations"):
                    continue
                
                # ä½¿ç”¨ analyze_file è¿›è¡Œå®Œæ•´åˆ†æï¼ˆå†…éƒ¨ä¼šåˆ†é˜¶æ®µå¤„ç†å¹¶åˆå¹¶ï¼‰
                attempts = 0
                while True:
                    try:
                        result = self.factory.analyze_file(
                            source_code=source_code,
                            file_path=file_name,
                            algorithm=current_algo,
                            architecture="é€šç”¨"
                        )
                        # æ›´æ–°æ¡ç›®
                        entry.update(result)
                        self._save_analysis(analysis_path, existing_analyses, current_algo, len(input_files))
                        break
                    except Exception as fe:
                        if attempts >= 3:
                            err = f"æ–‡ä»¶åˆ†æå¤±è´¥(é‡è¯•å·²è¾¾ä¸Šé™): {fe}"
                            print(f"    âŒ {err}")
                            state.setdefault("errors", []).append(err)
                            break
                        wait = [3, 6, 12][attempts] if attempts < 3 else 12
                        print(f"    - æ–‡ä»¶åˆ†æå¤±è´¥ï¼Œç¬¬ {attempts+1} æ¬¡é‡è¯•å‰ç­‰å¾… {wait}s: {fe}")
                        time.sleep(wait)
                        attempts += 1

                # é™æµä¿æŠ¤
                if i < len(input_files):
                    time.sleep(8)

            print(f"  âœ… {current_algo} åˆ†æå®Œæˆ â†’ {os.path.basename(analysis_path)}")
            state["completed_analysis"].append(current_algo)
        except Exception as e:
            err = f"åˆ†æç®—å­ '{current_algo}' å¤±è´¥: {e}"
            print(f"  âŒ {err}")
            state.setdefault("errors", []).append(err)
            state.setdefault("completed_analysis", []).append(current_algo)
        return state

    def individual_summary_work_node(self, state: WorkState) -> WorkState:
        completed_summaries = state.get("completed_individual_summaries", [])
        algorithms = state.get("algorithms", [])
        idx = len(completed_summaries)
        if idx >= len(algorithms):
            return state
        current_algo = algorithms[idx]
        state["current_algorithm"] = current_algo
        report_folder = state["report_folder"]
        print(f"ğŸ“ å¼€å§‹æ€»ç»“ç®—å­: {current_algo}")

        try:
            analysis_path = self.file_mgr.get_analysis_output_path(report_folder, current_algo)
            summary_path = self.file_mgr.get_individual_summary_path(report_folder, current_algo)
            with open(analysis_path, 'r', encoding='utf-8') as f:
                analysis_data = json.load(f)

            first = (analysis_data.get("individual_analyses") or [ {} ])[0]
            current_summary = {
                "algorithm": current_algo,
                "algorithm_level_optimizations": first.get("algorithm_level_optimizations", []),
                "code_level_optimizations": first.get("code_level_optimizations", []),
                "instruction_level_optimizations": first.get("instruction_level_optimizations", []),
            }
            FileManager.save_content(summary_path, json.dumps(current_summary, ensure_ascii=False, indent=2))

            # è¿­ä»£æ•´åˆåç»­æ–‡ä»¶ç»“æœ
            for analysis in (analysis_data.get("individual_analyses", []))[1:]:
                summary_input = (
                    f"å¢é‡æ•´åˆç®—å­ {current_algo} çš„ä¼˜åŒ–ç­–ç•¥ã€‚\n\n"
                    f"å·²æœ‰æ€»ç»“:\nç®—æ³•å±‚: {json.dumps(current_summary.get('algorithm_level_optimizations', []), ensure_ascii=False, indent=2)}\n"
                    f"ä»£ç å±‚: {json.dumps(current_summary.get('code_level_optimizations', []), ensure_ascii=False, indent=2)}\n"
                    f"æŒ‡ä»¤å±‚: {json.dumps(current_summary.get('instruction_level_optimizations', []), ensure_ascii=False, indent=2)}\n\n"
                    f"æ–°å¢åˆ†æ:\nç®—æ³•å±‚: {json.dumps(analysis.get('algorithm_level_optimizations', []), ensure_ascii=False, indent=2)}\n"
                    f"ä»£ç å±‚: {json.dumps(analysis.get('code_level_optimizations', []), ensure_ascii=False, indent=2)}\n"
                    f"æŒ‡ä»¤å±‚: {json.dumps(analysis.get('instruction_level_optimizations', []), ensure_ascii=False, indent=2)}\n"
                    "è¯·è¾“å‡ºæ•´åˆåçš„JSONï¼Œä»…åŒ…å« algorithm_level_optimizations, code_level_optimizations, instruction_level_optimizationsã€‚"
                )
                attempts = 0
                while True:
                    try:
                        result = self.individual_summarizer.invoke({"input": summary_input})
                        break
                    except Exception as fe:
                        if attempts >= 3:
                            print(f"  âŒ ä¸ªä½“æ€»ç»“è°ƒç”¨å¤±è´¥(é‡è¯•å·²è¾¾ä¸Šé™): {fe}")
                            result = {"output": "{}"}
                            break
                        wait = [3, 6, 12][attempts] if attempts < 3 else 12
                        print(f"  - ä¸ªä½“æ€»ç»“å¤±è´¥ï¼Œç¬¬ {attempts+1} æ¬¡é‡è¯•å‰ç­‰å¾… {wait}s: {fe}")
                        time.sleep(wait)
                        attempts += 1
                updated = self._extract_json_from_result(result)
                if isinstance(updated, dict):
                    for key in [
                        "algorithm_level_optimizations",
                        "code_level_optimizations",
                        "instruction_level_optimizations",
                    ]:
                        current_summary[key] = updated.get(key, current_summary.get(key, []))
                    FileManager.save_content(summary_path, json.dumps(current_summary, ensure_ascii=False, indent=2))

            print(f"  âœ… {current_algo} æ€»ç»“å®Œæˆ â†’ {os.path.basename(summary_path)}")
            state["completed_individual_summaries"].append(current_algo)
        except Exception as e:
            err = f"æ€»ç»“ç®—å­ '{current_algo}' å¤±è´¥: {e}"
            print(f"  âŒ {err}")
            state.setdefault("errors", []).append(err)
        return state

    def final_summary_work_node(self, state: WorkState) -> WorkState:
        report_folder = state["report_folder"]
        algorithms = state.get("algorithms", [])
        print("\nğŸ”— æ±‡æ€»æ‰€æœ‰ç®—å­çš„æ€»ç»“ï¼Œç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šâ€¦")
        try:
            if not algorithms:
                raise ValueError("æ²¡æœ‰ç®—å­å¯æ±‡æ€»")
            first_summary_path = self.file_mgr.get_individual_summary_path(report_folder, algorithms[0])
            with open(first_summary_path, 'r', encoding='utf-8') as f:
                first_summary = json.load(f)
            current_final = {
                "analyzed_algorithms": [algorithms[0]],
                "algorithm_level_optimizations": first_summary.get("algorithm_level_optimizations", []),
                "code_level_optimizations": first_summary.get("code_level_optimizations", []),
                "instruction_level_optimizations": first_summary.get("instruction_level_optimizations", []),
            }
            final_path = self.file_mgr.get_final_summary_path(report_folder)
            FileManager.save_content(final_path, json.dumps(current_final, ensure_ascii=False, indent=2))

            for algorithm in algorithms[1:]:
                path = self.file_mgr.get_individual_summary_path(report_folder, algorithm)
                if not os.path.exists(path):
                    continue
                with open(path, 'r', encoding='utf-8') as f:
                    summary = json.load(f)
                prompt = (
                    "è·¨ç®—å­æ•´åˆä¼˜åŒ–ç­–ç•¥ã€‚\n"
                    f"å·²æœ‰:\nç®—æ³•å±‚: {json.dumps(current_final.get('algorithm_level_optimizations', []), ensure_ascii=False, indent=2)}\n"
                    f"ä»£ç å±‚: {json.dumps(current_final.get('code_level_optimizations', []), ensure_ascii=False, indent=2)}\n"
                    f"æŒ‡ä»¤å±‚: {json.dumps(current_final.get('instruction_level_optimizations', []), ensure_ascii=False, indent=2)}\n"
                    f"æ–°ç®—å­ {algorithm}:\nç®—æ³•å±‚: {json.dumps(summary.get('algorithm_level_optimizations', []), ensure_ascii=False, indent=2)}\n"
                    f"ä»£ç å±‚: {json.dumps(summary.get('code_level_optimizations', []), ensure_ascii=False, indent=2)}\n"
                    f"æŒ‡ä»¤å±‚: {json.dumps(summary.get('instruction_level_optimizations', []), ensure_ascii=False, indent=2)}\n"
                    "è¯·è¾“å‡ºåŒ…å« algorithm_level_optimizations, code_level_optimizations, instruction_level_optimizations çš„JSONã€‚"
                )
                attempts = 0
                while True:
                    try:
                        result = self.final_summarizer.invoke({"input": prompt})
                        break
                    except Exception as fe:
                        if attempts >= 3:
                            print(f"  âŒ æœ€ç»ˆæ€»ç»“è°ƒç”¨å¤±è´¥(é‡è¯•å·²è¾¾ä¸Šé™): {fe}")
                            result = {"output": "{}"}
                            break
                        wait = [3, 6, 12][attempts] if attempts < 3 else 12
                        print(f"  - æœ€ç»ˆæ€»ç»“å¤±è´¥ï¼Œç¬¬ {attempts+1} æ¬¡é‡è¯•å‰ç­‰å¾… {wait}s: {fe}")
                        time.sleep(wait)
                        attempts += 1
                updated = self._extract_json_from_result(result)
                if isinstance(updated, dict):
                    current_final["algorithm_level_optimizations"] = updated.get(
                        "algorithm_level_optimizations", current_final["algorithm_level_optimizations"])
                    current_final["code_level_optimizations"] = updated.get(
                        "code_level_optimizations", current_final["code_level_optimizations"])
                    current_final["instruction_level_optimizations"] = updated.get(
                        "instruction_level_optimizations", current_final["instruction_level_optimizations"])
                    current_final["analyzed_algorithms"].append(algorithm)
                    FileManager.save_content(final_path, json.dumps(current_final, ensure_ascii=False, indent=2))

            print(f"  âœ… æœ€ç»ˆæ€»ç»“å®Œæˆ â†’ {os.path.basename(final_path)}")
            state["final_summary_done"] = True
        except Exception as e:
            err = f"æœ€ç»ˆæ€»ç»“å¤±è´¥: {e}"
            print(f"  âŒ {err}")
            state.setdefault("errors", []).append(err)
        return state

    # --- å·¥å…· ---
    def _read_source(self, file_path: str, limit: int = 15000) -> str:
        """æœ¬åœ°æ–‡ä»¶è¯»å–å‡½æ•°ã€‚"""
        try:
            full_path = os.path.join("openblas-output/GENERIC/kernel", file_path)
            with open(full_path, "r", encoding="utf-8", errors="ignore") as f:
                return f.read(limit)
        except Exception as e:
            return f"è¯»å–å¤±è´¥: {e}"

    def _find_or_create_entry(self, analyses: List[Dict[str, Any]], algorithm: str, file_path: str) -> Dict[str, Any]:
        for ea in analyses:
            if isinstance(ea, dict) and ea.get("file_path") == file_path:
                return ea
        entry = {
            "algorithm": algorithm,
            "file_path": file_path,
            "architecture": "é€šç”¨",
            "computational_patterns": [],
            "algorithm_level_optimizations": [],
            "code_level_optimizations": [],
            "instruction_level_optimizations": [],
            "implementation_details": "",
            "performance_insights": "",
        }
        analyses.append(entry)
        return entry

    def _save_analysis(self, analysis_path: str, existing_analyses: List[Dict[str, Any]], algorithm: str, total_files: int):
        payload = {
            "algorithm": algorithm,
            "total_files": total_files,
            "analyzed_files": len(existing_analyses),
            "individual_analyses": existing_analyses,
        }
        FileManager.save_content(analysis_path, json.dumps(payload, ensure_ascii=False, indent=2))

    def _extract_json_from_result(self, result):
        def wrap_if_list(obj):
            if isinstance(obj, list):
                return {
                    "algorithm_level_optimizations": obj,
                    "code_level_optimizations": [],
                    "instruction_level_optimizations": [],
                }
            return obj

        if isinstance(result, dict) and "output" in result:
            output = result["output"]
            if "```json" in output:
                s = output.find("```json") + 7
                e = output.find("```", s)
                parsed = self._parse_json(output[s:e])
                return wrap_if_list(parsed)
            if "```" in output:
                s = output.find("```") + 3
                e = output.find("```", s)
                parsed = self._parse_json(output[s:e])
                return wrap_if_list(parsed)
            parsed = self._parse_json(output)
            return wrap_if_list(parsed)
        elif isinstance(result, dict):
            return result
        elif isinstance(result, list):
            return wrap_if_list(result)
        return None

    @staticmethod
    def _parse_json(text: str):
        try:
            return json.loads(text.strip())
        except Exception:
            return None

    def _scan_and_classify_files_locally(self) -> Dict[str, Dict]:
        """ç›´æ¥æ‰«æå¹¶åˆ†ç±»æ–‡ä»¶ï¼ˆä¸ agent_work21/22 ä¿æŒä¸€è‡´é€»è¾‘ï¼‰ã€‚"""
        kernel_path = "openblas-output/GENERIC/kernel"
        if not os.path.exists(kernel_path):
            return {}

        all_files = sorted([f for f in os.listdir(kernel_path) if f.endswith('.c') and 'clean' in f])
        import re

        algorithm_patterns = {
            'axpy': r'.*axpy.*', 'gemm': r'.*gemm.*', 'dot': r'.*(dot|dotu|dotc).*',
            'asum': r'.*asum.*', 'nrm2': r'.*nrm2.*', 'scal': r'.*scal.*', 'copy': r'.*copy.*',
            'swap': r'.*swap.*', 'amax': r'.*amax.*', 'amin': r'.*amin.*', 'ger': r'.*ger.*',
            'gemv': r'.*gemv.*', 'symv': r'.*symv.*', 'hemv': r'.*hemv.*', 'trmm': r'.*trmm.*',
            'trsm': r'.*trsm.*', 'symm': r'.*symm.*', 'hemm': r'.*hemm.*', 'rot': r'.*rot.*',
            'rotm': r'.*rotm.*', 'geadd': r'.*geadd.*', 'imatcopy': r'.*imatcopy.*',
            'omatcopy': r'.*omatcopy.*', 'laswp': r'.*laswp.*', 'max': r'.*max.*',
            'min': r'.*min.*', 'sum': r'.*sum.*', 'neg': r'.*neg.*'
        }

        algorithms: Dict[str, Dict] = {}
        for filename in all_files:
            classified = False
            for algo_name, pattern in algorithm_patterns.items():
                if re.match(pattern, filename, re.IGNORECASE):
                    algorithms.setdefault(algo_name, {"algorithm": algo_name, "files": []})
                    algorithms[algo_name]["files"].append({"name": filename})
                    classified = True
                    break

            if not classified:
                base_name = filename.replace('.clean.c', '')
                if len(base_name) > 1 and base_name[0] in 'sdcz':
                    potential_algo = base_name[1:]
                else:
                    potential_algo = base_name

                potential_algo = re.sub(r'_.*', '', potential_algo)

                if len(potential_algo) > 2:
                    algorithms.setdefault(potential_algo, {"algorithm": potential_algo, "files": []})
                    algorithms[potential_algo]["files"].append({"name": filename})
        return algorithms


def _scan_algorithms_default() -> List[str]:
    wf_for_scan = Workflow23()
    algos_map = wf_for_scan._scan_and_classify_files_locally()
    return sorted(list(algos_map.keys()))


def _save_run_state(report_folder: str, mode: str, algorithms: List[str], files_per_algorithm: Optional[int]):
    """ä¿å­˜è¿è¡ŒçŠ¶æ€åˆ°æ–‡ä»¶å¤¹ï¼Œç”¨äºç»­è·‘ã€‚"""
    state_file = os.path.join(report_folder, "run_state.json")
    state = {
        "mode": mode,
        "algorithms": algorithms,
        "files_per_algorithm": files_per_algorithm,
        "created_time": time.strftime('%Y-%m-%d %H:%M:%S'),
        "last_updated": time.strftime('%Y-%m-%d %H:%M:%S')
    }
    try:
        with open(state_file, 'w', encoding='utf-8') as f:
            json.dump(state, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜è¿è¡ŒçŠ¶æ€å¤±è´¥: {e}")

def _load_run_state(report_folder: str) -> Optional[Dict]:
    """ä»æ–‡ä»¶å¤¹åŠ è½½è¿è¡ŒçŠ¶æ€ã€‚"""
    state_file = os.path.join(report_folder, "run_state.json")
    if not os.path.exists(state_file):
        return None
    try:
        with open(state_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"âš ï¸ åŠ è½½è¿è¡ŒçŠ¶æ€å¤±è´¥: {e}")
        return None

def main():
    parser = argparse.ArgumentParser(description="OpenBLASä¼˜åŒ–åˆ†æå·¥ä½œæµ (agent_work23 - LangGraph)")
    parser.add_argument("mode", choices=['analysis_only', 'full'], help="æ‰§è¡Œæ¨¡å¼")
    parser.add_argument("--algorithms", nargs='+', help="æŒ‡å®šè¦åˆ†æçš„ç®—å­åˆ—è¡¨ï¼›æœªæä¾›åˆ™è‡ªåŠ¨æ‰«æå†…ç½®é›†åˆã€‚")
    parser.add_argument("--files-per-algorithm", type=int, help="é™åˆ¶æ¯ä¸ªç®—å­è¦åˆ†æçš„æ–‡ä»¶æ•°é‡ï¼ˆæ­£æ•´æ•°ï¼‰ã€‚")
    parser.add_argument("--resume", help="æ¢å¤æŒ‡å®šæ–‡ä»¶å¤¹çš„åˆ†æï¼ˆå¦‚ï¼šresults/20251103_173600ï¼‰")
    args = parser.parse_args()

    if not os.getenv("DASHSCOPE_API_KEY"):
        print("âŒ é”™è¯¯: è¯·è®¾ç½® DASHSCOPE_API_KEY")
        return
    if not os.path.exists("./openblas-output/GENERIC/kernel"):
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ° openblas-output/GENERIC/kernel ç›®å½•")
        return

    # å¤„ç†ç»­è·‘é€»è¾‘
    if args.resume:
        if not os.path.exists(args.resume):
            print(f"âŒ é”™è¯¯: æŒ‡å®šçš„æ¢å¤è·¯å¾„ä¸å­˜åœ¨: {args.resume}")
            return
        report_folder = args.resume
        print(f"ğŸ“ æ¢å¤åˆ†æï¼Œä½¿ç”¨æ–‡ä»¶å¤¹: {report_folder}")
        
        # åŠ è½½ä¹‹å‰çš„è¿è¡ŒçŠ¶æ€
        saved_state = _load_run_state(report_folder)
        if saved_state:
            # ä½¿ç”¨ä¿å­˜çš„å‚æ•°ï¼Œä½†å…è®¸å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
            mode = args.mode if hasattr(args, 'mode') and args.mode else saved_state.get("mode", "analysis_only")
            algorithms = args.algorithms if args.algorithms else saved_state.get("algorithms", [])
            files_per_algorithm = args.files_per_algorithm if args.files_per_algorithm else saved_state.get("files_per_algorithm")
            print(f"ğŸ“‹ æ¢å¤çŠ¶æ€: æ¨¡å¼={mode}, ç®—å­={len(algorithms)}ä¸ª, æ–‡ä»¶é™åˆ¶={files_per_algorithm}")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°è¿è¡ŒçŠ¶æ€æ–‡ä»¶ï¼Œä½¿ç”¨å½“å‰å‘½ä»¤è¡Œå‚æ•°")
            mode = args.mode
            algorithms = args.algorithms or _scan_algorithms_default()
            files_per_algorithm = args.files_per_algorithm
    else:
        # æ–°å»ºåˆ†æ
        report_folder = f"results/{time.strftime('%Y%m%d_%H%M%S')}"
        FileManager.ensure_directories(report_folder)
        print(f"ğŸ“ æŠ¥å‘Šå°†ä¿å­˜åœ¨: {report_folder}")
        
        mode = args.mode
        algorithms = args.algorithms or _scan_algorithms_default()
        files_per_algorithm = args.files_per_algorithm

    if not algorithms:
        print("âŒ æœªå‘ç°å¯åˆ†æçš„ç®—å­")
        return
    print(f"ğŸ¯ å°†åˆ†æä»¥ä¸‹ç®—å­: {', '.join(algorithms)}")

    # ä¿å­˜/æ›´æ–°è¿è¡ŒçŠ¶æ€
    _save_run_state(report_folder, mode, algorithms, files_per_algorithm)

    wf = Workflow23(files_per_algorithm=files_per_algorithm)
    initial_state: WorkState = {
        "mode": mode,
        "report_folder": report_folder,
        "algorithms": algorithms,
        "errors": [],
    }
    print("\nğŸš€ å¼€å§‹æ‰§è¡Œ LangGraph å·¥ä½œæµ...")
    final_state = wf.graph.invoke(initial_state)
    print("\nğŸ å·¥ä½œæµæ‰§è¡Œå®Œæ¯•ã€‚")
    if final_state.get("errors"):
        print("\nâš ï¸ æœŸé—´å‡ºç°é”™è¯¯:")
        for i, err in enumerate(final_state["errors"], 1):
            print(f"  {i}. {err}")


if __name__ == "__main__":
    main()


