"""
Run all the generated code.
运行所有的代码
"""
# std imports
from argparse import ArgumentParser
import json
import logging
import os
import tempfile
from typing import Optional

# tpl imports
from tqdm import tqdm

# local imports
from driver_wrapper import DriverWrapper
from cpp.cpp_driver_wrapper import CppDriverWrapper
from util import await_input, load_json


""" 将语言名称映射到相应的驱动器包装器 """
LANGUAGE_DRIVERS = {
    "cpp": CppDriverWrapper,
}
# 解析命令行参数
def get_args():
    parser = ArgumentParser(description="运行所有生成的代码")
    parser.add_argument("--input_json", type=str, default="../prompt_test/promptsv1_code.json", help="包括测试用例的输入json文件")
    parser.add_argument("-o", "--output", type=str, default="../prompt_test/promptsv1_code_run.json", help="包括运行结果的输出json文件")
    parser.add_argument("--scratch-dir", type=str, default="tmp/", help="临时文件存放目录")
    parser.add_argument("--launch-configs", type=str, default="launch-configs.json", help="样本运行配置文件")
    parser.add_argument("--problem-sizes", type=str, default="problem-sizes.json", help="样本问题大小配置文件")
    parser.add_argument("--yes-to-all", action="store_true", help="自动回答所有的提示为是")
    parser.add_argument("--dry", action="store_true", help="仅模拟运行，不真正执行代码。")
    parser.add_argument("--overwrite", action="store_true", help="覆盖已有的输出结果。默认跳过已有结果。")
    parser.add_argument("--hide-progress", action="store_true", help="隐藏进度条显示。")

    # 模型选择互斥组（包含或排除模型）
    model_group = parser.add_mutually_exclusive_group()
    model_group.add_argument("--exclude-models", nargs="+", type=str, choices=["serial", "omp", "mpi", "mpi+omp", "kokkos", "cuda", "hip"], help="排除指定的并行模型。")
    model_group.add_argument("--include-models", nargs="+", type=str,
                             default=["serial", "omp", "cuda"], 
                             choices=["serial", "omp", "mpi", "mpi+omp", "kokkos", "cuda", "hip"], help="只测试指定的并行模型。")
    
    # 问题筛选互斥组
    model_group = parser.add_mutually_exclusive_group()
    model_group.add_argument("--problem", type=str, help="只测试指定的问题名称。")
    model_group.add_argument("--problem-type", type=str, help="只测试指定类型的问题。")

    parser.add_argument("--early-exit-runs", action="store_true", help="一旦某个配置失败，则停止评估该模型输出。")
    parser.add_argument("--build-timeout", type=int, default=30, help="构建程序超时时间（秒）。")
    parser.add_argument("--run-timeout", type=int, default=120, help="运行程序超时时间（秒）。")
    parser.add_argument("--log", choices=["INFO", "DEBUG", "WARNING", "ERROR", "CRITICAL"], default="INFO", type=str.upper, help="日志记录级别。")
    parser.add_argument("--log-build-errors", action="store_true", help="构建错误时输出错误信息。")
    parser.add_argument("--log-runs", action="store_true", help="显示运行时的标准输出和错误输出。")
    return parser.parse_args()

def get_driver(prompt: dict, scratch_dir: Optional[os.PathLike], launch_configs: dict, problem_sizes: dict, dry: bool, **kwargs) -> DriverWrapper:
    """获取当前 prompt 使用的语言对应的驱动器"""
    driver_cls = LANGUAGE_DRIVERS[prompt["language"]]
    return driver_cls(parallelism_model=prompt["parallelism_model"], launch_configs=launch_configs, 
        problem_sizes=problem_sizes, scratch_dir=scratch_dir, dry=dry, **kwargs)

def already_has_results(prompt: dict) -> bool:
    """检查 prompt 是否已包含结果"""
    if "outputs" not in prompt or not isinstance(prompt["outputs"], list):
        raise ValueError(f"Prompt {prompt.get('name', 'unknown')} does not have any outputs.")
    
    outputs = prompt["outputs"]
    if len(outputs) == 0 or all(isinstance(o, str) for o in outputs):
        return False  # 没有有效的结果

    if len(outputs) > 0 and all(isinstance(o, dict) for o in outputs):
        return True   # 已经有结果

    raise ValueError(f"Prompt {prompt.get('name', 'unknown')} has invalid outputs.")

def main():
    args = get_args()

    # 确保 临时文件 scratch 目录存在
    os.makedirs(args.scratch_dir, exist_ok=True)

    # 设置日志级别
    numeric_level = getattr(logging, args.log.upper(), None)
    if not isinstance(numeric_level, int):
        raise ValueError("Invalid log level: {}".format(args.log))
    logging.basicConfig(format="%(asctime)s [%(levelname)s] -- %(message)s", level=numeric_level)

    # 安全提示
    logging.warning("This script will compile and run code generated by an LLM. " +
        "It is recommended that you run this script in a sandboxed environment.")
    if not args.yes_to_all:
        response = await_input("Continue? [y/n] ", lambda r: r.lower() in ["y", "n", "yes", "no"])
        if response.lower() in ["n", "no"]:
            logging.info("Exiting.")
            return

    # load in the generated text（加载json文件）
    data = load_json(args.input_json)
    logging.info(f"Loaded {len(data)} prompts from {args.input_json}.")

    # load launch configs（加载运行配置）
    launch_configs = load_json(args.launch_configs)
    logging.info(f"Loaded launch configs from {args.launch_configs}.")

    # load problem sizes（加载问题规模配置）
    problem_sizes = load_json(args.problem_sizes)
    logging.info(f"Loaded problem sizes from {args.problem_sizes}.")

    # gather the list of parallelism models to test（确定需要测试的模型列表）
    models_to_test = args.include_models if args.include_models else ["serial", "omp", "mpi", "mpi+omp", "kokkos", "cuda", "hip"]
    if args.exclude_models:
        models_to_test = [m for m in models_to_test if m not in args.exclude_models]

    # run each prompt（遍历每一个prompt进行测试）
    all_prompts = data if args.hide_progress else tqdm(data, desc="Testing prompts")
    for prompt in all_prompts:
        # 跳过不在模型列表中的prompt
        if prompt["parallelism_model"] not in models_to_test:
            logging.debug(f"Skipping prompt {prompt['name']} because it uses {prompt['parallelism_model']}.")
            continue

        # 按照名称或者类型筛选prompt
        if args.problem and prompt["name"] != args.problem:
            logging.debug(f"Skipping prompt {prompt['name']} because it is not {args.problem}.")
            continue

        if args.problem_type and prompt["problem_type"] != args.problem_type:
            logging.debug(f"Skipping prompt {prompt['name']} because it is not {args.problem_type}.")
            continue

        # 跳过已有结果的prompt
        if already_has_results(prompt):
            if args.overwrite:
                logging.debug(f"Prompt {prompt['name']} already has results. Overwriting.")
                prompt["outputs"] = [p["generated_output"] for p in prompt["outputs"]]
            else:
                logging.debug(f"Skipping prompt {prompt['name']} because it already has results. \
                    Use --overwrite to overwrite existing results.")
                continue

        # 获取驱动器并运行
        driver = get_driver(
            prompt, 
            args.scratch_dir, 
            launch_configs, 
            problem_sizes,
            args.dry, 
            display_build_errors=args.log_build_errors,
            display_runs=args.log_runs,
            early_exit_runs=args.early_exit_runs,
            build_timeout=args.build_timeout,
            run_timeout=args.run_timeout
        )
        driver.test_all_outputs_in_prompt(prompt)

        # go ahead and write out outputs now（中间保存结果）
        if args.output and args.output != '-':
            with open(args.output, "w") as fp:
                json.dump(data, fp, indent=4)
            logging.debug(f"Wrote intermediate results to {args.output}.")

    # 最后写入结果
    if args.output and args.output != '-':
        with open(args.output, "w") as fp:
            json.dump(data, fp, indent=4)
        logging.info(f"Wrote results to {args.output}.")
    else:
        print(json.dumps(data, indent=4))

if __name__ == "__main__":
    main()
