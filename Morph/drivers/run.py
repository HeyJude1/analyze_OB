"""
运行代码
"""
# std imports
from argparse import ArgumentParser
import json
import logging
import os
import tempfile
from typing import Optional

# tpl imports
from tqdm import tqdm

# local imports
from driver_wrapper import DriverWrapper
from cpp.cpp_driver_wrapper import CppDriverWrapper
from util import await_input, load_json

""" 将语言名称映射到相应的驱动器包装器 """
LANGUAGE_DRIVERS = {
    "cpp": CppDriverWrapper,
}

def get_driver(prompt: dict, scratch_dir: Optional[os.PathLike], launch_configs: dict, problem_sizes: dict, dry: bool, **kwargs) -> DriverWrapper:
    """获取当前 prompt 使用的语言对应的驱动器"""
    driver_cls = LANGUAGE_DRIVERS[prompt["language"]]
    return driver_cls(parallelism_model=prompt["parallelism_model"], launch_configs=launch_configs, 
        problem_sizes=problem_sizes, scratch_dir=scratch_dir, dry=dry, **kwargs)

def already_has_results(prompt: dict) -> bool:
    """检查 prompt 是否已包含结果"""
    if "outputs" not in prompt or not isinstance(prompt["outputs"], list):
        raise ValueError(f"Prompt {prompt.get('name', 'unknown')} does not have any outputs.")
    
    outputs = prompt["outputs"]
    if len(outputs) == 0 or all(isinstance(o, str) for o in outputs):
        return False  # 没有有效的结果

    if len(outputs) > 0 and all(isinstance(o, dict) for o in outputs):
        return True   # 已经有结果

    raise ValueError(f"Prompt {prompt.get('name', 'unknown')} has invalid outputs.")

def CodeRun(
    input_path: str,
    output_path: str,
    scratch_dir: str="tmp/",
    launch_configs_path: str="launch-configs.json",
    problem_sizes_path: str="problem-sizes.json",
    yes_to_all: bool=False,
    dry: bool=False,
    overwrite: bool=False,
    hide_progress: bool=False,
    include_models: list[str] | None=None,
    early_exit_runs: bool=False,
    build_timeout: int=30,
    run_timeout: int=120,
    log: str="INFO", # INFO, DEBUG, WARNING, ERROR, CRITICAL
    log_build_errors: bool=False,
    log_runs: bool=False,
    ) -> None:

    # 确定包含的模型
    if include_models is None:
        include_models = ["serial", "omp", "cuda"]
    
    # 确保临时文件存在
    os.makedirs(scratch_dir, exist_ok=True)

    # 设置日志级别
    numeric_level = getattr(logging, log.upper(), None)
    if not isinstance(numeric_level, int):
        raise ValueError("Invalid log level: {}".format(log))
    logging.basicConfig(format="%(asctime)s [%(levelname)s] -- %(message)s", level=numeric_level)

    # 安全提示
    logging.warning("This script will compile and run code generated by an LLM. " +
        "It is recommended that you run this script in a sandboxed environment.")
    if not yes_to_all:
        response = await_input("Continue? [y/n] ", lambda r: r.lower() in ["y", "n", "yes", "no"])
        if response.lower() in ["n", "no"]:
            logging.info("Exiting.")
            return
    
    # 加载json文件
    data = load_json(input_path)
    logging.info(f"Loaded {len(data)} prompts from {input_path}.")

    # 加载运行配置
    launch_configs = load_json(launch_configs_path)
    logging.info(f"Loaded launch configs from {launch_configs_path}.")

    # 加载问题规模配置
    problem_sizes = load_json(problem_sizes_path)
    logging.info(f"Loaded problem sizes from {problem_sizes_path}.")

    # 确定需要测试的模型列表
    models_to_test = include_models

    # 遍历每一个prompt进行测试
    all_prompts = data if hide_progress else tqdm(data, desc="Testing prompts") # 进度条隐藏判断
    for prompt in all_prompts:
        # 跳过不在模型列表中的prompt
        if prompt["parallelism_model"] not in models_to_test:
            logging.debug(f"Skipping prompt {prompt['name']} because it uses {prompt['parallelism_model']}.")
            continue

        # 跳过已有结果的prompt
        if already_has_results(prompt):
            if overwrite:
                logging.debug(f"Prompt {prompt['name']} already has results. Overwriting.")
                prompt["outputs"] = [p["generated_output"] for p in prompt["outputs"]]
            else:
                logging.debug(f"Skipping prompt {prompt['name']} because it already has results. \
                    Use --overwrite to overwrite existing results.")
                continue
        
        # 获取驱动器并运行
        driver = get_driver(
            prompt, 
            scratch_dir, 
            launch_configs, 
            problem_sizes,
            dry, 
            display_build_errors=log_build_errors,
            display_runs=log_runs,
            early_exit_runs=early_exit_runs,
            build_timeout=build_timeout,
            run_timeout=run_timeout
        )
        driver.test_all_outputs_in_prompt(prompt)

        # 中间保存结果
        if output_path and output_path != '-':
            with open(output_path, "w") as fp:
                json.dump(data, fp, indent=4)
            logging.debug(f"Wrote intermediate results to {output_path}.")
    
    # 最后写入结果
    if output_path and output_path != '-':
        with open(output_path, "w") as fp:
            json.dump(data, fp, indent=4)
        logging.info(f"Wrote results to {output_path}.")
    else:
        print(json.dumps(data, indent=4))

if __name__ == "__main__":
    # "../results/code.json"->"../results/code_run.json"
    # CodeRun(input_path="../results/code.json", output_path="../results/code_run.json")
    # "../results/code_review.json" ->"../results/code_review_run.json"
    CodeRun(input_path="../results/code_review.json", output_path="../results/code_review_run.json")

