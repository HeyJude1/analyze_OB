#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
OpenBLASä¼˜åŒ–ç­–ç•¥åˆ†æå·¥ä½œæµ v24
é›†æˆæ ¼å¼éªŒè¯å’Œé‡è¯•æœºåˆ¶
"""

import os
import json
import time
from typing import Dict, Any, List, TypedDict
from pathlib import Path
from dotenv import load_dotenv
from langgraph.graph import StateGraph, START, END
from langchain.output_parsers import StructuredOutputParser, ResponseSchema

from agent24 import create_algorithm_optimizer, create_code_optimizer, create_instruction_optimizer, load_config

load_dotenv()


class WorkflowState(TypedDict):
    """å·¥ä½œæµçŠ¶æ€"""
    algorithm: str
    source_files: List[str]
    analysis_results: Dict[str, Any]
    current_stage: str
    errors: List[str]
    retry_count: int


class Workflow24:
    """OpenBLASä¼˜åŒ–åˆ†æå·¥ä½œæµv24"""
    
    def __init__(self):
        self.config = load_config()
        self.model_config = self.config.get("model", {})
        
        # åˆ›å»ºä¼˜åŒ–åˆ†æå™¨
        self.algorithm_llm, self.algorithm_prompt = create_algorithm_optimizer(self.model_config)
        self.code_llm, self.code_prompt = create_code_optimizer(self.model_config)
        self.instruction_llm, self.instruction_prompt = create_instruction_optimizer(self.model_config)
        
        # æ„å»ºå·¥ä½œæµ
        self.workflow = self._build_workflow()
    
    def _build_workflow(self) -> StateGraph:
        """æ„å»ºLangGraphå·¥ä½œæµ"""
        workflow = StateGraph(WorkflowState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("algorithm_analysis", self.algorithm_analysis_node)
        workflow.add_node("code_analysis", self.code_analysis_node)
        workflow.add_node("instruction_analysis", self.instruction_analysis_node)
        workflow.add_node("finalize", self.finalize_node)
        
        # è®¾ç½®è¾¹
        workflow.add_edge(START, "algorithm_analysis")
        workflow.add_edge("algorithm_analysis", "code_analysis")
        workflow.add_edge("code_analysis", "instruction_analysis")
        workflow.add_edge("instruction_analysis", "finalize")
        workflow.add_edge("finalize", END)
        
        return workflow.compile()
    
    def _validate_optimization_format(self, optimization: Dict[str, Any]) -> bool:
        """éªŒè¯å•ä¸ªä¼˜åŒ–ç­–ç•¥çš„æ ¼å¼"""
        required_fields = ["optimization_name", "level", "description", 
                          "applicability_conditions", "tunable_parameters", "related_patterns"]
        
        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        for field in required_fields:
            if field not in optimization:
                return False
        
        # æ£€æŸ¥descriptionå­—æ®µç»“æ„
        description = optimization.get("description", {})
        if not isinstance(description, dict):
            return False
        
        required_desc_fields = ["strategy_rationale", "implementation_pattern", 
                               "performance_impact", "trade_offs"]
        
        for field in required_desc_fields:
            if field not in description:
                return False
        
        # ç¡®ä¿descriptionä¸­æ²¡æœ‰å…¶ä»–å­—æ®µ
        if len(description) != 4:
            return False
        
        return True
    
    def _validate_optimization_list(self, optimizations: List[Dict[str, Any]]) -> bool:
        """éªŒè¯ä¼˜åŒ–ç­–ç•¥åˆ—è¡¨çš„æ ¼å¼"""
        if not isinstance(optimizations, list):
            return False
        
        for opt in optimizations:
            if not self._validate_optimization_format(opt):
                return False
        
        return True
    
    def analyzer_work_node(self, state: WorkflowState, level: str, llm, prompt: str) -> WorkflowState:
        """é€šç”¨åˆ†æèŠ‚ç‚¹ï¼Œå¸¦æ ¼å¼éªŒè¯å’Œé‡è¯•"""
        algorithm = state["algorithm"]
        source_files = state.get("source_files", [])
        
        # å‡†å¤‡åˆ†æè¾“å…¥
        source_content = ""
        for file_path in source_files:
            if os.path.exists(file_path):
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    source_content += f"\n=== {file_path} ===\n{content}\n"
        
        # åˆ›å»ºç»“æ„åŒ–è¾“å‡ºè§£æå™¨
        response_schemas = [
            ResponseSchema(name="optimization_name", description="ä¼˜åŒ–ç­–ç•¥åç§°"),
            ResponseSchema(name="level", description="ä¼˜åŒ–å±‚çº§"),
            ResponseSchema(name="description", description="åŒ…å«4ä¸ªå­å­—æ®µçš„è¯¦ç»†æè¿°å¯¹è±¡"),
            ResponseSchema(name="applicability_conditions", description="é€‚ç”¨æ¡ä»¶"),
            ResponseSchema(name="tunable_parameters", description="å¯è°ƒå‚æ•°"),
            ResponseSchema(name="related_patterns", description="ç›¸å…³è®¡ç®—æµç¨‹")
        ]
        
        parser = StructuredOutputParser.from_response_schemas(response_schemas)
        format_instructions = parser.get_format_instructions()
        
        # æ„å»ºå®Œæ•´æç¤º
        full_prompt = f"""{prompt}

è¯·åˆ†æä»¥ä¸‹{algorithm}ç®—å­çš„æºä»£ç ï¼Œè¯†åˆ«{level}å±‚çš„ä¼˜åŒ–ç­–ç•¥ï¼š

{source_content}

{format_instructions}

è¯·ä»¥JSONæ•°ç»„æ ¼å¼è¿”å›æ‰€æœ‰è¯†åˆ«åˆ°çš„ä¼˜åŒ–ç­–ç•¥ã€‚"""
        
        max_retries = 3
        retry_count = 0
        
        while retry_count < max_retries:
            try:
                # è°ƒç”¨LLM
                response = llm.invoke(full_prompt)
                
                # è§£æå“åº”
                if hasattr(response, 'content'):
                    content = response.content
                else:
                    content = str(response)
                
                # å°è¯•è§£æJSON
                try:
                    # æå–JSONéƒ¨åˆ†
                    if '```json' in content:
                        start = content.find('```json') + 7
                        end = content.find('```', start)
                        json_str = content[start:end].strip()
                    elif '```' in content:
                        start = content.find('```') + 3
                        end = content.rfind('```')
                        json_str = content[start:end].strip()
                    else:
                        json_str = content
                    
                    optimizations = json.loads(json_str)
                    
                    # éªŒè¯æ ¼å¼
                    if self._validate_optimization_list(optimizations):
                        # æ ¼å¼æ­£ç¡®ï¼Œä¿å­˜ç»“æœ
                        state["analysis_results"][f"{level}_level_optimizations"] = optimizations
                        state["current_stage"] = f"{level}_completed"
                        print(f"âœ… {level}å±‚åˆ†æå®Œæˆï¼Œè¯†åˆ«åˆ° {len(optimizations)} ä¸ªä¼˜åŒ–ç­–ç•¥")
        return state
                    else:
                        raise ValueError("ä¼˜åŒ–ç­–ç•¥æ ¼å¼éªŒè¯å¤±è´¥")
                
                except json.JSONDecodeError as e:
                    raise ValueError(f"JSONè§£æå¤±è´¥: {e}")
            
        except Exception as e:
                retry_count += 1
                error_msg = f"{level}å±‚åˆ†æå¤±è´¥ (å°è¯• {retry_count}/{max_retries}): {e}"
                print(f"âš ï¸ {error_msg}")
                
                if retry_count >= max_retries:
                    state["errors"].append(error_msg)
                    state["analysis_results"][f"{level}_level_optimizations"] = []
                    state["current_stage"] = f"{level}_failed"
                    break
                else:
                    print(f"ğŸ”„ é‡è¯•{level}å±‚åˆ†æ...")
                    time.sleep(1)  # çŸ­æš‚å»¶è¿Ÿåé‡è¯•
        
        return state
    
    def algorithm_analysis_node(self, state: WorkflowState) -> WorkflowState:
        """ç®—æ³•å±‚åˆ†æèŠ‚ç‚¹"""
        return self.analyzer_work_node(state, "algorithm", self.algorithm_llm, self.algorithm_prompt)
    
    def code_analysis_node(self, state: WorkflowState) -> WorkflowState:
        """ä»£ç å±‚åˆ†æèŠ‚ç‚¹"""
        return self.analyzer_work_node(state, "code", self.code_llm, self.code_prompt)
    
    def instruction_analysis_node(self, state: WorkflowState) -> WorkflowState:
        """æŒ‡ä»¤å±‚åˆ†æèŠ‚ç‚¹"""
        return self.analyzer_work_node(state, "instruction", self.instruction_llm, self.instruction_prompt)
    
    def finalize_node(self, state: WorkflowState) -> WorkflowState:
        """æœ€ç»ˆåŒ–èŠ‚ç‚¹"""
        state["current_stage"] = "completed"
        
        # ç»Ÿè®¡ç»“æœ
        total_optimizations = 0
        for level in ["algorithm", "code", "instruction"]:
            opts = state["analysis_results"].get(f"{level}_level_optimizations", [])
            total_optimizations += len(opts)
        
        print(f"ğŸ‰ åˆ†æå®Œæˆï¼æ€»å…±è¯†åˆ«åˆ° {total_optimizations} ä¸ªä¼˜åŒ–ç­–ç•¥")
        
        if state["errors"]:
            print(f"âš ï¸ åˆ†æè¿‡ç¨‹ä¸­å‡ºç° {len(state['errors'])} ä¸ªé”™è¯¯")
        
        return state
    
    def run_analysis(self, algorithm: str, source_files: List[str]) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„åˆ†ææµç¨‹"""
        print(f"ğŸš€ å¼€å§‹åˆ†æ {algorithm} ç®—å­")
        print(f"ğŸ“ æºæ–‡ä»¶: {source_files}")
        
        # åˆå§‹åŒ–çŠ¶æ€
        initial_state = WorkflowState(
            algorithm=algorithm,
            source_files=source_files,
            analysis_results={},
            current_stage="starting",
            errors=[],
            retry_count=0
        )
        
        # è¿è¡Œå·¥ä½œæµ
        final_state = self.workflow.invoke(initial_state)
        
        return final_state["analysis_results"]


def main():
    """ä¸»å‡½æ•°"""
    workflow = Workflow24()
    
    # ç¤ºä¾‹ç”¨æ³•
    algorithm = "gemm"
    source_files = [
        "OpenBLAS-develop/kernel/x86_64/gemm_kernel_4x4.c",
        "OpenBLAS-develop/kernel/generic/gemm_beta.c"
    ]
    
    results = workflow.run_analysis(algorithm, source_files)
    
    # ä¿å­˜ç»“æœ
    output_file = f"{algorithm}_analysis_v24.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“„ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")


if __name__ == "__main__":
    main()
